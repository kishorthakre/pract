{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf32c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba4c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a89eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x157596f0290>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('pr').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30bf2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------------------+\n",
      "|uid| pid|qunt|              pur_dt|\n",
      "+---+----+----+--------------------+\n",
      "|333|1122|   9|2022-02-06T01:00:...|\n",
      "|333|1122|  10|2022-02-06T02:00:...|\n",
      "|536|1435|  10|2022-03-02T08:40:...|\n",
      "|536|3223|   5|2022-03-02T09:33:...|\n",
      "|536|3223|   6|2022-01-11T12:33:...|\n",
      "|827|2452|  45|2022-03-02T00:00:...|\n",
      "|827|3585|  35|2022-02-20T14:05:...|\n",
      "+---+----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the customers who brought the same product more than once but on different days\n",
    "# ( if same product is purchased multiple times but on same date shouldn't be counted)\n",
    "\n",
    "data = [(333, 1122, 9, '2022-02-06T01:00:00.000+00:00'),\n",
    "        (333,1122,10,'2022-02-06T02:00:00.000+00:00'), \n",
    "        (536,1435, 10,'2022-03-02T08:40:00.000+00:00'),\n",
    "        (536,3223,5,'2022-03-02T09:33:28.000+00:00'),\n",
    "        (536, 3223, 6,'2022-01-11T12:33:44.000+00:00'),\n",
    "        (827, 2452, 45,'2022-03-02T00:00:00.000+00:00'), \n",
    "        (827, 3585, 35,'2022-02-20T14:05:26.000+00:00')]\n",
    "df = spark.createDataFrame(data = data, schema=['uid', 'pid', 'qunt', 'pur_dt'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0882e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+-----+\n",
      "|uid| pid|        dt|count|\n",
      "+---+----+----------+-----+\n",
      "|333|1122|2022-02-06|    2|\n",
      "|536|1435|2022-03-02|    1|\n",
      "|536|3223|2022-03-02|    1|\n",
      "|536|3223|2022-01-11|    1|\n",
      "|827|2452|2022-03-02|    1|\n",
      "|827|3585|2022-02-20|    1|\n",
      "+---+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('dt', to_date('pur_dt')).groupBy('uid', 'pid', 'dt').count()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7f5e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|uid| pid|count|\n",
      "+---+----+-----+\n",
      "|536|3223|    2|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('uid', 'pid').count().filter(col('count') >= 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8fdba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| Id|           name|\n",
      "+---+---------------+\n",
      "|  1|sagar-prajapati|\n",
      "|  2|      alex-john|\n",
      "|  3|      john cena|\n",
      "|  4|        kim joe|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In the given dataset, names contain for some names and space for some names, extract the first name and last name \n",
    "\n",
    "data = [(1, 'sagar-prajapati'), (2, 'alex-john'), (3, 'john cena'), (4, 'kim joe')]\n",
    "schema = ['Id','name']\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d756944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "| Id|           name|              A|\n",
      "+---+---------------+---------------+\n",
      "|  1|sagar-prajapati|sagar prajapati|\n",
      "|  2|      alex-john|      alex john|\n",
      "|  3|      john cena|      john cena|\n",
      "|  4|        kim joe|        kim joe|\n",
      "+---+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_regex = \"((,)?\\s|[-])\"\n",
    "df1 = df.withColumn(\"A\", regexp_replace(col(\"name\"), replace_regex, \" \"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41ceb8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------+---------+\n",
      "| Id|           name|First_name|Last_Name|\n",
      "+---+---------------+----------+---------+\n",
      "|  1|sagar-prajapati|         s|        a|\n",
      "|  2|      alex-john|         a|        l|\n",
      "|  3|      john cena|         j|        o|\n",
      "|  4|        kim joe|         k|        i|\n",
      "+---+---------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn('First_name', split(df1['A'], '').getItem(0)).withColumn('Last_Name', split(df1['A'], '').getItem(1))\n",
    "df2.drop('A').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "792957f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+\n",
      "| Id|           name|                 A|\n",
      "+---+---------------+------------------+\n",
      "|  1|sagar-prajapati|[sagar, prajapati]|\n",
      "|  2|      alex-john|      [alex, john]|\n",
      "|  3|      john cena|      [john, cena]|\n",
      "|  4|        kim joe|        [kim, joe]|\n",
      "+---+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_regex = \"((,)?\\s|[-])\"\n",
    "df1 = df.withColumn(\"A\", split(col(\"name\"), split_regex))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "010bc83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|person1|person2|call_duration|\n",
      "+-------+-------+-------------+\n",
      "|     10|     20|           58|\n",
      "|     20|     10|           12|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "|     40|     30|          500|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call_duration \n",
    "\n",
    "data = [(10, 20, 58), (20,10,12), (10,30, 20),(30,40,100),(30, 40, 200), (30, 40, 200), (40, 30, 500)]\n",
    "df = spark.createDataFrame(data = data, schema=['person1', 'person2', 'call_duration'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c678489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|person1|person2|call_duration|\n",
      "+-------+-------+-------------+\n",
      "|     10|     20|           58|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "|     10|     20|           58|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.alias('t1').unionAll(df.alias('t2')).filter(col('person1') < col('person2'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f662b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------+\n",
      "|person1|person2|call_count|total_duration|\n",
      "+-------+-------+----------+--------------+\n",
      "|     10|     20|         2|           116|\n",
      "|     10|     30|         2|            40|\n",
      "|     30|     40|         6|          1000|\n",
      "+-------+-------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy('person1','person2').agg(count(col('call_duration')).alias('call_count'), sum(col('call_duration')).alias('total_duration'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96b2c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| sub|\n",
      "+---+----+\n",
      "|  1|MATH|\n",
      "|  2|MATH|\n",
      "|  4|CHEM|\n",
      "|  5|MATH|\n",
      "|  2| ENG|\n",
      "|  3| PHY|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the teachers who teaches only math and not any other subject \n",
    "\n",
    "data = [(1, \"MATH\"), (2,'MATH'), (4, 'CHEM'),(5, 'MATH'),(2, 'ENG'), (3, 'PHY')]\n",
    "df = spark.createDataFrame(data = data, schema=['id', 'sub'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3051b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  4|    1|\n",
      "|  5|    1|\n",
      "|  3|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('id').count().filter(col('count') == 1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b512244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| sub|\n",
      "+---+----+\n",
      "|  1|MATH|\n",
      "|  5|MATH|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df1, df.id == df1.id, how = 'inner').filter(df['sub'] == 'MATH').select(df['*']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9dbfc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+\n",
      "|COMPANY|YEAR|REVENUE|\n",
      "+-------+----+-------+\n",
      "|    ABC|2000|    100|\n",
      "|    ABC|2001|    110|\n",
      "|    ABC|2002|    120|\n",
      "|    XYZ|2000|    100|\n",
      "|    XYZ|2001|     90|\n",
      "|    XYZ|2002|    120|\n",
      "|    RXC|2000|    500|\n",
      "|    RXC|2001|    400|\n",
      "|    RXC|2002|    600|\n",
      "|    RXC|2003|    800|\n",
      "+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out the companies where revenue has only increased over the years and there was no decrease at all for any point.\n",
    "\n",
    "data = [('ABC', 2000, 100),\n",
    "('ABC', 2001, 110),\n",
    "('ABC', 2002, 120),\n",
    "('XYZ', 2000, 100),\n",
    "('XYZ', 2001, 90),\n",
    "('XYZ', 2002, 120),\n",
    "('RXC', 2000, 500),\n",
    "('RXC', 2001, 400),\n",
    "('RXC', 2002, 600),\n",
    "('RXC', 2003, 800)]\n",
    "schema = StructType([StructField('COMPANY', StringType(), True),\n",
    "                     StructField('YEAR', IntegerType(), True),\n",
    "                     StructField('REVENUE', IntegerType(), True)]) \n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "167fafd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+----+\n",
      "|COMPANY|YEAR|REVENUE| lag|\n",
      "+-------+----+-------+----+\n",
      "|    ABC|2000|    100| 100|\n",
      "|    ABC|2001|    110|  10|\n",
      "|    ABC|2002|    120|  10|\n",
      "|    RXC|2000|    500| 500|\n",
      "|    RXC|2001|    400|-100|\n",
      "|    RXC|2002|    600| 200|\n",
      "|    RXC|2003|    800| 200|\n",
      "|    XYZ|2000|    100| 100|\n",
      "|    XYZ|2001|     90| -10|\n",
      "|    XYZ|2002|    120|  30|\n",
      "+-------+----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy('COMPANY').orderBy('YEAR')\n",
    "df1 = df.withColumn('lag', col('REVENUE')-lag(col('REVENUE'), 1, 0).over(window))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "396390e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|COMPANY|diff|\n",
      "+-------+----+\n",
      "|    ABC|  10|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy('COMPANY').agg(min(col('lag')).alias('diff')).filter(col('diff') > 0)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d65d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     Movie|       Type|Rating|\n",
      "+---+----------+-----------+------+\n",
      "|  1|       war|   great ed|   8.9|\n",
      "|  2|   science|    fiction|   8.5|\n",
      "|  3|     irish|     boring|   6.2|\n",
      "|  4|  Ice song|    fantacy|   8.6|\n",
      "|  5|house card|interesting|   9.1|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lIst down the movies with an odd ID and which is not boring and order by id desc \n",
    "\n",
    "data = [(1, 'war', 'great ed',8.9),\n",
    "    (2,'science','fiction',8.5),\n",
    "    (3,'irish', 'boring', 6.2),\n",
    "    (4, 'Ice song', 'fantacy', 8.6),\n",
    "    (5, \"house card\", 'interesting', 9.1)]\n",
    "sch = ['ID', 'Movie', 'Type', 'Rating']\n",
    "df = spark.createDataFrame(data=data, schema=sch)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df7f6e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     Movie|       Type|Rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|house card|interesting|   9.1|\n",
      "|  1|       war|   great ed|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.filter(((df['ID'] %2 ) != 0 ) & (col('Type') != 'boring')).orderBy(col('ID').desc())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "302ce2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----+\n",
      "| id| name|salary| mid|\n",
      "+---+-----+------+----+\n",
      "|  1| John|  6000|   4|\n",
      "|  2|Kevin| 11000|   4|\n",
      "|  3|  Bob|  8000|   5|\n",
      "|  4|Laura|  9000|NULL|\n",
      "|  5|Sarah| 10000|NULL|\n",
      "+---+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the employees earning more than managers \n",
    "\n",
    "data = [(1, \"John\", 6000, 4), (2,'Kevin',11000,4), (3, 'Bob',8000, 5),(4, 'Laura',9000,None),(5, 'Sarah',10000, None)]\n",
    "df = spark.createDataFrame(data = data, schema=['id', 'name', 'salary','mid'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6169934e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+---+\n",
      "| id| name|salary|mid|\n",
      "+---+-----+------+---+\n",
      "|  2|Kevin| 11000|  4|\n",
      "+---+-----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('emp').join(df.alias('mgr'), col('emp.mid')  == col('mgr.id'), 'inner').filter(col('emp.salary') > col('mgr.salary'))\\\n",
    ".select(col('emp.id'), col('emp.name'), col('emp.salary'), col('emp.mid')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698d1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9f03fd0",
   "metadata": {},
   "source": [
    "\"\"\"𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧\n",
    "You are working as a Data Engineer at a fintech company. Your team is working on integrating two datasets:\n",
    "\n",
    "1. Customer Transactions Data (transactions_df) - Contains customer transactions with columns: customer_id, txn_id, amount, and txn_date. \n",
    "\n",
    "2. Customer Profile Data (profile_df) - Contains customer information with columns: customer_id, name, age, and txn_id (latest transaction ID for reference).\n",
    "\n",
    " The requirement is to merge these two DataFrames on customer_id while keeping track of:\n",
    "\n",
    "Conflicting column names (txn_id) should be renamed properly.\n",
    "\n",
    "If a customer exists in profile_df but not in transactions_df, the row should still be present with NULL values for transaction-related columns.\n",
    "\n",
    "Your task is to write an optimized PySpark code to achieve this.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0224c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----------+\n",
      "|customer_id|txn_id|amount|  txn_date|\n",
      "+-----------+------+------+----------+\n",
      "|        101|  T001|   500|2024-08-10|\n",
      "|        102|  T002|  1200|2024-08-09|\n",
      "|        103|  T003|   300|2024-08-08|\n",
      "|        104|  T004|   450|2024-08-07|\n",
      "+-----------+------+------+----------+\n",
      "\n",
      "+-----------+----+---+------+\n",
      "|customer_id|name|age|txn_id|\n",
      "+-----------+----+---+------+\n",
      "|        101|John| 30|  T001|\n",
      "|        102|Emma| 27|  T005|\n",
      "|        103|Alex| 35|  T003|\n",
      "|        105| Sam| 40|  T006|\n",
      "+-----------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_data = [ (101, \"T001\", 500, \"2024-08-10\"), (102, \"T002\", 1200, \"2024-08-09\"), \n",
    "                     (103, \"T003\", 300, \"2024-08-08\"), (104, \"T004\", 450, \"2024-08-07\"), ] \n",
    "\n",
    "profile_data = [ (101, \"John\", 30, \"T001\"), (102, \"Emma\", 27, \"T005\"), \n",
    "                (103, \"Alex\", 35, \"T003\"), (105, \"Sam\", 40, \"T006\"), ]\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_data, [\"customer_id\", \"txn_id\", \"amount\", \"txn_date\"])\n",
    "transactions_df.show()\n",
    "\n",
    "profile_df = spark.createDataFrame(profile_data, [\"customer_id\", \"name\", \"age\", \"txn_id\"])\n",
    "profile_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764ca0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+-----------+\n",
      "|customer_id|name|age|last_txn_id|\n",
      "+-----------+----+---+-----------+\n",
      "|        101|John| 30|       T001|\n",
      "|        102|Emma| 27|       T005|\n",
      "|        103|Alex| 35|       T003|\n",
      "|        105| Sam| 40|       T006|\n",
      "+-----------+----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df = profile_df.withColumnRenamed('txn_id', 'last_txn_id')\n",
    "profile_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0597d463",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+-----------+------+------+----------+\n",
      "|customer_id|name| age|last_txn_id|txn_id|amount|  txn_date|\n",
      "+-----------+----+----+-----------+------+------+----------+\n",
      "|        101|John|  30|       T001|  T001|   500|2024-08-10|\n",
      "|        102|Emma|  27|       T005|  T002|  1200|2024-08-09|\n",
      "|        103|Alex|  35|       T003|  T003|   300|2024-08-08|\n",
      "|        104|NULL|NULL|       NULL|  T004|   450|2024-08-07|\n",
      "|        105| Sam|  40|       T006|  NULL|  NULL|      NULL|\n",
      "+-----------+----+----+-----------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df.join(transactions_df, on = \"customer_id\", how = \"full_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ab082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5d0009c",
   "metadata": {},
   "source": [
    "𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧\n",
    "\n",
    "You are given an employee dataset containing information about employees and their managers. Each employee has a manager_id that refers to another employee in the same table. Your task is to use self-join to find hierarchical relationships between employees, such as finding all employees under a specific manager or the reporting hierarchy of an employee.\n",
    "\n",
    "Interview Task\n",
    "- Write a PySpark self-join query to find the direct reports of each manager. Additionally, extend the logic to find all hierarchical relationships up to any level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd26875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+\n",
      "|employee_id|employee_name|manager_id|\n",
      "+-----------+-------------+----------+\n",
      "|          1|        Alice|      NULL|\n",
      "|          2|          Bob|         1|\n",
      "|          3|      Charlie|         1|\n",
      "|          4|        David|         2|\n",
      "|          5|          Eva|         2|\n",
      "|          6|        Frank|         3|\n",
      "|          7|        Grace|         3|\n",
      "+-----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, \"Alice\", None), (2, \"Bob\", 1),\n",
    "(3, \"Charlie\", 1), (4, \"David\", 2),\n",
    "(5, \"Eva\", 2), (6, \"Frank\", 3), (7, \"Grace\", 3) ]\n",
    "\n",
    "columns = [\"employee_id\", \"employee_name\", \"manager_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4516b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|employee_name|employee_name|\n",
      "+-------------+-------------+\n",
      "|         NULL|        Alice|\n",
      "|        Alice|          Bob|\n",
      "|        Alice|      Charlie|\n",
      "|          Bob|        David|\n",
      "|          Bob|          Eva|\n",
      "|      Charlie|        Frank|\n",
      "|      Charlie|        Grace|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('emp').join(df.alias('mgr'), col('mgr.employee_id') == col('emp.manager_id'), 'left')\\\n",
    "           .select(col('mgr.employee_name'), col('emp.employee_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5dd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5558d682",
   "metadata": {},
   "source": [
    "𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧\n",
    "\n",
    "You are working with large datasets in PySpark and need to join two DataFrames. However, one of the tables has highly skewed data, causing performance issues due to data shuffling. How would you optimize this join using salting techniques?\n",
    "You are given the following sample datasets:\n",
    "\n",
    "sales_df (Fact Table - Large Dataset, Highly Skewed on store_id)\n",
    "Your task is to perform an optimized join between sales_df and store_df on store_id, ensuring that the skewness does not degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8801c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|store_id|product_id|amount|\n",
      "+--------+----------+------+\n",
      "|     101|      P001|   100|\n",
      "|     101|      P002|   200|\n",
      "|     101|      P003|   150|\n",
      "|     102|      P004|   300|\n",
      "|     103|      P005|   400|\n",
      "|     101|      P006|   500|\n",
      "|     104|      P007|   250|\n",
      "+--------+----------+------+\n",
      "\n",
      "+--------+----------+\n",
      "|store_id|store_name|\n",
      "+--------+----------+\n",
      "|     101|   Walmart|\n",
      "|     102|    Target|\n",
      "|     103|    Costco|\n",
      "|     104|   BestBuy|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = [ (101, \"P001\", 100), (101, \"P002\", 200), (101, \"P003\", 150), (102, \"P004\", 300), \n",
    "              (103, \"P005\", 400), (101, \"P006\", 500), (104, \"P007\", 250) ] \n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"store_id\", \"product_id\", \"amount\"]) \n",
    "sales_df.show()\n",
    "\n",
    "store_data = [(101, \"Walmart\"), (102, \"Target\"), (103, \"Costco\"), (104, \"BestBuy\")] \n",
    "store_df = spark.createDataFrame(store_data, [\"store_id\", \"store_name\"]) \n",
    "store_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a808948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----+---------------+\n",
      "|store_id|product_id|amount|salt|salted_store_id|\n",
      "+--------+----------+------+----+---------------+\n",
      "|     101|      P001|   100|   2|          101_2|\n",
      "|     101|      P002|   200|   0|          101_0|\n",
      "|     101|      P003|   150|   2|          101_2|\n",
      "|     102|      P004|   300|   0|          102_0|\n",
      "|     103|      P005|   400|   1|          103_1|\n",
      "|     101|      P006|   500|   2|          101_2|\n",
      "|     104|      P007|   250|   1|          104_1|\n",
      "+--------+----------+------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Adding Salt to skewed 'sales_df'\n",
    "\n",
    "num_salt_keys = 3  # Define the range of salt keys \n",
    "\n",
    "sales_df_salted = sales_df.withColumn('salt', floor(rand() * num_salt_keys))\\\n",
    "                          .withColumn('salted_store_id', concat_ws(\"_\", col('store_id'), col('salt')))\n",
    "sales_df_salted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d6b2341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----+---------------+\n",
      "|store_id|store_name|salt|salted_store_id|\n",
      "+--------+----------+----+---------------+\n",
      "|     101|   Walmart|   0|          101_0|\n",
      "|     101|   Walmart|   1|          101_1|\n",
      "|     101|   Walmart|   2|          101_2|\n",
      "|     102|    Target|   0|          102_0|\n",
      "|     102|    Target|   1|          102_1|\n",
      "|     102|    Target|   2|          102_2|\n",
      "|     103|    Costco|   0|          103_0|\n",
      "|     103|    Costco|   1|          103_1|\n",
      "|     103|    Costco|   2|          103_2|\n",
      "|     104|   BestBuy|   0|          104_0|\n",
      "|     104|   BestBuy|   1|          104_1|\n",
      "|     104|   BestBuy|   2|          104_2|\n",
      "+--------+----------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Expanding 'store_df' for Join compatibility \n",
    "\n",
    "expanded_store_df = store_df.crossJoin(spark.range(0, num_salt_keys).toDF('salt'))\\\n",
    "                    .withColumn('salted_store_id', concat_ws('_', col('store_id'), col('salt')))\n",
    "expanded_store_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7115ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+--------+----------+\n",
      "|store_id|product_id|amount|store_id|store_name|\n",
      "+--------+----------+------+--------+----------+\n",
      "|     101|      P002|   200|     101|   Walmart|\n",
      "|     101|      P001|   100|     101|   Walmart|\n",
      "|     101|      P003|   150|     101|   Walmart|\n",
      "|     101|      P006|   500|     101|   Walmart|\n",
      "|     102|      P004|   300|     102|    Target|\n",
      "|     103|      P005|   400|     103|    Costco|\n",
      "|     104|      P007|   250|     104|   BestBuy|\n",
      "+--------+----------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Performing the Optimized Join on Salted Keys\n",
    "\n",
    "joined_df = sales_df_salted.join(expanded_store_df,'salted_store_id', 'inner').drop('salted_store_id', 'salt')\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/company/seekho-bigdata-institute/posts/?feedView=all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
