{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf32c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba4c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a89eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x212c3a07210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('pr').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30bf2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------------------+\n",
      "|uid| pid|qunt|              pur_dt|\n",
      "+---+----+----+--------------------+\n",
      "|333|1122|   9|2022-02-06T01:00:...|\n",
      "|333|1122|  10|2022-02-06T02:00:...|\n",
      "|536|1435|  10|2022-03-02T08:40:...|\n",
      "|536|3223|   5|2022-03-02T09:33:...|\n",
      "|536|3223|   6|2022-01-11T12:33:...|\n",
      "|827|2452|  45|2022-03-02T00:00:...|\n",
      "|827|3585|  35|2022-02-20T14:05:...|\n",
      "+---+----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the customers who brought the same product more than once but on different days\n",
    "# ( if same product is purchased multiple times but on same date shouldn't be counted)\n",
    "\n",
    "data = [(333, 1122, 9, '2022-02-06T01:00:00.000+00:00'),\n",
    "        (333,1122,10,'2022-02-06T02:00:00.000+00:00'), \n",
    "        (536,1435, 10,'2022-03-02T08:40:00.000+00:00'),\n",
    "        (536,3223,5,'2022-03-02T09:33:28.000+00:00'),\n",
    "        (536, 3223, 6,'2022-01-11T12:33:44.000+00:00'),\n",
    "        (827, 2452, 45,'2022-03-02T00:00:00.000+00:00'), \n",
    "        (827, 3585, 35,'2022-02-20T14:05:26.000+00:00')]\n",
    "df = spark.createDataFrame(data = data, schema=['uid', 'pid', 'qunt', 'pur_dt'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0882e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+-----+\n",
      "|uid| pid|        dt|count|\n",
      "+---+----+----------+-----+\n",
      "|333|1122|2022-02-06|    2|\n",
      "|536|1435|2022-03-02|    1|\n",
      "|536|3223|2022-03-02|    1|\n",
      "|536|3223|2022-01-11|    1|\n",
      "|827|2452|2022-03-02|    1|\n",
      "|827|3585|2022-02-20|    1|\n",
      "+---+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('dt', to_date('pur_dt')).groupBy('uid', 'pid', 'dt').count()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7f5e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|uid| pid|count|\n",
      "+---+----+-----+\n",
      "|536|3223|    2|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('uid', 'pid').count().filter(col('count') >= 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fdba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| Id|           name|\n",
      "+---+---------------+\n",
      "|  1|sagar-prajapati|\n",
      "|  2|      alex-john|\n",
      "|  3|      john cena|\n",
      "|  4|        kim joe|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In the given dataset, names contain for some names and space for some names, extract the first name and last name \n",
    "\n",
    "data = [(1, 'sagar-prajapati'), (2, 'alex-john'), (3, 'john cena'), (4, 'kim joe')]\n",
    "schema = ['Id','name']\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d756944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "| Id|           name|              A|\n",
      "+---+---------------+---------------+\n",
      "|  1|sagar-prajapati|sagar prajapati|\n",
      "|  2|      alex-john|      alex john|\n",
      "|  3|      john cena|      john cena|\n",
      "|  4|        kim joe|        kim joe|\n",
      "+---+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_regexp = \"((,)?\\s|[-])\"\n",
    "df1 = df.withColumn(\"A\", regexp_replace(col(\"name\"), replace_regexp, \" \"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ceb8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------+---------+\n",
      "| Id|           name|First_name|Last_Name|\n",
      "+---+---------------+----------+---------+\n",
      "|  1|sagar-prajapati|     sagar|prajapati|\n",
      "|  2|      alex-john|      alex|     john|\n",
      "|  3|      john cena|      john|     cena|\n",
      "|  4|        kim joe|       kim|      joe|\n",
      "+---+---------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn('First_name', split(df1['A'], ' ').getItem(0)).withColumn('Last_Name', split(df1['A'], ' ').getItem(1))\n",
    "df2.drop('A').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d55ffe11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "792957f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+\n",
      "| Id|           name|                 A|\n",
      "+---+---------------+------------------+\n",
      "|  1|sagar-prajapati|[sagar, prajapati]|\n",
      "|  2|      alex-john|      [alex, john]|\n",
      "|  3|      john cena|      [john, cena]|\n",
      "|  4|        kim joe|        [kim, joe]|\n",
      "+---+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_regex = \"((,)?\\s|[-])\"\n",
    "df1 = df.withColumn(\"A\", split(col(\"name\"), split_regex))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "010bc83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|person1|person2|call_duration|\n",
      "+-------+-------+-------------+\n",
      "|     10|     20|           58|\n",
      "|     20|     10|           12|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "|     40|     30|          500|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call_duration \n",
    "\n",
    "data = [(10, 20, 58), (20,10,12), (10,30, 20),(30,40,100),(30, 40, 200), (30, 40, 200), (40, 30, 500)]\n",
    "df = spark.createDataFrame(data = data, schema=['person1', 'person2', 'call_duration'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c678489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|person1|person2|call_duration|\n",
      "+-------+-------+-------------+\n",
      "|     10|     20|           58|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "|     10|     20|           58|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.alias('t1').unionAll(df.alias('t2')).filter(col('person1') < col('person2'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f662b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------+\n",
      "|person1|person2|call_count|total_duration|\n",
      "+-------+-------+----------+--------------+\n",
      "|     10|     20|         2|           116|\n",
      "|     10|     30|         2|            40|\n",
      "|     30|     40|         6|          1000|\n",
      "+-------+-------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy('person1','person2').agg(count(col('call_duration')).alias('call_count'), sum(col('call_duration')).alias('total_duration'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96b2c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| sub|\n",
      "+---+----+\n",
      "|  1|MATH|\n",
      "|  2|MATH|\n",
      "|  4|CHEM|\n",
      "|  5|MATH|\n",
      "|  2| ENG|\n",
      "|  3| PHY|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the teachers who teaches only math and not any other subject \n",
    "\n",
    "data = [(1, \"MATH\"), (2,'MATH'), (4, 'CHEM'),(5, 'MATH'),(2, 'ENG'), (3, 'PHY')]\n",
    "df = spark.createDataFrame(data = data, schema=['id', 'sub'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3051b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  4|    1|\n",
      "|  5|    1|\n",
      "|  3|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('id').count().filter(col('count') == 1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b512244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| sub|\n",
      "+---+----+\n",
      "|  1|MATH|\n",
      "|  5|MATH|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df1, df.id == df1.id, how = 'inner').filter(df['sub'] == 'MATH').select(df['*']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9dbfc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+\n",
      "|COMPANY|YEAR|REVENUE|\n",
      "+-------+----+-------+\n",
      "|    ABC|2000|    100|\n",
      "|    ABC|2001|    110|\n",
      "|    ABC|2002|    120|\n",
      "|    XYZ|2000|    100|\n",
      "|    XYZ|2001|     90|\n",
      "|    XYZ|2002|    120|\n",
      "|    RXC|2000|    500|\n",
      "|    RXC|2001|    400|\n",
      "|    RXC|2002|    600|\n",
      "|    RXC|2003|    800|\n",
      "+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out the companies where revenue has only increased over the years and there was no decrease at all for any point.\n",
    "\n",
    "data = [('ABC', 2000, 100),\n",
    "('ABC', 2001, 110),\n",
    "('ABC', 2002, 120),\n",
    "('XYZ', 2000, 100),\n",
    "('XYZ', 2001, 90),\n",
    "('XYZ', 2002, 120),\n",
    "('RXC', 2000, 500),\n",
    "('RXC', 2001, 400),\n",
    "('RXC', 2002, 600),\n",
    "('RXC', 2003, 800)]\n",
    "schema = StructType([StructField('COMPANY', StringType(), True),\n",
    "                     StructField('YEAR', IntegerType(), True),\n",
    "                     StructField('REVENUE', IntegerType(), True)]) \n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "167fafd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+----+\n",
      "|COMPANY|YEAR|REVENUE| lag|\n",
      "+-------+----+-------+----+\n",
      "|    ABC|2000|    100| 100|\n",
      "|    ABC|2001|    110|  10|\n",
      "|    ABC|2002|    120|  10|\n",
      "|    RXC|2000|    500| 500|\n",
      "|    RXC|2001|    400|-100|\n",
      "|    RXC|2002|    600| 200|\n",
      "|    RXC|2003|    800| 200|\n",
      "|    XYZ|2000|    100| 100|\n",
      "|    XYZ|2001|     90| -10|\n",
      "|    XYZ|2002|    120|  30|\n",
      "+-------+----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy('COMPANY').orderBy('YEAR')\n",
    "df1 = df.withColumn('lag', col('REVENUE')-lag(col('REVENUE'), 1, 0).over(window))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "396390e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|COMPANY|diff|\n",
      "+-------+----+\n",
      "|    ABC|  10|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy('COMPANY').agg(min(col('lag')).alias('diff')).filter(col('diff') > 0)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d65d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     Movie|       Type|Rating|\n",
      "+---+----------+-----------+------+\n",
      "|  1|       war|   great ed|   8.9|\n",
      "|  2|   science|    fiction|   8.5|\n",
      "|  3|     irish|     boring|   6.2|\n",
      "|  4|  Ice song|    fantacy|   8.6|\n",
      "|  5|house card|interesting|   9.1|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lIst down the movies with an odd ID and which is not boring and order by id desc \n",
    "\n",
    "data = [(1, 'war', 'great ed',8.9),\n",
    "    (2,'science','fiction',8.5),\n",
    "    (3,'irish', 'boring', 6.2),\n",
    "    (4, 'Ice song', 'fantacy', 8.6),\n",
    "    (5, \"house card\", 'interesting', 9.1)]\n",
    "sch = ['ID', 'Movie', 'Type', 'Rating']\n",
    "df = spark.createDataFrame(data=data, schema=sch)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df7f6e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     Movie|       Type|Rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|house card|interesting|   9.1|\n",
      "|  1|       war|   great ed|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.filter(((df['ID'] %2 ) != 0 ) & (col('Type') != 'boring')).orderBy(col('ID').desc())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "302ce2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----+\n",
      "| id| name|salary| mid|\n",
      "+---+-----+------+----+\n",
      "|  1| John|  6000|   4|\n",
      "|  2|Kevin| 11000|   4|\n",
      "|  3|  Bob|  8000|   5|\n",
      "|  4|Laura|  9000|NULL|\n",
      "|  5|Sarah| 10000|NULL|\n",
      "+---+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the employees earning more than managers \n",
    "\n",
    "data = [(1, \"John\", 6000, 4), (2,'Kevin',11000,4), (3, 'Bob',8000, 5),(4, 'Laura',9000,None),(5, 'Sarah',10000, None)]\n",
    "df = spark.createDataFrame(data = data, schema=['id', 'name', 'salary','mid'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6169934e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+---+\n",
      "| id| name|salary|mid|\n",
      "+---+-----+------+---+\n",
      "|  2|Kevin| 11000|  4|\n",
      "+---+-----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('emp').join(df.alias('mgr'), col('emp.mid')  == col('mgr.id'), 'inner').filter(col('emp.salary') > col('mgr.salary'))\\\n",
    ".select(col('emp.id'), col('emp.name'), col('emp.salary'), col('emp.mid')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3cc74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b9d0d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   team|points|\n",
      "+-------+------+\n",
      "|  Mavs^|    18|\n",
      "|  Ne%ts|    33|\n",
      "|Hawk**s|    12|\n",
      "|  Mavs@|    15|\n",
      "| Hawks!|    19|\n",
      "| (Cavs)|    24|\n",
      "|  Magic|    28|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove special charactors\n",
    "\n",
    "data = [['Mavs^', 18], \n",
    "        ['Ne%ts', 33], \n",
    "        ['Hawk**s', 12], \n",
    "        ['Mavs@', 15], \n",
    "        ['Hawks!', 19],\n",
    "        ['(Cavs)', 24],\n",
    "        ['Magic', 28]] \n",
    "columns = ['team', 'points'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "399c47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|   team|points|team2|\n",
      "+-------+------+-----+\n",
      "|  Mavs^|    18| Mavs|\n",
      "|  Ne%ts|    33| Nets|\n",
      "|Hawk**s|    12|Hawks|\n",
      "|  Mavs@|    15| Mavs|\n",
      "| Hawks!|    19|Hawks|\n",
      "| (Cavs)|    24| Cavs|\n",
      "|  Magic|    28|Magic|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('team2', regexp_replace('team', '[^a-zA-Z0-9]', ''))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e03fcce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+-----------------+\n",
      "|   team|points|team2|count_of_spe_char|\n",
      "+-------+------+-----+-----------------+\n",
      "|  Mavs^|    18| Mavs|                1|\n",
      "|  Ne%ts|    33| Nets|                1|\n",
      "|Hawk**s|    12|Hawks|                2|\n",
      "|  Mavs@|    15| Mavs|                1|\n",
      "| Hawks!|    19|Hawks|                1|\n",
      "| (Cavs)|    24| Cavs|                2|\n",
      "|  Magic|    28|Magic|                0|\n",
      "+-------+------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn('count_of_spe_char',length(col('team')) - length(col('team2'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698d1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc9b8e6",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/company/seekho-bigdata-institute/posts/?feedView=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e200f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65426db5",
   "metadata": {},
   "source": [
    "29 The columns contain different types of data, including numeric, categorical, and string values. Your objective is to:\n",
    "\n",
    "1. Fill numeric columns with the median value.\n",
    "2. Fill categorical columns with the most frequent value.\n",
    "3. Fill string columns with \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3b6282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Customer_ID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Last_Visit: string (nullable = true)\n",
      " |-- Purchase_Amount: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+-----------+----+------+------+----------+---------------+\n",
      "|Customer_ID| Age|Region|Gender|Last_Visit|Purchase_Amount|\n",
      "+-----------+----+------+------+----------+---------------+\n",
      "|          1|  25| North|     M|2025-01-01|            150|\n",
      "|          2|NULL|  East|  NULL|2025-01-02|           NULL|\n",
      "|          3|  30| South|     F|      NULL|            200|\n",
      "|          4|  22|  NULL|     M|2025-01-03|            180|\n",
      "|          5|  28|  West|     F|      NULL|           NULL|\n",
      "+-----------+----+------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, 25, 'North', 'M', '2025-01-01', 150),\n",
    "(2, None, 'East', None, '2025-01-02', None),\n",
    "(3, 30, 'South', 'F', None, 200),\n",
    "(4, 22, None, 'M', '2025-01-03', 180),\n",
    "(5, 28, 'West', 'F', None, None), ]\n",
    "schema = StructType([StructField('Customer_ID', IntegerType\n",
    "                                 (), True),\n",
    "                    StructField('Age', IntegerType(), True),\n",
    "                    StructField('Region', StringType(), True),\n",
    "                    StructField('Gender', StringType(), True),\n",
    "                     StructField('Last_Visit', StringType(), True),\n",
    "                     StructField('Purchase_Amount', IntegerType(), True),\n",
    "                    ])\n",
    "\n",
    "#columns = ['Customer_ID', 'Age', 'Region', 'Gender', 'Last_Visit', 'Purchase_Amount'] \n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(df.printSchema())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82d07d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_types:  [('Customer_ID', 'int'), ('Age', 'int'), ('Region', 'string'), ('Gender', 'string'), ('Last_Visit', 'string'), ('Purchase_Amount', 'int')]\n",
      "+-----------+---+-------+-------+----------+---------------+\n",
      "|Customer_ID|Age| Region| Gender|Last_Visit|Purchase_Amount|\n",
      "+-----------+---+-------+-------+----------+---------------+\n",
      "|          1| 25|  North|      M|2025-01-01|            150|\n",
      "|          2| 25|   East|Unknown|2025-01-02|            180|\n",
      "|          3| 30|  South|      F|   Unknown|            200|\n",
      "|          4| 22|Unknown|      M|2025-01-03|            180|\n",
      "|          5| 28|   West|      F|   Unknown|            180|\n",
      "+-----------+---+-------+-------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to fill missing values dynamically\n",
    "def fill_missing_values(df):\n",
    "    \n",
    "    column_types = df.dtypes\n",
    "    print('column_types: ', column_types)\n",
    "    \n",
    "    # loop through each column based on type\n",
    "    for column, dtype in column_types:\n",
    "        \n",
    "        if dtype == 'int' or dtype == 'double' or dtype == 'long':\n",
    "            median_value = df.approxQuantile(column, [0.5], 0)[0]\n",
    "            df = df.fillna({column: median_value})\n",
    "            \n",
    "        elif dtype == 'string':\n",
    "            df = df.fillna({column: 'Unknown'})\n",
    "            \n",
    "        else:\n",
    "            df = df.fillna({column: 'Unknown'})\n",
    "        \n",
    "    return df\n",
    "filled_df = fill_missing_values(df)\n",
    "filled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40796307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4bc1191",
   "metadata": {},
   "source": [
    "𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧 34\n",
    "\n",
    "You are given a dataset of sales transactions for multiple stores and products.\n",
    "- Calculate the percentage contribution of each product's sales to the total sales of its store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee0516c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|StoreID|Product|Sales|\n",
      "+-------+-------+-----+\n",
      "|     S1|     P1|  100|\n",
      "|     S1|     P2|  200|\n",
      "|     S1|     P3|  300|\n",
      "|     S2|     P1|  400|\n",
      "|     S2|     P2|  100|\n",
      "|     S2|     P3|  500|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"S1\", \"P1\", 100), (\"S1\", \"P2\", 200),\n",
    "(\"S1\", \"P3\", 300), (\"S2\", \"P1\", 400),\n",
    "(\"S2\", \"P2\", 100), (\"S2\", \"P3\", 500) ]\n",
    "columns = [\"StoreID\", \"Product\", \"Sales\"] \n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9220d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|StoreID|Total_Sales|\n",
      "+-------+-----------+\n",
      "|     S1|        600|\n",
      "|     S2|       1000|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('StoreID').agg(sum('Sales').alias('Total_Sales'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "062a90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----------+\n",
      "|StoreID|Product|Sales|Total_Sales|\n",
      "+-------+-------+-----+-----------+\n",
      "|     S1|     P1|  100|        600|\n",
      "|     S1|     P2|  200|        600|\n",
      "|     S1|     P3|  300|        600|\n",
      "|     S2|     P1|  400|       1000|\n",
      "|     S2|     P2|  100|       1000|\n",
      "|     S2|     P3|  500|       1000|\n",
      "+-------+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = df.join(df1, on='StoreID', how= 'inner')\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284210dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----------+------+\n",
      "|StoreID|Product|Sales|Total_Sales|percnt|\n",
      "+-------+-------+-----+-----------+------+\n",
      "|     S1|     P1|  100|        600| 16.67|\n",
      "|     S1|     P2|  200|        600| 33.33|\n",
      "|     S1|     P3|  300|        600|  50.0|\n",
      "|     S2|     P1|  400|       1000|  40.0|\n",
      "|     S2|     P2|  100|       1000|  10.0|\n",
      "|     S2|     P3|  500|       1000|  50.0|\n",
      "+-------+-------+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.withColumn('percnt', round((col('Sales') / col('Total_sales'))*100,2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d5967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ff55a6f",
   "metadata": {},
   "source": [
    "𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧 35\n",
    "\n",
    "You are working as a Data Engineer at a retail company. The marketing team has provided a dataset of customer purchases to analyze the relationship between the amount spent on advertisements and the revenue generated. \n",
    "- Using PySpark, compute the correlation between the \"Ad_Spend\" and \"Revenue\" columns to determine if there's a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df8646ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+\n",
      "|Customer_ID|Ad_Spend|Revenue|\n",
      "+-----------+--------+-------+\n",
      "|       C001|    2000|  25000|\n",
      "|       C002|    1500|  23000|\n",
      "|       C003|    3000|  40000|\n",
      "|       C004|    1200|  18000|\n",
      "|       C005|    2500|  30000|\n",
      "+-----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ StructField(\"Customer_ID\", StringType(), True), StructField(\"Ad_Spend\", IntegerType(), True), StructField(\"Revenue\", IntegerType(), True) ])\n",
    "data = [ (\"C001\", 2000, 25000), (\"C002\", 1500, 23000),\n",
    "(\"C003\", 3000, 40000), (\"C004\", 1200, 18000),\n",
    "(\"C005\", 2500, 30000) ] \n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3febbf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between ad_spend and revenue is: 0.9704535552410213\n"
     ]
    }
   ],
   "source": [
    "correlation = df.stat.corr('Ad_Spend', 'Revenue')\n",
    "print('The correlation between ad_spend and revenue is:' ,correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40323a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8911103",
   "metadata": {},
   "source": [
    "𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧 36\n",
    "\n",
    "You are given a large e-commerce transaction dataset stored in a partitioned format based on country. \n",
    "- Count the distinct number of products purchased (product_id) for each customer_id in every country. The result should include the country, customer ID, and the distinct product count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56fad9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+\n",
      "|country|customer_id|product_id|\n",
      "+-------+-----------+----------+\n",
      "|    USA|        101|      P001|\n",
      "|    USA|        101|      P002|\n",
      "|    USA|        101|      P001|\n",
      "|    USA|        102|      P003|\n",
      "|    USA|        102|      P003|\n",
      "|     UK|        201|      P004|\n",
      "|     UK|        201|      P005|\n",
      "|     UK|        202|      P004|\n",
      "|     UK|        202|      P005|\n",
      "|     UK|        202|      P004|\n",
      "+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"USA\", 101, \"P001\"), \n",
    "(\"USA\", 101, \"P002\"), (\"USA\", 101, \"P001\"), \n",
    "(\"USA\", 102, \"P003\"), (\"USA\", 102, \"P003\"), \n",
    "(\"UK\", 201, \"P004\"), (\"UK\", 201, \"P005\"), \n",
    "(\"UK\", 202, \"P004\"), (\"UK\", 202, \"P005\"), (\"UK\", 202, \"P004\") ]\n",
    "\n",
    "columns = [\"country\", \"customer_id\", \"product_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "575b4c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------------+\n",
      "|country|customer_id|count(DISTINCT product_id)|\n",
      "+-------+-----------+--------------------------+\n",
      "|    USA|        101|                         2|\n",
      "|     UK|        202|                         2|\n",
      "|     UK|        201|                         2|\n",
      "|    USA|        102|                         1|\n",
      "+-------+-----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('country', 'customer_id').agg(countDistinct(col('product_id'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94eb9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63194096",
   "metadata": {},
   "source": [
    "$Broadcast$ the smaller DataFrame (product_data). 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9400748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----------+\n",
      "|sale_id|product_id|quantity| sale_date|\n",
      "+-------+----------+--------+----------+\n",
      "|      1|       101|       5|2025-01-01|\n",
      "|      2|       102|       3|2025-01-02|\n",
      "|      3|       103|       2|2025-01-03|\n",
      "|      4|       101|       1|2025-01-04|\n",
      "|      5|       104|       4|2025-01-05|\n",
      "|      6|       105|       6|2025-01-06|\n",
      "+-------+----------+--------+----------+\n",
      "\n",
      "+----------+------------+-----------+-----+\n",
      "|product_id|product_name|   category|price|\n",
      "+----------+------------+-----------+-----+\n",
      "|       101|      Laptop|Electronics| 1000|\n",
      "|       102|       Phone|Electronics|  500|\n",
      "|       103|  Headphones|Accessories|  150|\n",
      "|       104|      Tablet|Electronics|  600|\n",
      "|       105|  Smartwatch|Accessories|  200|\n",
      "+----------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = [ (1, 101, 5, '2025-01-01'), (2, 102, 3, '2025-01-02'),\n",
    "(3, 103, 2, '2025-01-03'), (4, 101, 1, '2025-01-04'),\n",
    "(5, 104, 4, '2025-01-05'), (6, 105, 6, '2025-01-06'), ]\n",
    "\n",
    "# product_data (Small DataFrame)\n",
    "\n",
    "product_data = [ (101, 'Laptop', 'Electronics', 1000),\n",
    "(102, 'Phone', 'Electronics', 500),\n",
    "(103, 'Headphones', 'Accessories', 150),\n",
    "(104, 'Tablet', 'Electronics', 600),\n",
    "(105, 'Smartwatch', 'Accessories', 200), ] \n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, ['sale_id', 'product_id','quantity', 'sale_date'])\n",
    "product_df= spark.createDataFrame(product_data, ['product_id', 'product_name', 'category', 'price'])\n",
    "sales_df.show()\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2738e9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+----------+------------+-----------+-----+\n",
      "|product_id|sale_id|quantity|sale_date |product_name|category   |price|\n",
      "+----------+-------+--------+----------+------------+-----------+-----+\n",
      "|101       |1      |5       |2025-01-01|Laptop      |Electronics|1000 |\n",
      "|102       |2      |3       |2025-01-02|Phone       |Electronics|500  |\n",
      "|103       |3      |2       |2025-01-03|Headphones  |Accessories|150  |\n",
      "|101       |4      |1       |2025-01-04|Laptop      |Electronics|1000 |\n",
      "|104       |5      |4       |2025-01-05|Tablet      |Electronics|600  |\n",
      "|105       |6      |6       |2025-01-06|Smartwatch  |Accessories|200  |\n",
      "+----------+-------+--------+----------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "prod_df_broadcast = broadcast(product_df)\n",
    "\n",
    "joined_df = sales_df.join(prod_df_broadcast, on='product_id', how='inner')\n",
    "joined_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa7402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "170ccafe",
   "metadata": {},
   "source": [
    "𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧 39\n",
    "\n",
    "You are working with large datasets in PySpark and need to join two DataFrames. However, one of the tables has highly skewed data, causing performance issues due to data shuffling. How would you optimize this join using salting techniques?\n",
    "You are given the following sample datasets:\n",
    "\n",
    "sales_df (Fact Table - Large Dataset, Highly Skewed on store_id)\n",
    "Your task is to perform an optimized join between sales_df and store_df on store_id, ensuring that the skewness does not degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d478d53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|store_id|product_id|amount|\n",
      "+--------+----------+------+\n",
      "|     101|      P001|   100|\n",
      "|     101|      P002|   200|\n",
      "|     101|      P003|   150|\n",
      "|     102|      P004|   300|\n",
      "|     103|      P005|   400|\n",
      "|     101|      P006|   500|\n",
      "|     104|      P007|   250|\n",
      "+--------+----------+------+\n",
      "\n",
      "+--------+----------+\n",
      "|store_id|store_name|\n",
      "+--------+----------+\n",
      "|     101|   Walmart|\n",
      "|     102|    Target|\n",
      "|     103|    Costco|\n",
      "|     104|   BestBuy|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = [ (101, \"P001\", 100), (101, \"P002\", 200), (101, \"P003\", 150), (102, \"P004\", 300), \n",
    "              (103, \"P005\", 400), (101, \"P006\", 500), (104, \"P007\", 250) ] \n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"store_id\", \"product_id\", \"amount\"]) \n",
    "sales_df.show()\n",
    "\n",
    "store_data = [(101, \"Walmart\"), (102, \"Target\"), (103, \"Costco\"), (104, \"BestBuy\")] \n",
    "store_df = spark.createDataFrame(store_data, [\"store_id\", \"store_name\"]) \n",
    "store_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9acce019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----+---------------+\n",
      "|store_id|product_id|amount|salt|salted_store_id|\n",
      "+--------+----------+------+----+---------------+\n",
      "|     101|      P001|   100|   1|          101_1|\n",
      "|     101|      P002|   200|   0|          101_0|\n",
      "|     101|      P003|   150|   0|          101_0|\n",
      "|     102|      P004|   300|   0|          102_0|\n",
      "|     103|      P005|   400|   0|          103_0|\n",
      "|     101|      P006|   500|   2|          101_2|\n",
      "|     104|      P007|   250|   0|          104_0|\n",
      "+--------+----------+------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Adding Salt to skewed 'sales_df'\n",
    "\n",
    "num_salt_keys = 3  # Define the range of salt keys \n",
    "\n",
    "sales_df_salted = sales_df.withColumn('salt', floor(rand() * num_salt_keys))\\\n",
    "                          .withColumn('salted_store_id', concat_ws(\"_\", col('store_id'), col('salt')))\n",
    "sales_df_salted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c81b3f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----+---------------+\n",
      "|store_id|store_name|salt|salted_store_id|\n",
      "+--------+----------+----+---------------+\n",
      "|     101|   Walmart|   0|          101_0|\n",
      "|     101|   Walmart|   1|          101_1|\n",
      "|     101|   Walmart|   2|          101_2|\n",
      "|     102|    Target|   0|          102_0|\n",
      "|     102|    Target|   1|          102_1|\n",
      "|     102|    Target|   2|          102_2|\n",
      "|     103|    Costco|   0|          103_0|\n",
      "|     103|    Costco|   1|          103_1|\n",
      "|     103|    Costco|   2|          103_2|\n",
      "|     104|   BestBuy|   0|          104_0|\n",
      "|     104|   BestBuy|   1|          104_1|\n",
      "|     104|   BestBuy|   2|          104_2|\n",
      "+--------+----------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Expanding 'store_df' for Join compatibility \n",
    "\n",
    "expanded_store_df = store_df.crossJoin(spark.range(0, num_salt_keys).toDF('salt'))\\\n",
    "                    .withColumn('salted_store_id', concat_ws('_', col('store_id'), col('salt')))\n",
    "expanded_store_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebd7faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+--------+----------+\n",
      "|store_id|product_id|amount|store_id|store_name|\n",
      "+--------+----------+------+--------+----------+\n",
      "|     101|      P002|   200|     101|   Walmart|\n",
      "|     101|      P003|   150|     101|   Walmart|\n",
      "|     101|      P001|   100|     101|   Walmart|\n",
      "|     101|      P006|   500|     101|   Walmart|\n",
      "|     102|      P004|   300|     102|    Target|\n",
      "|     103|      P005|   400|     103|    Costco|\n",
      "|     104|      P007|   250|     104|   BestBuy|\n",
      "+--------+----------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Performing the Optimized Join on Salted Keys\n",
    "\n",
    "joined_df = sales_df_salted.join(expanded_store_df,'salted_store_id', 'inner').drop('salted_store_id', 'salt')\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e8244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9f03fd0",
   "metadata": {},
   "source": [
    "\"\"\"𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧 40\n",
    "\n",
    "You are working as a Data Engineer at a fintech company. Your team is working on integrating two datasets:\n",
    "\n",
    "1. Customer Transactions Data (transactions_df) - Contains customer transactions with columns: customer_id, txn_id, amount, and txn_date. \n",
    "\n",
    "2. Customer Profile Data (profile_df) - Contains customer information with columns: customer_id, name, age, and txn_id (latest transaction ID for reference).\n",
    "\n",
    " The requirement is to merge these two DataFrames on customer_id while keeping track of:\n",
    "\n",
    "Conflicting column names (txn_id) should be renamed properly.\n",
    "\n",
    "If a customer exists in profile_df but not in transactions_df, the row should still be present with NULL values for transaction-related columns.\n",
    "\n",
    "Your task is to write an optimized PySpark code to achieve this.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0224c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----------+\n",
      "|customer_id|txn_id|amount|  txn_date|\n",
      "+-----------+------+------+----------+\n",
      "|        101|  T001|   500|2024-08-10|\n",
      "|        102|  T002|  1200|2024-08-09|\n",
      "|        103|  T003|   300|2024-08-08|\n",
      "|        104|  T004|   450|2024-08-07|\n",
      "+-----------+------+------+----------+\n",
      "\n",
      "+-----------+----+---+------+\n",
      "|customer_id|name|age|txn_id|\n",
      "+-----------+----+---+------+\n",
      "|        101|John| 30|  T001|\n",
      "|        102|Emma| 27|  T005|\n",
      "|        103|Alex| 35|  T003|\n",
      "|        105| Sam| 40|  T006|\n",
      "+-----------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_data = [ (101, \"T001\", 500, \"2024-08-10\"), (102, \"T002\", 1200, \"2024-08-09\"), \n",
    "                     (103, \"T003\", 300, \"2024-08-08\"), (104, \"T004\", 450, \"2024-08-07\"), ] \n",
    "\n",
    "profile_data = [ (101, \"John\", 30, \"T001\"), (102, \"Emma\", 27, \"T005\"), \n",
    "                (103, \"Alex\", 35, \"T003\"), (105, \"Sam\", 40, \"T006\"), ]\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_data, [\"customer_id\", \"txn_id\", \"amount\", \"txn_date\"])\n",
    "transactions_df.show()\n",
    "\n",
    "profile_df = spark.createDataFrame(profile_data, [\"customer_id\", \"name\", \"age\", \"txn_id\"])\n",
    "profile_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764ca0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+-----------+\n",
      "|customer_id|name|age|last_txn_id|\n",
      "+-----------+----+---+-----------+\n",
      "|        101|John| 30|       T001|\n",
      "|        102|Emma| 27|       T005|\n",
      "|        103|Alex| 35|       T003|\n",
      "|        105| Sam| 40|       T006|\n",
      "+-----------+----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df = profile_df.withColumnRenamed('txn_id', 'last_txn_id')\n",
    "profile_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0597d463",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+-----------+------+------+----------+\n",
      "|customer_id|name| age|last_txn_id|txn_id|amount|  txn_date|\n",
      "+-----------+----+----+-----------+------+------+----------+\n",
      "|        101|John|  30|       T001|  T001|   500|2024-08-10|\n",
      "|        102|Emma|  27|       T005|  T002|  1200|2024-08-09|\n",
      "|        103|Alex|  35|       T003|  T003|   300|2024-08-08|\n",
      "|        104|NULL|NULL|       NULL|  T004|   450|2024-08-07|\n",
      "|        105| Sam|  40|       T006|  NULL|  NULL|      NULL|\n",
      "+-----------+----+----+-----------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df.join(transactions_df, on = \"customer_id\", how = \"full_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ab082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5d0009c",
   "metadata": {},
   "source": [
    "𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧 42\n",
    "\n",
    "You are given an employee dataset containing information about employees and their managers. Each employee has a manager_id that refers to another employee in the same table. Your task is to use self-join to find hierarchical relationships between employees, such as finding all employees under a specific manager or the reporting hierarchy of an employee.\n",
    "\n",
    "Interview Task\n",
    "- Write a PySpark self-join query to find the direct reports of each manager. Additionally, extend the logic to find all hierarchical relationships up to any level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd26875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+\n",
      "|employee_id|employee_name|manager_id|\n",
      "+-----------+-------------+----------+\n",
      "|          1|        Alice|      NULL|\n",
      "|          2|          Bob|         1|\n",
      "|          3|      Charlie|         1|\n",
      "|          4|        David|         2|\n",
      "|          5|          Eva|         2|\n",
      "|          6|        Frank|         3|\n",
      "|          7|        Grace|         3|\n",
      "+-----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, \"Alice\", None), (2, \"Bob\", 1),\n",
    "(3, \"Charlie\", 1), (4, \"David\", 2),\n",
    "(5, \"Eva\", 2), (6, \"Frank\", 3), (7, \"Grace\", 3) ]\n",
    "\n",
    "columns = [\"employee_id\", \"employee_name\", \"manager_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4516b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|employee_name|employee_name|\n",
      "+-------------+-------------+\n",
      "|         NULL|        Alice|\n",
      "|        Alice|          Bob|\n",
      "|        Alice|      Charlie|\n",
      "|          Bob|        David|\n",
      "|          Bob|          Eva|\n",
      "|      Charlie|        Frank|\n",
      "|      Charlie|        Grace|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('emp').join(df.alias('mgr'), col('mgr.employee_id') == col('emp.manager_id'), 'left')\\\n",
    "           .select(col('mgr.employee_name'), col('emp.employee_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5dd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c358dda",
   "metadata": {},
   "source": [
    "𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧 43\n",
    "\n",
    "You are working as a Data Engineer, and the company has a log system where timestamps are recorded for every user action (e.g., when the user logs in and logs out). Your manager wants to know how much time each user spends between log in and log out.\n",
    "\n",
    "calculate the difference between the logout_timestamp and login_timestamp in hours, minutes, and seconds. The result should be formatted like \"HH:mm:ss\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6921de45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|\n",
      "+-------+-------------------+-------------------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, \"2025-01-31 08:00:00\", \"2025-01-31 10:30:45\"),\n",
    "(2, \"2025-01-31 09:00:30\", \"2025-01-31 12:15:10\"),\n",
    "(3, \"2025-01-31 07:45:00\", \"2025-01-31 09:00:15\") ]\n",
    "\n",
    "columns = [\"user_id\", \"login_timestamp\", \"logout_timestamp\"] \n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32ab0aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|\n",
      "+-------+-------------------+-------------------+----------+-----------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|\n",
      "+-------+-------------------+-------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('login_time', unix_timestamp('login_timestamp'))\n",
    "df = df.withColumn('logout_time', unix_timestamp('logout_timestamp'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb3c4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+----------------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|duration_seconds|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|            9045|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|           11680|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|            4515|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate difference \n",
    "\n",
    "df = df.withColumn('duration_seconds', col('logout_time')-col('login_time'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b44e132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|duration_seconds|hours|minutes|seconds|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|            9045|    2|     30|     45|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|           11680|    3|     14|     40|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|            4515|    1|     15|     15|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate hours, minutes, and seconds\n",
    "\n",
    "df = df.withColumn('hours', (col('duration_seconds') / 3600).cast('int'))\n",
    "df = df.withColumn('minutes', ((col('duration_seconds') % 3600) / 60).cast('int'))\n",
    "df = df.withColumn('seconds', (col('duration_seconds') % 60).cast('int'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "520e24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+------------------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|duration_seconds|hours|minutes|seconds|formatted_duration|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+------------------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|            9045|    2|     30|     45|          02:30:45|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|           11680|    3|     14|     40|          03:14:40|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|            4515|    1|     15|     15|          01:15:15|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('formatted_duration', expr(\"lpad(hours, 2,'0') ||':' || lpad(minutes, 2,'0') ||':' || lpad(seconds, 2,'0')\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5131354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|user_id|formatted_duration|\n",
      "+-------+------------------+\n",
      "|1      |02:30:45          |\n",
      "|2      |03:14:40          |\n",
      "|3      |01:15:15          |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('user_id', 'formatted_duration').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5a20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93371807",
   "metadata": {},
   "source": [
    "𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧 44 \n",
    "\n",
    "You have a dataset of user activities in an e-commerce application, where each row represents an activity performed by a user. The dataset contains duplicate activity entries (based on user and activity type) and you need to remove the duplicates. Furthermore, you want to keep only the most recent record for each user, based on a timestamp column.\n",
    "\n",
    "Problem\n",
    "- Remove duplicates based on user_id and activity_type.\n",
    "- Keep only the most recent activity_timestamp for each user and activity type combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af70c991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+\n",
      "|user_id|activity_type| activity_timestamp|\n",
      "+-------+-------------+-------------------+\n",
      "|      1|        login|2025-02-01 10:00:00|\n",
      "|      1| view_product|2025-02-01 10:05:00|\n",
      "|      1|        login|2025-02-01 10:30:00|\n",
      "|      2|     purchase|2025-02-01 11:00:00|\n",
      "|      2|        login|2025-02-01 11:15:00|\n",
      "|      2| view_product|2025-02-01 11:30:00|\n",
      "|      3|        login|2025-02-01 12:00:00|\n",
      "|      3|        login|2025-02-01 12:05:00|\n",
      "+-------+-------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- activity_type: string (nullable = true)\n",
      " |-- activity_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, 'login', '2025-02-01 10:00:00'), (1, 'view_product', '2025-02-01 10:05:00'), \n",
    "        (1, 'login', '2025-02-01 10:30:00'), (2, 'purchase', '2025-02-01 11:00:00'), (2, 'login', '2025-02-01 11:15:00'), \n",
    "(2, 'view_product', '2025-02-01 11:30:00'), (3, 'login', '2025-02-01 12:00:00'), (3, 'login', '2025-02-01 12:05:00') ]\n",
    " \n",
    "df = spark.createDataFrame(data, [\"user_id\", \"activity_type\", \"activity_timestamp\"])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8755fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+\n",
      "|user_id|activity_type| activity_timestamp|\n",
      "+-------+-------------+-------------------+\n",
      "|      1|        login|2025-02-01 10:00:00|\n",
      "|      1| view_product|2025-02-01 10:05:00|\n",
      "|      1|        login|2025-02-01 10:30:00|\n",
      "|      2|     purchase|2025-02-01 11:00:00|\n",
      "|      2|        login|2025-02-01 11:15:00|\n",
      "|      2| view_product|2025-02-01 11:30:00|\n",
      "|      3|        login|2025-02-01 12:00:00|\n",
      "|      3|        login|2025-02-01 12:05:00|\n",
      "+-------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('activity_timestamp', col('activity_timestamp').cast('timestamp'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba31b675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+-------+\n",
      "|user_id|activity_type| activity_timestamp|row_num|\n",
      "+-------+-------------+-------------------+-------+\n",
      "|      1|        login|2025-02-01 10:30:00|      1|\n",
      "|      1|        login|2025-02-01 10:00:00|      2|\n",
      "|      1| view_product|2025-02-01 10:05:00|      1|\n",
      "|      2|        login|2025-02-01 11:15:00|      1|\n",
      "|      2|     purchase|2025-02-01 11:00:00|      1|\n",
      "|      2| view_product|2025-02-01 11:30:00|      1|\n",
      "|      3|        login|2025-02-01 12:05:00|      1|\n",
      "|      3|        login|2025-02-01 12:00:00|      2|\n",
      "+-------+-------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id', 'activity_type').orderBy(col('activity_timestamp').desc())\n",
    "# Add a row number to each partition\n",
    "df_with_row_num = df.withColumn('row_num', row_number().over(window_spec))\n",
    "df_with_row_num.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58afa248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+\n",
      "|user_id|activity_type| activity_timestamp|\n",
      "+-------+-------------+-------------------+\n",
      "|      1|        login|2025-02-01 10:30:00|\n",
      "|      1| view_product|2025-02-01 10:05:00|\n",
      "|      2|        login|2025-02-01 11:15:00|\n",
      "|      2|     purchase|2025-02-01 11:00:00|\n",
      "|      2| view_product|2025-02-01 11:30:00|\n",
      "|      3|        login|2025-02-01 12:05:00|\n",
      "+-------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df_with_row_num.filter(col('row_num') == 1).drop('row_num')\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d436d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034825fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
