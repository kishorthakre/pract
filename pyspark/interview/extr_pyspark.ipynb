{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "faf32c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8ba4c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "84a89eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x266256e6c50>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('pr').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30bf2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------------------+\n",
      "|uid| pid|qunt|              pur_dt|\n",
      "+---+----+----+--------------------+\n",
      "|333|1122|   9|2022-02-06T01:00:...|\n",
      "|333|1122|  10|2022-02-06T02:00:...|\n",
      "|536|1435|  10|2022-03-02T08:40:...|\n",
      "|536|3223|   5|2022-03-02T09:33:...|\n",
      "|536|3223|   6|2022-01-11T12:33:...|\n",
      "|827|2452|  45|2022-03-02T00:00:...|\n",
      "|827|3585|  35|2022-02-20T14:05:...|\n",
      "+---+----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the customers who brought the same product more than once but on different days\n",
    "# ( if same product is purchased multiple times but on same date shouldn't be counted)\n",
    "\n",
    "data = [(333, 1122, 9, '2022-02-06T01:00:00.000+00:00'),\n",
    "        (333,1122,10,'2022-02-06T02:00:00.000+00:00'), \n",
    "        (536,1435, 10,'2022-03-02T08:40:00.000+00:00'),\n",
    "        (536,3223,5,'2022-03-02T09:33:28.000+00:00'),\n",
    "        (536, 3223, 6,'2022-01-11T12:33:44.000+00:00'),\n",
    "        (827, 2452, 45,'2022-03-02T00:00:00.000+00:00'), \n",
    "        (827, 3585, 35,'2022-02-20T14:05:26.000+00:00')]\n",
    "df = spark.createDataFrame(data = data, schema=['uid', 'pid', 'qunt', 'pur_dt'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0882e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+-----+\n",
      "|uid| pid|        dt|count|\n",
      "+---+----+----------+-----+\n",
      "|333|1122|2022-02-06|    2|\n",
      "|536|1435|2022-03-02|    1|\n",
      "|536|3223|2022-03-02|    1|\n",
      "|536|3223|2022-01-11|    1|\n",
      "|827|2452|2022-03-02|    1|\n",
      "|827|3585|2022-02-20|    1|\n",
      "+---+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('dt', to_date('pur_dt')).groupBy('uid', 'pid', 'dt').count()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7f5e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|uid| pid|count|\n",
      "+---+----+-----+\n",
      "|536|3223|    2|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('uid', 'pid').count().filter(col('count') >= 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8fdba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| Id|           name|\n",
      "+---+---------------+\n",
      "|  1|sagar-prajapati|\n",
      "|  2|      alex-john|\n",
      "|  3|      john cena|\n",
      "|  4|        kim joe|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In the given dataset, names contain for some names and space for some names, extract the first name and last name \n",
    "\n",
    "data = [(1, 'sagar-prajapati'), (2, 'alex-john'), (3, 'john cena'), (4, 'kim joe')]\n",
    "schema = ['Id','name']\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d756944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "| Id|           name|              A|\n",
      "+---+---------------+---------------+\n",
      "|  1|sagar-prajapati|sagar prajapati|\n",
      "|  2|      alex-john|      alex john|\n",
      "|  3|      john cena|      john cena|\n",
      "|  4|        kim joe|        kim joe|\n",
      "+---+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_regexp = \"((,)?\\s|[-])\"\n",
    "df1 = df.withColumn(\"A\", regexp_replace(col(\"name\"), replace_regexp, \" \"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41ceb8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------+---------+\n",
      "| Id|           name|First_name|Last_Name|\n",
      "+---+---------------+----------+---------+\n",
      "|  1|sagar-prajapati|     sagar|prajapati|\n",
      "|  2|      alex-john|      alex|     john|\n",
      "|  3|      john cena|      john|     cena|\n",
      "|  4|        kim joe|       kim|      joe|\n",
      "+---+---------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn('First_name', split(df1['A'], ' ').getItem(0)).withColumn('Last_Name', split(df1['A'], ' ').getItem(1))\n",
    "df2.drop('A').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d55ffe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+\n",
      "| Id|           name|                 A|\n",
      "+---+---------------+------------------+\n",
      "|  1|sagar-prajapati|[sagar, prajapati]|\n",
      "|  2|      alex-john|      [alex, john]|\n",
      "|  3|      john cena|      [john, cena]|\n",
      "|  4|        kim joe|        [kim, joe]|\n",
      "+---+---------------+------------------+\n",
      "\n",
      "+---+---------------+------------------+-----+---------+\n",
      "| Id|           name|                 A|fname|    lname|\n",
      "+---+---------------+------------------+-----+---------+\n",
      "|  1|sagar-prajapati|[sagar, prajapati]|sagar|prajapati|\n",
      "|  2|      alex-john|      [alex, john]| alex|     john|\n",
      "|  3|      john cena|      [john, cena]| john|     cena|\n",
      "|  4|        kim joe|        [kim, joe]|  kim|      joe|\n",
      "+---+---------------+------------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_regex = \"((,)?\\s|[-])\"\n",
    "df1 = df.withColumn(\"A\", split(col(\"name\"), split_regex))\n",
    "df1.show()\n",
    "\n",
    "df1.select('*', df1.A.getItem(0).alias('fname'), df1.A.getItem(1).alias('lname')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "792957f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|                 A|array_contains|\n",
      "+------------------+--------------+\n",
      "|[sagar, prajapati]|         false|\n",
      "|      [alex, john]|          true|\n",
      "|      [john, cena]|         false|\n",
      "|        [kim, joe]|         false|\n",
      "+------------------+--------------+\n",
      "\n",
      "+---+---------+------------+\n",
      "| Id|     name|           A|\n",
      "+---+---------+------------+\n",
      "|  2|alex-john|[alex, john]|\n",
      "|  3|john cena|[john, cena]|\n",
      "+---+---------+------------+\n",
      "\n",
      "+---------------+------------------+\n",
      "|           Name|    Sorted_Numbers|\n",
      "+---------------+------------------+\n",
      "|sagar-prajapati|[prajapati, sagar]|\n",
      "|      alex-john|      [alex, john]|\n",
      "|      john cena|      [cena, john]|\n",
      "|        kim joe|        [joe, kim]|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select('A',array_contains('A',\"alex\").alias(\"array_contains\")).show()\n",
    "df1.filter(array_contains(df1.A, 'john')).show()\n",
    "df1.select(\"Name\", array_sort(df1.A).alias(\"Sorted_Numbers\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "112eb487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+----+\n",
      "| Id|           name|                 A|Size|\n",
      "+---+---------------+------------------+----+\n",
      "|  1|sagar-prajapati|[sagar, prajapati]|   2|\n",
      "|  2|      alex-john|      [alex, john]|   2|\n",
      "|  3|      john cena|      [john, cena]|   2|\n",
      "|  4|        kim joe|        [kim, joe]|   2|\n",
      "+---+---------------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"Size\", size(df1.A)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bf22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "010bc83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|person1|person2|call_duration|\n",
      "+-------+-------+-------------+\n",
      "|     10|     20|           58|\n",
      "|     20|     10|           12|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "|     40|     30|          500|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call_duration \n",
    "\n",
    "data = [(10, 20, 58), (20,10,12), (10,30, 20),(30,40,100),(30, 40, 200), (30, 40, 200), (40, 30, 500)]\n",
    "df = spark.createDataFrame(data = data, schema=['person1', 'person2', 'call_duration'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c678489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|person1|person2|call_duration|\n",
      "+-------+-------+-------------+\n",
      "|     10|     20|           58|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "|     10|     20|           58|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.alias('t1').unionAll(df.alias('t2')).filter(col('person1') < col('person2'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f662b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------+\n",
      "|person1|person2|call_count|total_duration|\n",
      "+-------+-------+----------+--------------+\n",
      "|     10|     20|         2|           116|\n",
      "|     10|     30|         2|            40|\n",
      "|     30|     40|         6|          1000|\n",
      "+-------+-------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy('person1','person2').agg(count(col('call_duration')).alias('call_count'), sum(col('call_duration')).alias('total_duration'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96b2c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| sub|\n",
      "+---+----+\n",
      "|  1|MATH|\n",
      "|  2|MATH|\n",
      "|  4|CHEM|\n",
      "|  5|MATH|\n",
      "|  2| ENG|\n",
      "|  3| PHY|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the teachers who teaches only math and not any other subject \n",
    "\n",
    "data = [(1, \"MATH\"), (2,'MATH'), (4, 'CHEM'),(5, 'MATH'),(2, 'ENG'), (3, 'PHY')]\n",
    "df = spark.createDataFrame(data = data, schema=['id', 'sub'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3051b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  4|    1|\n",
      "|  5|    1|\n",
      "|  3|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('id').count().filter(col('count') == 1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b512244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| sub|\n",
      "+---+----+\n",
      "|  1|MATH|\n",
      "|  5|MATH|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df1, df.id == df1.id, how = 'inner').filter(df['sub'] == 'MATH').select(df['*']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9dbfc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+\n",
      "|COMPANY|YEAR|REVENUE|\n",
      "+-------+----+-------+\n",
      "|    ABC|2000|    100|\n",
      "|    ABC|2001|    110|\n",
      "|    ABC|2002|    120|\n",
      "|    XYZ|2000|    100|\n",
      "|    XYZ|2001|     90|\n",
      "|    XYZ|2002|    120|\n",
      "|    RXC|2000|    500|\n",
      "|    RXC|2001|    400|\n",
      "|    RXC|2002|    600|\n",
      "|    RXC|2003|    800|\n",
      "+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out the companies where revenue has only increased over the years and there was no decrease at all for any point.\n",
    "\n",
    "data = [('ABC', 2000, 100),\n",
    "('ABC', 2001, 110),\n",
    "('ABC', 2002, 120),\n",
    "('XYZ', 2000, 100),\n",
    "('XYZ', 2001, 90),\n",
    "('XYZ', 2002, 120),\n",
    "('RXC', 2000, 500),\n",
    "('RXC', 2001, 400),\n",
    "('RXC', 2002, 600),\n",
    "('RXC', 2003, 800)]\n",
    "schema = StructType([StructField('COMPANY', StringType(), True),\n",
    "                     StructField('YEAR', IntegerType(), True),\n",
    "                     StructField('REVENUE', IntegerType(), True)]) \n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "167fafd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+----+\n",
      "|COMPANY|YEAR|REVENUE| lag|\n",
      "+-------+----+-------+----+\n",
      "|    ABC|2000|    100| 100|\n",
      "|    ABC|2001|    110|  10|\n",
      "|    ABC|2002|    120|  10|\n",
      "|    RXC|2000|    500| 500|\n",
      "|    RXC|2001|    400|-100|\n",
      "|    RXC|2002|    600| 200|\n",
      "|    RXC|2003|    800| 200|\n",
      "|    XYZ|2000|    100| 100|\n",
      "|    XYZ|2001|     90| -10|\n",
      "|    XYZ|2002|    120|  30|\n",
      "+-------+----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy('COMPANY').orderBy('YEAR')\n",
    "df1 = df.withColumn('lag', col('REVENUE')-lag(col('REVENUE'), 1, 0).over(window))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "396390e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|COMPANY|diff|\n",
      "+-------+----+\n",
      "|    ABC|  10|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy('COMPANY').agg(min(col('lag')).alias('diff')).filter(col('diff') > 0)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d65d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     Movie|       Type|Rating|\n",
      "+---+----------+-----------+------+\n",
      "|  1|       war|   great ed|   8.9|\n",
      "|  2|   science|    fiction|   8.5|\n",
      "|  3|     irish|     boring|   6.2|\n",
      "|  4|  Ice song|    fantacy|   8.6|\n",
      "|  5|house card|interesting|   9.1|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lIst down the movies with an odd ID and which is not boring and order by id desc \n",
    "\n",
    "data = [(1, 'war', 'great ed',8.9),\n",
    "    (2,'science','fiction',8.5),\n",
    "    (3,'irish', 'boring', 6.2),\n",
    "    (4, 'Ice song', 'fantacy', 8.6),\n",
    "    (5, \"house card\", 'interesting', 9.1)]\n",
    "sch = ['ID', 'Movie', 'Type', 'Rating']\n",
    "df = spark.createDataFrame(data=data, schema=sch)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df7f6e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     Movie|       Type|Rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|house card|interesting|   9.1|\n",
      "|  1|       war|   great ed|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.filter(((df['ID'] %2 ) != 0 ) & (col('Type') != 'boring')).orderBy(col('ID').desc())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "302ce2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----+\n",
      "| id| name|salary| mid|\n",
      "+---+-----+------+----+\n",
      "|  1| John|  6000|   4|\n",
      "|  2|Kevin| 11000|   4|\n",
      "|  3|  Bob|  8000|   5|\n",
      "|  4|Laura|  9000|NULL|\n",
      "|  5|Sarah| 10000|NULL|\n",
      "+---+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the employees earning more than managers \n",
    "\n",
    "data = [(1, \"John\", 6000, 4), (2,'Kevin',11000,4), (3, 'Bob',8000, 5),(4, 'Laura',9000,None),(5, 'Sarah',10000, None)]\n",
    "df = spark.createDataFrame(data = data, schema=['id', 'name', 'salary','mid'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6169934e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+---+\n",
      "| id| name|salary|mid|\n",
      "+---+-----+------+---+\n",
      "|  2|Kevin| 11000|  4|\n",
      "+---+-----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('emp').join(df.alias('mgr'), col('emp.mid')  == col('mgr.id'), 'inner').filter(col('emp.salary') > col('mgr.salary'))\\\n",
    ".select(col('emp.id'), col('emp.name'), col('emp.salary'), col('emp.mid')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3cc74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b9d0d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   team|points|\n",
      "+-------+------+\n",
      "|  Mavs^|    18|\n",
      "|  Ne%ts|    33|\n",
      "|Hawk**s|    12|\n",
      "|  Mavs@|    15|\n",
      "| Hawks!|    19|\n",
      "| (Cavs)|    24|\n",
      "|  Magic|    28|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove special charactors\n",
    "\n",
    "data = [['Mavs^', 18], \n",
    "        ['Ne%ts', 33], \n",
    "        ['Hawk**s', 12], \n",
    "        ['Mavs@', 15], \n",
    "        ['Hawks!', 19],\n",
    "        ['(Cavs)', 24],\n",
    "        ['Magic', 28]] \n",
    "columns = ['team', 'points'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "399c47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|   team|points|team2|\n",
      "+-------+------+-----+\n",
      "|  Mavs^|    18| Mavs|\n",
      "|  Ne%ts|    33| Nets|\n",
      "|Hawk**s|    12|Hawks|\n",
      "|  Mavs@|    15| Mavs|\n",
      "| Hawks!|    19|Hawks|\n",
      "| (Cavs)|    24| Cavs|\n",
      "|  Magic|    28|Magic|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('team2', regexp_replace('team', '[^a-zA-Z0-9]', ''))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e03fcce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+-----------------+\n",
      "|   team|points|team2|count_of_spe_char|\n",
      "+-------+------+-----+-----------------+\n",
      "|  Mavs^|    18| Mavs|                1|\n",
      "|  Ne%ts|    33| Nets|                1|\n",
      "|Hawk**s|    12|Hawks|                2|\n",
      "|  Mavs@|    15| Mavs|                1|\n",
      "| Hawks!|    19|Hawks|                1|\n",
      "| (Cavs)|    24| Cavs|                2|\n",
      "|  Magic|    28|Magic|                0|\n",
      "+-------+------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn('count_of_spe_char',length(col('team')) - length(col('team2'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698d1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc9b8e6",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/company/seekho-bigdata-institute/posts/?feedView=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1eee32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffee7795",
   "metadata": {},
   "source": [
    "### 9\n",
    "The dataset has duplicate entries, and some entries are missing values. You are required to:\n",
    "    \n",
    "-- Deduplicate the dataset.\n",
    "- Handle any missing values appropriately.\n",
    "- Determine the top 3 most frequent activity_type for each user_id.\n",
    "- Calculate the time spent by each user on each activity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "aa42a870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+\n",
      "|user_id|          timestamp|activity_type|\n",
      "+-------+-------------------+-------------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|   NULL|2024-12-30 12:00:00|        LOGIN|\n",
      "|     U3|               NULL|       LOGOUT|\n",
      "+-------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"U1\", \"2024-12-30 10:00:00\", \"LOGIN\"), \n",
    "        (\"U1\", \"2024-12-30 10:05:00\", \"BROWSE\"),\n",
    "        (\"U1\", \"2024-12-30 10:20:00\", \"LOGOUT\"),\n",
    "        (\"U2\", \"2024-12-30 11:00:00\", \"LOGIN\"),\n",
    "        (\"U2\", \"2024-12-30 11:15:00\", \"BROWSE\"),\n",
    "        (\"U2\", \"2024-12-30 11:30:00\", \"LOGOUT\"),\n",
    "        (\"U1\", \"2024-12-30 10:20:00\", \"LOGOUT\"),  \n",
    "        (None, \"2024-12-30 12:00:00\", \"LOGIN\"),   \n",
    "        (\"U3\", None, \"LOGOUT\")           ]\n",
    "\n",
    "schema = StructType([ StructField(\"user_id\", StringType(), True),\n",
    "                     StructField(\"timestamp\", StringType(), True),\n",
    "                     StructField(\"activity_type\", StringType(), True) ])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d882e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+\n",
      "|user_id|          timestamp|activity_type|\n",
      "+-------+-------------------+-------------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|   NULL|2024-12-30 12:00:00|        LOGIN|\n",
      "|     U3|               NULL|       LOGOUT|\n",
      "+-------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert timestamp to TimesstampType\n",
    "df = df.withColumn('timestamp', col('timestamp').cast(TimestampType()))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8bba8c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+\n",
      "|user_id|          timestamp|activity_type|\n",
      "+-------+-------------------+-------------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|\n",
      "|     U3|               NULL|       LOGOUT|\n",
      "|   NULL|2024-12-30 12:00:00|        LOGIN|\n",
      "+-------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate the dataset \n",
    "df_deduplicated = df.dropDuplicates()\n",
    "df_deduplicated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c504b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+\n",
      "|user_id|          timestamp|activity_type|\n",
      "+-------+-------------------+-------------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|\n",
      "+-------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2 Handle missing values \n",
    "df_cleaned = df_deduplicated.dropna(subset=['user_id', 'timestamp', 'activity_type'])\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "94627899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+\n",
      "|user_id|activity_type|count|\n",
      "+-------+-------------+-----+\n",
      "|     U1|       BROWSE|    1|\n",
      "|     U2|       BROWSE|    1|\n",
      "|     U1|       LOGOUT|    1|\n",
      "|     U2|        LOGIN|    1|\n",
      "|     U2|       LOGOUT|    1|\n",
      "|     U1|        LOGIN|    1|\n",
      "+-------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Determine the top 3 most frequent activity_type for each user_id.\n",
    "\n",
    "activity_count = df_cleaned.groupBy('user_id', 'activity_type').count()\n",
    "activity_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f4a702fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+----+\n",
      "|user_id|activity_type|count|rank|\n",
      "+-------+-------------+-----+----+\n",
      "|     U1|       BROWSE|    1|   1|\n",
      "|     U1|       LOGOUT|    1|   2|\n",
      "|     U1|        LOGIN|    1|   3|\n",
      "|     U2|       BROWSE|    1|   1|\n",
      "|     U2|        LOGIN|    1|   2|\n",
      "|     U2|       LOGOUT|    1|   3|\n",
      "+-------+-------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_count= activity_count.withColumn('rank', row_number().over(Window.partitionBy('user_id').orderBy(desc('count'))))\n",
    "activity_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "299ecf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+\n",
      "|user_id|activity_type|count|\n",
      "+-------+-------------+-----+\n",
      "|     U1|       BROWSE|    1|\n",
      "|     U1|       LOGOUT|    1|\n",
      "|     U1|        LOGIN|    1|\n",
      "|     U2|       BROWSE|    1|\n",
      "|     U2|        LOGIN|    1|\n",
      "|     U2|       LOGOUT|    1|\n",
      "+-------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_activities = activity_count.filter(col('rank') <= 3).select('user_id', 'activity_type', 'count')\n",
    "top_activities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1bb18dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+-------------------+\n",
      "|user_id|          timestamp|activity_type|     next_timestamp|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|               NULL|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|2024-12-30 10:00:00|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|2024-12-30 10:05:00|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|               NULL|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|2024-12-30 11:00:00|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|2024-12-30 11:15:00|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the time spent by each user on each activity_type\n",
    "\n",
    "win_spec = Window.partitionBy('user_id').orderBy('timestamp')\n",
    "df_with_lag = df_cleaned.withColumn('next_timestamp', lag('timestamp').over(win_spec))\n",
    "df_with_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f6c88192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+-------------------+----------+\n",
      "|user_id|          timestamp|activity_type|     next_timestamp|time_spent|\n",
      "+-------+-------------------+-------------+-------------------+----------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|               NULL|      NULL|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|2024-12-30 10:00:00|      -5.0|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|2024-12-30 10:05:00|     -15.0|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|               NULL|      NULL|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|2024-12-30 11:00:00|     -15.0|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|2024-12-30 11:15:00|     -15.0|\n",
      "+-------+-------------------+-------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_time_spent = df_with_lag.withColumn('time_spent', (col('next_timestamp').cast('long') - col('timestamp').cast('long')).cast('double') / 60)\n",
    "df_with_time_spent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9be40899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+---------------+\n",
      "|user_id|activity_type|sum(time_spent)|\n",
      "+-------+-------------+---------------+\n",
      "|     U1|        LOGIN|           NULL|\n",
      "|     U1|       BROWSE|           -5.0|\n",
      "|     U1|       LOGOUT|          -15.0|\n",
      "|     U2|        LOGIN|           NULL|\n",
      "|     U2|       BROWSE|          -15.0|\n",
      "|     U2|       LOGOUT|          -15.0|\n",
      "+-------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_spent_per_activity = df_with_time_spent.groupBy('user_id', 'activity_type').sum('time_spent')\n",
    "time_spent_per_activity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73889513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "aba6ec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------------------+----------------+\n",
      "|transaction_id|user_id|transaction_amount|transaction_date|\n",
      "+--------------+-------+------------------+----------------+\n",
      "|             1|    101|             500.0|      2024-01-01|\n",
      "|             2|    102|             200.0|      2024-01-02|\n",
      "|             3|    101|             300.0|      2024-01-03|\n",
      "|             4|    103|             100.0|      2024-01-04|\n",
      "|             5|    102|             400.0|      2024-01-05|\n",
      "|             6|    103|             600.0|      2024-01-06|\n",
      "|             7|    101|             200.0|      2024-01-07|\n",
      "+--------------+-------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13 Identify users who have made transactions on at least 3 different dates.\n",
    "#For these users, calculate their average transaction amount.\n",
    "\n",
    "data = [\n",
    " (1, 101, 500.0, \"2024-01-01\"), \n",
    " (2, 102, 200.0, \"2024-01-02\"), \n",
    " (3, 101, 300.0, \"2024-01-03\"), \n",
    " (4, 103, 100.0, \"2024-01-04\"), \n",
    " (5, 102, 400.0, \"2024-01-05\"), \n",
    " (6, 103, 600.0, \"2024-01-06\"), \n",
    " (7, 101, 200.0, \"2024-01-07\"),\n",
    "]\n",
    "columns = [\"transaction_id\", \"user_id\", \"transaction_amount\", \"transaction_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "af51a4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|distinct_date_count|\n",
      "+-------+-------------------+\n",
      "|    103|                  2|\n",
      "|    101|                  3|\n",
      "|    102|                  2|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count distinct transactons dates per uesr \n",
    "user_date_count = df.groupBy('user_id').agg(countDistinct('transaction_date').alias('distinct_date_count'))\n",
    "user_date_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "27d0ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|distinct_date_count|\n",
      "+-------+-------------------+\n",
      "|    101|                  3|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter users with at least 3 transactions dates \n",
    "filtered_users = user_date_count.filter(col('distinct_date_count') >= 3)\n",
    "filtered_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e7a1dd99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------------------+----------------+-------------------+\n",
      "|user_id|transaction_id|transaction_amount|transaction_date|distinct_date_count|\n",
      "+-------+--------------+------------------+----------------+-------------------+\n",
      "|    101|             1|             500.0|      2024-01-01|                  3|\n",
      "|    101|             3|             300.0|      2024-01-03|                  3|\n",
      "|    101|             7|             200.0|      2024-01-07|                  3|\n",
      "+-------+--------------+------------------+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join with original data to filter transactions of these users \n",
    "filtered_transactions = df.join(filtered_users, on ='user_id', how='inner')\n",
    "filtered_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7c784425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+\n",
      "|user_id|avg_transaction_amount|\n",
      "+-------+----------------------+\n",
      "|    101|     333.3333333333333|\n",
      "+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = filtered_transactions.groupBy('user_id').agg(avg('transaction_amount').alias('avg_transaction_amount'))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00617250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "39dae1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------------------+----------------+\n",
      "|transaction_id|user_id|transaction_amount|transaction_date|\n",
      "+--------------+-------+------------------+----------------+\n",
      "|             1|    101|             500.0|      2024-01-01|\n",
      "|             2|    102|             200.0|      2024-01-02|\n",
      "|             3|    101|             300.0|      2024-01-03|\n",
      "|             4|    103|             100.0|      2024-01-04|\n",
      "|             5|    102|             400.0|      2024-01-05|\n",
      "|             6|    103|             600.0|      2024-01-06|\n",
      "|             7|    101|             200.0|      2024-01-07|\n",
      "+--------------+-------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#15 calculate the average gap in days between consecutive transactions.\n",
    "# Identify the user with the largest average gap.\n",
    "\n",
    "data = [\n",
    " (1, 101, 500.0, \"2024-01-01\"), \n",
    " (2, 102, 200.0, \"2024-01-02\"), \n",
    " (3, 101, 300.0, \"2024-01-03\"), \n",
    " (4, 103, 100.0, \"2024-01-04\"), \n",
    " (5, 102, 400.0, \"2024-01-05\"), \n",
    " (6, 103, 600.0, \"2024-01-06\"), \n",
    " (7, 101, 200.0, \"2024-01-07\"),\n",
    "]\n",
    "columns = [\"transaction_id\", \"user_id\", \"transaction_amount\", \"transaction_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e0f7463a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------------------+----------------+\n",
      "|transaction_id|user_id|transaction_amount|transaction_date|\n",
      "+--------------+-------+------------------+----------------+\n",
      "|             1|    101|             500.0|      2024-01-01|\n",
      "|             2|    102|             200.0|      2024-01-02|\n",
      "|             3|    101|             300.0|      2024-01-03|\n",
      "|             4|    103|             100.0|      2024-01-04|\n",
      "|             5|    102|             400.0|      2024-01-05|\n",
      "|             6|    103|             600.0|      2024-01-06|\n",
      "|             7|    101|             200.0|      2024-01-07|\n",
      "+--------------+-------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('transaction_date', to_date(col('transaction_date'), 'yyyy-MM-dd'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "823dc9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------------------+----------------+----------+\n",
      "|transaction_id|user_id|transaction_amount|transaction_date| prev_date|\n",
      "+--------------+-------+------------------+----------------+----------+\n",
      "|             1|    101|             500.0|      2024-01-01|      NULL|\n",
      "|             3|    101|             300.0|      2024-01-03|2024-01-01|\n",
      "|             7|    101|             200.0|      2024-01-07|2024-01-03|\n",
      "|             2|    102|             200.0|      2024-01-02|      NULL|\n",
      "|             5|    102|             400.0|      2024-01-05|2024-01-02|\n",
      "|             4|    103|             100.0|      2024-01-04|      NULL|\n",
      "|             6|    103|             600.0|      2024-01-06|2024-01-04|\n",
      "+--------------+-------+------------------+----------------+----------+\n",
      "\n",
      "+--------------+-------+------------------+----------------+----------+-------+\n",
      "|transaction_id|user_id|transaction_amount|transaction_date| prev_date|day_gap|\n",
      "+--------------+-------+------------------+----------------+----------+-------+\n",
      "|             1|    101|             500.0|      2024-01-01|      NULL|   NULL|\n",
      "|             3|    101|             300.0|      2024-01-03|2024-01-01|      2|\n",
      "|             7|    101|             200.0|      2024-01-07|2024-01-03|      4|\n",
      "|             2|    102|             200.0|      2024-01-02|      NULL|   NULL|\n",
      "|             5|    102|             400.0|      2024-01-05|2024-01-02|      3|\n",
      "|             4|    103|             100.0|      2024-01-04|      NULL|   NULL|\n",
      "|             6|    103|             600.0|      2024-01-06|2024-01-04|      2|\n",
      "+--------------+-------+------------------+----------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the difference in days between consecutive transactions\n",
    "\n",
    "win = Window.partitionBy('user_id').orderBy('transaction_date')\n",
    "\n",
    "df = df.withColumn('prev_date', lag('transaction_date').over(win))\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('day_gap', datediff(col('transaction_date'), col('prev_date')))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cc55bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|user_id|avg_day_gap|\n",
      "+-------+-----------+\n",
      "|    101|        3.0|\n",
      "|    102|        3.0|\n",
      "|    103|        2.0|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average gap for each user\n",
    "\n",
    "avg_gap_df = df.groupBy('user_id').agg(avg('day_gap').alias('avg_day_gap'))\n",
    "avg_gap_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "64974f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average gap for each user:\n",
      "+-------+-----------+\n",
      "|user_id|avg_day_gap|\n",
      "+-------+-----------+\n",
      "|    101|        3.0|\n",
      "|    102|        3.0|\n",
      "|    103|        2.0|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Average gap for each user:')\n",
    "avg_gap_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f0a61390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User with the largest average gap:\n",
      "+-------+-----------+\n",
      "|user_id|avg_day_gap|\n",
      "+-------+-----------+\n",
      "|    101|        3.0|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the user with the largest average gap\n",
    "largest_avg_gap = avg_gap_df.orderBy(desc('avg_day_gap')).limit(1)\n",
    "\n",
    "print('User with the largest average gap:')\n",
    "largest_avg_gap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d0176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3cbff72",
   "metadata": {},
   "source": [
    "### 16\n",
    "\n",
    "- Calculate the average grade for each student across all semesters.\n",
    "- Filter out students with an average grade lower than 75.\n",
    "- Find the top 3 students with the highest grades in each subject for the latest semester.\n",
    "- Sort the final results by subject and then by grade in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5e8b8b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+-----+\n",
      "|Student|Subject| Semester|Grade|\n",
      "+-------+-------+---------+-----+\n",
      "|  Alice|   Math|Semester1|   85|\n",
      "|  Alice|   Math|Semester2|   90|\n",
      "|  Alice|English|Semester1|   78|\n",
      "|  Alice|English|Semester2|   82|\n",
      "|    Bob|   Math|Semester1|   65|\n",
      "|    Bob|   Math|Semester2|   70|\n",
      "|    Bob|English|Semester1|   60|\n",
      "|    Bob|English|Semester2|   65|\n",
      "|Charlie|   Math|Semester1|   95|\n",
      "|Charlie|   Math|Semester2|   98|\n",
      "|Charlie|English|Semester1|   88|\n",
      "|Charlie|English|Semester2|   90|\n",
      "|  David|   Math|Semester1|   78|\n",
      "|  David|   Math|Semester2|   80|\n",
      "|  David|English|Semester1|   75|\n",
      "|  David|English|Semester2|   72|\n",
      "|    Eve|   Math|Semester1|   88|\n",
      "|    Eve|   Math|Semester2|   85|\n",
      "|    Eve|English|Semester1|   80|\n",
      "|    Eve|English|Semester2|   83|\n",
      "+-------+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (\"Alice\", \"Math\", \"Semester1\", 85),\n",
    " (\"Alice\", \"Math\", \"Semester2\", 90),\n",
    " (\"Alice\", \"English\", \"Semester1\", 78),\n",
    " (\"Alice\", \"English\", \"Semester2\", 82),\n",
    " (\"Bob\", \"Math\", \"Semester1\", 65),\n",
    " (\"Bob\", \"Math\", \"Semester2\", 70),\n",
    " (\"Bob\", \"English\", \"Semester1\", 60),\n",
    " (\"Bob\", \"English\", \"Semester2\", 65),\n",
    " (\"Charlie\", \"Math\", \"Semester1\", 95),\n",
    " (\"Charlie\", \"Math\", \"Semester2\", 98),\n",
    " (\"Charlie\", \"English\", \"Semester1\", 88),\n",
    " (\"Charlie\", \"English\", \"Semester2\", 90),\n",
    " (\"David\", \"Math\", \"Semester1\", 78),\n",
    " (\"David\", \"Math\", \"Semester2\", 80),\n",
    " (\"David\", \"English\", \"Semester1\", 75),\n",
    " (\"David\", \"English\", \"Semester2\", 72),\n",
    " (\"Eve\", \"Math\", \"Semester1\", 88),\n",
    " (\"Eve\", \"Math\", \"Semester2\", 85),\n",
    " (\"Eve\", \"English\", \"Semester1\", 80),\n",
    " (\"Eve\", \"English\", \"Semester2\", 83)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Student\", \"Subject\", \"Semester\", \"Grade\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1a44a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "| Semester|avg(Grade)|\n",
      "+---------+----------+\n",
      "|Semester1|      79.2|\n",
      "|Semester2|      81.5|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average grade for each student across all semesters.\n",
    "std_avg_grd = df.groupBy('Semester').agg(avg(col('Grade')))\n",
    "std_avg_grd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2134dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+-----+\n",
      "|Student|Subject| Semester|Grade|\n",
      "+-------+-------+---------+-----+\n",
      "|  Alice|   Math|Semester1|   85|\n",
      "|  Alice|   Math|Semester2|   90|\n",
      "|  Alice|English|Semester1|   78|\n",
      "|  Alice|English|Semester2|   82|\n",
      "|Charlie|   Math|Semester1|   95|\n",
      "|Charlie|   Math|Semester2|   98|\n",
      "|Charlie|English|Semester1|   88|\n",
      "|Charlie|English|Semester2|   90|\n",
      "|  David|   Math|Semester1|   78|\n",
      "|  David|   Math|Semester2|   80|\n",
      "|    Eve|   Math|Semester1|   88|\n",
      "|    Eve|   Math|Semester2|   85|\n",
      "|    Eve|English|Semester1|   80|\n",
      "|    Eve|English|Semester2|   83|\n",
      "+-------+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter out students with an average grade lower than 75. \n",
    "\n",
    "df.filter(col('Grade') > 75).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "837d0e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|Subject|Latest_Semester|\n",
      "+-------+---------------+\n",
      "|English|      Semester2|\n",
      "|   Math|      Semester2|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the top 3 students with the highest grades in each subject for the latest semester. \n",
    "# Step 1: Determine the latest semester for each subject.\n",
    "\n",
    "latest_semester_df = df.groupBy('Subject').agg({'Semester':'max'}).withColumnRenamed('max(Semester)', 'Latest_Semester')\n",
    "latest_semester_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ae0d294b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Student|Subject|Grade|\n",
      "+-------+-------+-----+\n",
      "|  Alice|   Math|   90|\n",
      "|  Alice|English|   82|\n",
      "|    Bob|   Math|   70|\n",
      "|    Bob|English|   65|\n",
      "|Charlie|   Math|   98|\n",
      "|Charlie|English|   90|\n",
      "|  David|   Math|   80|\n",
      "|  David|English|   72|\n",
      "|    Eve|English|   83|\n",
      "|    Eve|   Math|   85|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the final results by subject and then by grade in descending order. \n",
    "\n",
    "df1 = df.alias('main')\n",
    "df2 = latest_semester_df.alias('latest')\n",
    "\n",
    "latest_grades_df = df1.join(df2, (df1.Subject == df2.Subject) & (df1.Semester == df2.Latest_Semester), how='inner')\\\n",
    "                   .select(df1.Student, df1.Subject, df1.Grade)\n",
    "latest_grades_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b9d125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41cc15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b11c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd08d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ad4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8eb11de",
   "metadata": {},
   "source": [
    "19\n",
    "- Display the schema of the created DataFrame.\n",
    "- Filter the DataFrame to show only transactions where the amount is greater than 100.\n",
    "- Add a new column discounted_amount that applies a 10% discount to all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8c5f0518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+--------------+\n",
      "|amount|customer_id|      date|transaction_id|\n",
      "+------+-----------+----------+--------------+\n",
      "|   150|        101|2025-01-01|             1|\n",
      "|    90|        102|2025-01-02|             2|\n",
      "|   200|        103|2025-01-03|             3|\n",
      "|    50|        104|2025-01-04|             4|\n",
      "|   120|        105|2025-01-05|             5|\n",
      "+------+-----------+----------+--------------+\n",
      "\n",
      "root\n",
      " |-- amount: long (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- transaction_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_data = [\n",
    " {\"transaction_id\": 1, \"customer_id\": 101, \"amount\": 150, \"date\": \"2025-01-01\"},\n",
    " {\"transaction_id\": 2, \"customer_id\": 102, \"amount\": 90, \"date\": \"2025-01-02\"},\n",
    " {\"transaction_id\": 3, \"customer_id\": 103, \"amount\": 200, \"date\": \"2025-01-03\"},\n",
    " {\"transaction_id\": 4, \"customer_id\": 104, \"amount\": 50, \"date\": \"2025-01-04\"},\n",
    " {\"transaction_id\": 5, \"customer_id\": 105, \"amount\": 120, \"date\": \"2025-01-05\"}\n",
    "]\n",
    "df = spark.createDataFrame(transaction_data)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dcecd16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+--------------+\n",
      "|amount|customer_id|      date|transaction_id|\n",
      "+------+-----------+----------+--------------+\n",
      "|   150|        101|2025-01-01|             1|\n",
      "|   200|        103|2025-01-03|             3|\n",
      "|   120|        105|2025-01-05|             5|\n",
      "+------+-----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.amount > 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dcf1df05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+--------------+-----------------+\n",
      "|amount|customer_id|      date|transaction_id|discounted_amount|\n",
      "+------+-----------+----------+--------------+-----------------+\n",
      "|   150|        101|2025-01-01|             1|            135.0|\n",
      "|    90|        102|2025-01-02|             2|             81.0|\n",
      "|   200|        103|2025-01-03|             3|            180.0|\n",
      "|    50|        104|2025-01-04|             4|             45.0|\n",
      "|   120|        105|2025-01-05|             5|            108.0|\n",
      "+------+-----------+----------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column discounted_amount that applies a 10% discount to all transactions.\n",
    "\n",
    "df.withColumn('discounted_amount', col('amount') * 0.9).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb724e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a20890",
   "metadata": {},
   "source": [
    "### 20 \n",
    "\n",
    "The sales data is collected daily, and the company wants to analyze the performance of each product across different regions. The data is stored in a PySpark DataFrame with the following schema:\n",
    "The management has requested a report where each region becomes a column and the values represent the total sales for each product in that region. Your task is to write PySpark code to generate this pivot table.\n",
    "\n",
    "Task:\n",
    "\n",
    "1. Load the sample data into a PySpark DataFrame.\n",
    "2. Use PySpark's pivot functionality to create a table where:\n",
    "Each region is a column.\n",
    "- The rows represent the products.\n",
    "- The values are the total sales for each product in each region.\n",
    "3. Provide the output DataFrame in a user-friendly format for the stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "389cefcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|Product|Region|Sales|\n",
      "+-------+------+-----+\n",
      "|      A| North|  100|\n",
      "|      B|  East|  200|\n",
      "|      A|  East|  150|\n",
      "|      C| North|  300|\n",
      "|      B| South|  400|\n",
      "|      C|  East|  250|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data =([ (\"A\", \"North\", 100), \n",
    "        (\"B\", \"East\", 200), \n",
    "        (\"A\", \"East\", 150),\n",
    "        (\"C\", \"North\", 300), \n",
    "        (\"B\", \"South\", 400), \n",
    "        (\"C\", \"East\", 250) ] )\n",
    "columns = [\"Product\", \"Region\", \"Sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae17ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+\n",
      "|Product|East|North|South|\n",
      "+-------+----+-----+-----+\n",
      "|      B| 200| NULL|  400|\n",
      "|      C| 250|  300| NULL|\n",
      "|      A| 150|  100| NULL|\n",
      "+-------+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Product').pivot('Region').agg(sum('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2b981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0896aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "944197eb",
   "metadata": {},
   "source": [
    "### 21 \n",
    "\n",
    "calculate the ROW_NUMBER() partitioned by department and ordered by salary in descending order. Additionally, the employees should be ranked within each department based on their hiring date if their salaries are the same. Add a new column called rank to the DataFrame that contains the calculated row numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c796d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+----------+\n",
      "|name| department|salary| hire_date|\n",
      "+----+-----------+------+----------+\n",
      "|John|         HR|  5000|2021-05-01|\n",
      "|Jane|         HR|  6000|2022-03-15|\n",
      "| Sam|Engineering|  7000|2021-06-01|\n",
      "|Anna|Engineering|  8000|2020-07-01|\n",
      "|Paul|         HR|  4500|2021-05-01|\n",
      "|Sara|Engineering|  7000|2020-08-01|\n",
      "| Tom|Engineering|  7500|2021-07-01|\n",
      "+----+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"John\", \"HR\", 5000, \"2021-05-01\"), \n",
    "(\"Jane\", \"HR\", 6000, \"2022-03-15\"), \n",
    "(\"Sam\", \"Engineering\", 7000, \"2021-06-01\"), \n",
    "(\"Anna\", \"Engineering\", 8000, \"2020-07-01\"), \n",
    "(\"Paul\", \"HR\", 4500, \"2021-05-01\"), \n",
    "(\"Sara\", \"Engineering\", 7000, \"2020-08-01\"), \n",
    "(\"Tom\", \"Engineering\", 7500, \"2021-07-01\") ]\n",
    "\n",
    "columns = [\"name\", \"department\", \"salary\", \"hire_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9a8fd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+----------+---+\n",
      "|name| department|salary| hire_date| rn|\n",
      "+----+-----------+------+----------+---+\n",
      "|Anna|Engineering|  8000|2020-07-01|  1|\n",
      "| Tom|Engineering|  7500|2021-07-01|  2|\n",
      "| Sam|Engineering|  7000|2021-06-01|  3|\n",
      "|Sara|Engineering|  7000|2020-08-01|  4|\n",
      "|Jane|         HR|  6000|2022-03-15|  1|\n",
      "|John|         HR|  5000|2021-05-01|  2|\n",
      "|Paul|         HR|  4500|2021-05-01|  3|\n",
      "+----+-----------+------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "win = Window.partitionBy('department').orderBy(col('salary').desc())\n",
    "df.withColumn('rn', row_number().over(win)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb7f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2567e981",
   "metadata": {},
   "source": [
    "## 22 :\n",
    "    \n",
    "You have the following dataset containing sales information for different products and regions. Reshape the data using PySpark's pivot() method to calculate the total sales for each product across regions, and then optimize it further by applying specific transformations.\n",
    "\n",
    "Task 1: Use pivot() to create a table showing the total sales for each product by region.\n",
    "\n",
    "Task 2: Add a column calculating the percentage contribution of each region to the total sales for that product.\n",
    "\n",
    "Task 3: Sort the data in descending order by total sales for each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5856b18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+-------+\n",
      "|Region|Product|Sales|Quarter|\n",
      "+------+-------+-----+-------+\n",
      "| North| Laptop| 2000|     Q1|\n",
      "| South| Laptop| 3000|     Q1|\n",
      "|  East| Laptop| 2500|     Q1|\n",
      "| North|  Phone| 1500|     Q1|\n",
      "| South|  Phone| 1000|     Q1|\n",
      "|  East|  Phone| 2000|     Q1|\n",
      "| North| Laptop| 3000|     Q2|\n",
      "| South| Laptop| 4000|     Q2|\n",
      "|  East| Laptop| 3500|     Q2|\n",
      "| North|  Phone| 2500|     Q2|\n",
      "| South|  Phone| 1500|     Q2|\n",
      "|  East|  Phone| 3000|     Q2|\n",
      "+------+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"North\", \"Laptop\", 2000, \"Q1\"),\n",
    "        (\"South\", \"Laptop\", 3000, \"Q1\"), \n",
    "        (\"East\", \"Laptop\", 2500, \"Q1\"), \n",
    "        (\"North\", \"Phone\", 1500, \"Q1\"),\n",
    "        (\"South\", \"Phone\", 1000, \"Q1\"), \n",
    "        (\"East\", \"Phone\", 2000, \"Q1\"),\n",
    "        (\"North\", \"Laptop\", 3000, \"Q2\"), \n",
    "        (\"South\", \"Laptop\", 4000, \"Q2\"),\n",
    "        (\"East\", \"Laptop\", 3500, \"Q2\"),\n",
    "        (\"North\", \"Phone\", 2500, \"Q2\"),\n",
    "        (\"South\", \"Phone\", 1500, \"Q2\"), \n",
    "        (\"East\", \"Phone\", 3000, \"Q2\") ]\n",
    "\n",
    "columns = [\"Region\", \"Product\", \"Sales\", \"Quarter\"] \n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b41fbc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+\n",
      "|Product|East|North|South|\n",
      "+-------+----+-----+-----+\n",
      "|  Phone|5000| 4000| 2500|\n",
      "| Laptop|6000| 5000| 7000|\n",
      "+-------+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot to calculate total sales by region\n",
    "pivot_df = df.groupBy('Product').pivot('Region').sum('Sales')\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9c2de97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "|Product|East|North|South|Total_Sales|North_%|South_%|East_%|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "|  Phone|5000| 4000| 2500|      11500|  34.78|  21.74| 43.48|\n",
      "| Laptop|6000| 5000| 7000|      18000|  27.78|  38.89| 33.33|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column calculating the percentage contribution of each region to the total sales for that product.\n",
    "\n",
    "total_seles_df = pivot_df\\\n",
    "    .withColumn('Total_Sales', col('North') + col('South') + col('East'))\\\n",
    "    .withColumn('North_%', round(col('North') / col('Total_Sales') * 100, 2))\\\n",
    "    .withColumn('South_%', round(col('South') / col('Total_Sales') * 100, 2))\\\n",
    "    .withColumn('East_%', round(col('East') / col('Total_Sales') * 100, 2))\n",
    "\n",
    "total_seles_df.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6059836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "|Product|East|North|South|Total_Sales|North_%|South_%|East_%|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "| Laptop|6000| 5000| 7000|      18000|  27.78|  38.89| 33.33|\n",
      "|  Phone|5000| 4000| 2500|      11500|  34.78|  21.74| 43.48|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "|Product|East|North|South|Total_Sales|North_%|South_%|East_%|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "| Laptop|6000| 5000| 7000|      18000|  27.78|  38.89| 33.33|\n",
      "|  Phone|5000| 4000| 2500|      11500|  34.78|  21.74| 43.48|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data in descending order by total sales for each product.\n",
    "\n",
    "total_seles_df.sort(col('Total_Sales').desc()).show()\n",
    "# OR \n",
    "total_seles_df.orderBy(col('Total_Sales').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0251e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "523e2952",
   "metadata": {},
   "source": [
    "### 24  \n",
    "\n",
    "You are given a nested JSON file named sample_data.json stored in an S3 bucket at s3://your-bucket/sample_data.json. The JSON file contains details about employees, including their names, departments, and address details (nested fields).\n",
    "\n",
    "Write a PySpark program to:\n",
    "- Load the JSON file into a DataFrame.\n",
    "- Flatten the nested structure to create a tabular format.\n",
    "- Write the resulting DataFrame as a Parquet file to the output path s3://your-bucket/output/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "73f9c366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+---+-------+\n",
      "|            address|department| id|   name|\n",
      "+-------------------+----------+---+-------+\n",
      "|     {New York, NY}|        HR|  1|  Alice|\n",
      "|{San Francisco, CA}|        IT|  2|    Bob|\n",
      "|      {Chicago, IL}|   Finance|  3|Charlie|\n",
      "+-------------------+----------+---+-------+\n",
      "\n",
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from google.cloud import storage\n",
    "storage_client = storage.client(project = project_id)\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blobs = bucket.list_blobs(prefix = subdirectory_name)\n",
    "for obj in blobs:\n",
    "    blob1 = obj.name\n",
    "    blob = bucket.blob(blob1)\n",
    "    blob.download_to_filename(blob1)\n",
    "    \n",
    "#upload file \n",
    "path = (\"{0}/output/{1}\".format(subdirectory_name, f_name))\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(path)\n",
    "blob.upload_from_filename(f_name)\"\"\"\n",
    "\n",
    "#df = spark.read.json('sample_data.json')\n",
    "\n",
    "f_df = spark.read.format('json')\\\n",
    "           .option('inferschema', True)\\\n",
    "           .option('multiline', True)\\\n",
    "           .load('sample_data.json')\n",
    "f_df.show()     \n",
    "f_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74f26655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------------+-------------+\n",
      "| id|   name|department| address_city|address_state|\n",
      "+---+-------+----------+-------------+-------------+\n",
      "|  1|  Alice|        HR|     New York|           NY|\n",
      "|  2|    Bob|        IT|San Francisco|           CA|\n",
      "|  3|Charlie|   Finance|      Chicago|           IL|\n",
      "+---+-------+----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = f_df.select('id','name','department', 'address.city','address.state')\\\n",
    "                                           .withColumnRenamed('city','address_city')\\\n",
    "                                           .withColumnRenamed('state','address_state')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "efda0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.mode('overwrite').parquet('path')\n",
    "\"\"\"df.write.format('json')\\\n",
    "        .option('header', True)\\\n",
    "        .option('mode', 'overwrite')\\\n",
    "        .option('path','newJson/')\\\n",
    "        .save()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba0ec1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48b11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd9221cb",
   "metadata": {},
   "source": [
    "## 26 \n",
    "\n",
    "split the values in this column into multiple rows to make the dataset suitable for downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "beb9227d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| ID|               Tags|\n",
      "+---+-------------------+\n",
      "|  1|apple,banana,orange|\n",
      "|  2|       mango,grapes|\n",
      "|  3|          pineapple|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, \"apple,banana,orange\"), \n",
    " (2, \"mango,grapes\"), (3, \"pineapple\") ] \n",
    "\n",
    "columns = [\"ID\", \"Tags\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21ea1c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| ID|      Tag|\n",
      "+---+---------+\n",
      "|  1|    apple|\n",
      "|  1|   banana|\n",
      "|  1|   orange|\n",
      "|  2|    mango|\n",
      "|  2|   grapes|\n",
      "|  3|pineapple|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Tag', explode(split(df['Tags'], ','))).drop('Tags').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101ceb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfe5c39e",
   "metadata": {},
   "source": [
    " 27 \n",
    "\n",
    "Imagine you're analyzing the monthly sales performance of a company across different regions. You want to calculate:\n",
    "\n",
    "- The cumulative sales for each region over months.\n",
    "- The rank of each month based on sales within the same region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfe905ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|Region|Month|Sales|\n",
      "+------+-----+-----+\n",
      "|  East|  Jan|  200|\n",
      "|  East|  Feb|  300|\n",
      "|  East|  Mar|  250|\n",
      "|  West|  Jan|  400|\n",
      "|  West|  Feb|  350|\n",
      "|  West|  Mar|  450|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"East\", \"Jan\", 200), (\"East\", \"Feb\", 300), \n",
    "(\"East\", \"Mar\", 250), (\"West\", \"Jan\", 400), \n",
    "(\"West\", \"Feb\", 350), (\"West\", \"Mar\", 450) ]\n",
    "\n",
    "columns = [\"Region\", \"Month\", \"Sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70ec4b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+----------------+\n",
      "|Region|Month|Sales|Cumulative_Sales|\n",
      "+------+-----+-----+----------------+\n",
      "|  East|  Jan|  200|             200|\n",
      "|  East|  Mar|  250|             450|\n",
      "|  East|  Feb|  300|             750|\n",
      "|  West|  Feb|  350|             350|\n",
      "|  West|  Jan|  400|             750|\n",
      "|  West|  Mar|  450|            1200|\n",
      "+------+-----+-----+----------------+\n",
      "\n",
      "+------+-----+-----+----------------+----+\n",
      "|Region|Month|Sales|Cumulative_Sales|Rank|\n",
      "+------+-----+-----+----------------+----+\n",
      "|  East|  Jan|  200|             200|   1|\n",
      "|  East|  Mar|  250|             450|   2|\n",
      "|  East|  Feb|  300|             750|   3|\n",
      "|  West|  Feb|  350|             350|   1|\n",
      "|  West|  Jan|  400|             750|   2|\n",
      "|  West|  Mar|  450|            1200|   3|\n",
      "+------+-----+-----+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a window partitionBy by Region and ordered by Sales\n",
    "window_spec = Window.partitionBy('Region').orderBy('Sales')\n",
    "\n",
    "# Add cumulative sum and rank columns:\n",
    "res_df = df.withColumn('Cumulative_Sales', sum('Sales').over(window_spec))\n",
    "res_df.show()\n",
    "res_df = res_df.withColumn('Rank', rank().over(window_spec))\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1abad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309178f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2180875a",
   "metadata": {},
   "source": [
    " 28\n",
    "\n",
    "You are working as a Data Engineer and need to clean up a dataset that contains customer order information. The dataset includes details such as the customer ID, order ID, order date, and the total amount. Due to a data processing issue, some rows are duplicated, and you need to remove duplicates based on a composite key of customer_id and order_id, keeping only the latest order (based on the order_date).\n",
    "\n",
    "You need to remove the duplicate rows based on the composite key (customer_id, order_id) and retain only the row with the latest order_date for each combination of customer_id and order_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9046ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+\n",
      "|customer_id|order_id|order_date|total_amount|\n",
      "+-----------+--------+----------+------------+\n",
      "|        101|    1001|2025-01-15|       500.0|\n",
      "|        102|    1002|2025-01-14|       300.0|\n",
      "|        101|    1001|2025-01-17|       550.0|\n",
      "|        103|    1003|2025-01-16|       450.0|\n",
      "|        102|    1002|2025-01-18|       320.0|\n",
      "|        103|    1003|2025-01-19|       460.0|\n",
      "+-----------+--------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (101, 1001, \"2025-01-15\", 500.00), (102, 1002, \"2025-01-14\", 300.00), (101, 1001, \"2025-01-17\", 550.00), (103, 1003, \"2025-01-16\", 450.00),\n",
    "(102, 1002, \"2025-01-18\", 320.00), (103, 1003, \"2025-01-19\", 460.00) ]\n",
    "\n",
    "schema = [\"customer_id\", \"order_id\", \"order_date\", \"total_amount\"] \n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad4b125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+\n",
      "|customer_id|order_id|order_date|total_amount|\n",
      "+-----------+--------+----------+------------+\n",
      "|        101|    1001|2025-01-15|       500.0|\n",
      "|        102|    1002|2025-01-14|       300.0|\n",
      "|        101|    1001|2025-01-17|       550.0|\n",
      "|        103|    1003|2025-01-16|       450.0|\n",
      "|        102|    1002|2025-01-18|       320.0|\n",
      "|        103|    1003|2025-01-19|       460.0|\n",
      "+-----------+--------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.drop_duplicates(['customer_id','order_id']).show()\n",
    "\n",
    "# Cast the order_date to DateType for proper compasion\n",
    "df = df.withColumn('order_date', to_date(df['order_date'], 'yyyy-MM-dd'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "204612ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+-------+\n",
      "|customer_id|order_id|order_date|total_amount|row_num|\n",
      "+-----------+--------+----------+------------+-------+\n",
      "|        101|    1001|2025-01-17|       550.0|      1|\n",
      "|        101|    1001|2025-01-15|       500.0|      2|\n",
      "|        102|    1002|2025-01-18|       320.0|      1|\n",
      "|        102|    1002|2025-01-14|       300.0|      2|\n",
      "|        103|    1003|2025-01-19|       460.0|      1|\n",
      "|        103|    1003|2025-01-16|       450.0|      2|\n",
      "+-----------+--------+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window specification to get the latest order per customer_id and order_id\n",
    "\n",
    "window_spec = Window.partitionBy('customer_id', 'order_id').orderBy(col('order_date').desc())\n",
    "\n",
    "# Add row_number to identify the latest record\n",
    "df_with_row_number = df.withColumn('row_num', row_number().over(window_spec))\n",
    "df_with_row_number.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58613696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+\n",
      "|customer_id|order_id|order_date|total_amount|\n",
      "+-----------+--------+----------+------------+\n",
      "|        101|    1001|2025-01-17|       550.0|\n",
      "|        102|    1002|2025-01-18|       320.0|\n",
      "|        103|    1003|2025-01-19|       460.0|\n",
      "+-----------+--------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to get onnly the latest record \n",
    "\n",
    "res_df = df_with_row_number.filter(col('row_num') == 1).drop('row_num')\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c743c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65426db5",
   "metadata": {},
   "source": [
    "29 The columns contain different types of data, including numeric, categorical, and string values. Your objective is to:\n",
    "\n",
    "1. Fill numeric columns with the median value.\n",
    "2. Fill categorical columns with the most frequent value.\n",
    "3. Fill string columns with \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3b6282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Customer_ID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Last_Visit: string (nullable = true)\n",
      " |-- Purchase_Amount: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+-----------+----+------+------+----------+---------------+\n",
      "|Customer_ID| Age|Region|Gender|Last_Visit|Purchase_Amount|\n",
      "+-----------+----+------+------+----------+---------------+\n",
      "|          1|  25| North|     M|2025-01-01|            150|\n",
      "|          2|NULL|  East|  NULL|2025-01-02|           NULL|\n",
      "|          3|  30| South|     F|      NULL|            200|\n",
      "|          4|  22|  NULL|     M|2025-01-03|            180|\n",
      "|          5|  28|  West|     F|      NULL|           NULL|\n",
      "+-----------+----+------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, 25, 'North', 'M', '2025-01-01', 150),\n",
    "(2, None, 'East', None, '2025-01-02', None),\n",
    "(3, 30, 'South', 'F', None, 200),\n",
    "(4, 22, None, 'M', '2025-01-03', 180),\n",
    "(5, 28, 'West', 'F', None, None), ]\n",
    "schema = StructType([StructField('Customer_ID', IntegerType\n",
    "                                 (), True),\n",
    "                    StructField('Age', IntegerType(), True),\n",
    "                    StructField('Region', StringType(), True),\n",
    "                    StructField('Gender', StringType(), True),\n",
    "                     StructField('Last_Visit', StringType(), True),\n",
    "                     StructField('Purchase_Amount', IntegerType(), True),\n",
    "                    ])\n",
    "\n",
    "#columns = ['Customer_ID', 'Age', 'Region', 'Gender', 'Last_Visit', 'Purchase_Amount'] \n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(df.printSchema())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82d07d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_types:  [('Customer_ID', 'int'), ('Age', 'int'), ('Region', 'string'), ('Gender', 'string'), ('Last_Visit', 'string'), ('Purchase_Amount', 'int')]\n",
      "+-----------+---+-------+-------+----------+---------------+\n",
      "|Customer_ID|Age| Region| Gender|Last_Visit|Purchase_Amount|\n",
      "+-----------+---+-------+-------+----------+---------------+\n",
      "|          1| 25|  North|      M|2025-01-01|            150|\n",
      "|          2| 25|   East|Unknown|2025-01-02|            180|\n",
      "|          3| 30|  South|      F|   Unknown|            200|\n",
      "|          4| 22|Unknown|      M|2025-01-03|            180|\n",
      "|          5| 28|   West|      F|   Unknown|            180|\n",
      "+-----------+---+-------+-------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to fill missing values dynamically\n",
    "def fill_missing_values(df):\n",
    "    \n",
    "    column_types = df.dtypes\n",
    "    print('column_types: ', column_types)\n",
    "    \n",
    "    # loop through each column based on type\n",
    "    for column, dtype in column_types:\n",
    "        \n",
    "        if dtype == 'int' or dtype == 'double' or dtype == 'long':\n",
    "            median_value = df.approxQuantile(column, [0.5], 0)[0]\n",
    "            df = df.fillna({column: median_value})\n",
    "            \n",
    "        elif dtype == 'string':\n",
    "            df = df.fillna({column: 'Unknown'})\n",
    "            \n",
    "        else:\n",
    "            df = df.fillna({column: 'Unknown'})\n",
    "        \n",
    "    return df\n",
    "filled_df = fill_missing_values(df)\n",
    "filled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8def4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e13499d",
   "metadata": {},
   "source": [
    "30 Validate the date format and filter rows where input_date matches the format \"yyyy-MM-dd\".\n",
    "\n",
    "- Transform valid dates into the format \"MM/dd/yyyy\".\n",
    "- For invalid dates, replace them with the string \"Invalid Date\".\n",
    "- Output the transformed DataFrame with a new column named validated_date.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5d7806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|input_date|\n",
      "+----------+\n",
      "|2023-12-31|\n",
      "|31-12-2023|\n",
      "|2023/12/31|\n",
      "|2024-01-01|\n",
      "|13-01-2023|\n",
      "|   invalid|\n",
      "|2022-02-28|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"2023-12-31\",), (\"31-12-2023\",),\n",
    "(\"2023/12/31\",), (\"2024-01-01\",),\n",
    "(\"13-01-2023\",), (\"invalid\",), (\"2022-02-28\",) ]\n",
    "columns = [\"input_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40796307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|input_date|validated_date|\n",
      "+----------+--------------+\n",
      "|2023-12-31|    12/31/2023|\n",
      "|31-12-2023|  Invalid_date|\n",
      "|2023/12/31|  Invalid_date|\n",
      "|2024-01-01|    01/01/2024|\n",
      "|13-01-2023|  Invalid_date|\n",
      "|   invalid|  Invalid_date|\n",
      "|2022-02-28|    02/28/2022|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df.withColumn('validated_date', when(to_date(col('input_date'), 'yyyy-MM-dd').isNotNull(), date_format(to_date(col('input_date'), 'yyyy-MM-dd'), 'MM/dd/yyyy')).otherwise('Invalid_date'))\n",
    "result_df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee862f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4bc1191",
   "metadata": {},
   "source": [
    " 34\n",
    "\n",
    "You are given a dataset of sales transactions for multiple stores and products.\n",
    "- Calculate the percentage contribution of each product's sales to the total sales of its store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee0516c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|StoreID|Product|Sales|\n",
      "+-------+-------+-----+\n",
      "|     S1|     P1|  100|\n",
      "|     S1|     P2|  200|\n",
      "|     S1|     P3|  300|\n",
      "|     S2|     P1|  400|\n",
      "|     S2|     P2|  100|\n",
      "|     S2|     P3|  500|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"S1\", \"P1\", 100), (\"S1\", \"P2\", 200),\n",
    "(\"S1\", \"P3\", 300), (\"S2\", \"P1\", 400),\n",
    "(\"S2\", \"P2\", 100), (\"S2\", \"P3\", 500) ]\n",
    "columns = [\"StoreID\", \"Product\", \"Sales\"] \n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9220d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|StoreID|Total_Sales|\n",
      "+-------+-----------+\n",
      "|     S1|        600|\n",
      "|     S2|       1000|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('StoreID').agg(sum('Sales').alias('Total_Sales'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "062a90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----------+\n",
      "|StoreID|Product|Sales|Total_Sales|\n",
      "+-------+-------+-----+-----------+\n",
      "|     S1|     P1|  100|        600|\n",
      "|     S1|     P2|  200|        600|\n",
      "|     S1|     P3|  300|        600|\n",
      "|     S2|     P1|  400|       1000|\n",
      "|     S2|     P2|  100|       1000|\n",
      "|     S2|     P3|  500|       1000|\n",
      "+-------+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = df.join(df1, on='StoreID', how= 'inner')\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284210dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----------+------+\n",
      "|StoreID|Product|Sales|Total_Sales|percnt|\n",
      "+-------+-------+-----+-----------+------+\n",
      "|     S1|     P1|  100|        600| 16.67|\n",
      "|     S1|     P2|  200|        600| 33.33|\n",
      "|     S1|     P3|  300|        600|  50.0|\n",
      "|     S2|     P1|  400|       1000|  40.0|\n",
      "|     S2|     P2|  100|       1000|  10.0|\n",
      "|     S2|     P3|  500|       1000|  50.0|\n",
      "+-------+-------+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.withColumn('percnt', round((col('Sales') / col('Total_sales'))*100,2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d5967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ff55a6f",
   "metadata": {},
   "source": [
    " 35\n",
    "\n",
    "You are working as a Data Engineer at a retail company. The marketing team has provided a dataset of customer purchases to analyze the relationship between the amount spent on advertisements and the revenue generated. \n",
    "- Using PySpark, compute the correlation between the \"Ad_Spend\" and \"Revenue\" columns to determine if there's a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df8646ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+\n",
      "|Customer_ID|Ad_Spend|Revenue|\n",
      "+-----------+--------+-------+\n",
      "|       C001|    2000|  25000|\n",
      "|       C002|    1500|  23000|\n",
      "|       C003|    3000|  40000|\n",
      "|       C004|    1200|  18000|\n",
      "|       C005|    2500|  30000|\n",
      "+-----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ StructField(\"Customer_ID\", StringType(), True), StructField(\"Ad_Spend\", IntegerType(), True), StructField(\"Revenue\", IntegerType(), True) ])\n",
    "data = [ (\"C001\", 2000, 25000), (\"C002\", 1500, 23000),\n",
    "(\"C003\", 3000, 40000), (\"C004\", 1200, 18000),\n",
    "(\"C005\", 2500, 30000) ] \n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3febbf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between ad_spend and revenue is: 0.9704535552410213\n"
     ]
    }
   ],
   "source": [
    "correlation = df.stat.corr('Ad_Spend', 'Revenue')\n",
    "print('The correlation between ad_spend and revenue is:' ,correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40323a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8911103",
   "metadata": {},
   "source": [
    " 36\n",
    "\n",
    "You are given a large e-commerce transaction dataset stored in a partitioned format based on country. \n",
    "- Count the distinct number of products purchased (product_id) for each customer_id in every country. The result should include the country, customer ID, and the distinct product count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56fad9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+\n",
      "|country|customer_id|product_id|\n",
      "+-------+-----------+----------+\n",
      "|    USA|        101|      P001|\n",
      "|    USA|        101|      P002|\n",
      "|    USA|        101|      P001|\n",
      "|    USA|        102|      P003|\n",
      "|    USA|        102|      P003|\n",
      "|     UK|        201|      P004|\n",
      "|     UK|        201|      P005|\n",
      "|     UK|        202|      P004|\n",
      "|     UK|        202|      P005|\n",
      "|     UK|        202|      P004|\n",
      "+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"USA\", 101, \"P001\"), \n",
    "(\"USA\", 101, \"P002\"), (\"USA\", 101, \"P001\"), \n",
    "(\"USA\", 102, \"P003\"), (\"USA\", 102, \"P003\"), \n",
    "(\"UK\", 201, \"P004\"), (\"UK\", 201, \"P005\"), \n",
    "(\"UK\", 202, \"P004\"), (\"UK\", 202, \"P005\"), (\"UK\", 202, \"P004\") ]\n",
    "\n",
    "columns = [\"country\", \"customer_id\", \"product_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "575b4c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------------+\n",
      "|country|customer_id|count(DISTINCT product_id)|\n",
      "+-------+-----------+--------------------------+\n",
      "|    USA|        101|                         2|\n",
      "|     UK|        202|                         2|\n",
      "|     UK|        201|                         2|\n",
      "|    USA|        102|                         1|\n",
      "+-------+-----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('country', 'customer_id').agg(countDistinct(col('product_id'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94eb9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63194096",
   "metadata": {},
   "source": [
    "$Broadcast$ the smaller DataFrame (product_data). 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9400748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----------+\n",
      "|sale_id|product_id|quantity| sale_date|\n",
      "+-------+----------+--------+----------+\n",
      "|      1|       101|       5|2025-01-01|\n",
      "|      2|       102|       3|2025-01-02|\n",
      "|      3|       103|       2|2025-01-03|\n",
      "|      4|       101|       1|2025-01-04|\n",
      "|      5|       104|       4|2025-01-05|\n",
      "|      6|       105|       6|2025-01-06|\n",
      "+-------+----------+--------+----------+\n",
      "\n",
      "+----------+------------+-----------+-----+\n",
      "|product_id|product_name|   category|price|\n",
      "+----------+------------+-----------+-----+\n",
      "|       101|      Laptop|Electronics| 1000|\n",
      "|       102|       Phone|Electronics|  500|\n",
      "|       103|  Headphones|Accessories|  150|\n",
      "|       104|      Tablet|Electronics|  600|\n",
      "|       105|  Smartwatch|Accessories|  200|\n",
      "+----------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = [ (1, 101, 5, '2025-01-01'), (2, 102, 3, '2025-01-02'),\n",
    "(3, 103, 2, '2025-01-03'), (4, 101, 1, '2025-01-04'),\n",
    "(5, 104, 4, '2025-01-05'), (6, 105, 6, '2025-01-06'), ]\n",
    "\n",
    "# product_data (Small DataFrame)\n",
    "\n",
    "product_data = [ (101, 'Laptop', 'Electronics', 1000),\n",
    "(102, 'Phone', 'Electronics', 500),\n",
    "(103, 'Headphones', 'Accessories', 150),\n",
    "(104, 'Tablet', 'Electronics', 600),\n",
    "(105, 'Smartwatch', 'Accessories', 200), ] \n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, ['sale_id', 'product_id','quantity', 'sale_date'])\n",
    "product_df= spark.createDataFrame(product_data, ['product_id', 'product_name', 'category', 'price'])\n",
    "sales_df.show()\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2738e9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+----------+------------+-----------+-----+\n",
      "|product_id|sale_id|quantity|sale_date |product_name|category   |price|\n",
      "+----------+-------+--------+----------+------------+-----------+-----+\n",
      "|101       |1      |5       |2025-01-01|Laptop      |Electronics|1000 |\n",
      "|102       |2      |3       |2025-01-02|Phone       |Electronics|500  |\n",
      "|103       |3      |2       |2025-01-03|Headphones  |Accessories|150  |\n",
      "|101       |4      |1       |2025-01-04|Laptop      |Electronics|1000 |\n",
      "|104       |5      |4       |2025-01-05|Tablet      |Electronics|600  |\n",
      "|105       |6      |6       |2025-01-06|Smartwatch  |Accessories|200  |\n",
      "+----------+-------+--------+----------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "prod_df_broadcast = broadcast(product_df)\n",
    "\n",
    "joined_df = sales_df.join(prod_df_broadcast, on='product_id', how='inner')\n",
    "joined_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa7402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "170ccafe",
   "metadata": {},
   "source": [
    " 39\n",
    "\n",
    "You are working with large datasets in PySpark and need to join two DataFrames. However, one of the tables has highly skewed data, causing performance issues due to data shuffling. How would you optimize this join using salting techniques?\n",
    "You are given the following sample datasets:\n",
    "\n",
    "sales_df (Fact Table - Large Dataset, Highly Skewed on store_id)\n",
    "Your task is to perform an optimized join between sales_df and store_df on store_id, ensuring that the skewness does not degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d478d53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|store_id|product_id|amount|\n",
      "+--------+----------+------+\n",
      "|     101|      P001|   100|\n",
      "|     101|      P002|   200|\n",
      "|     101|      P003|   150|\n",
      "|     102|      P004|   300|\n",
      "|     103|      P005|   400|\n",
      "|     101|      P006|   500|\n",
      "|     104|      P007|   250|\n",
      "+--------+----------+------+\n",
      "\n",
      "+--------+----------+\n",
      "|store_id|store_name|\n",
      "+--------+----------+\n",
      "|     101|   Walmart|\n",
      "|     102|    Target|\n",
      "|     103|    Costco|\n",
      "|     104|   BestBuy|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = [ (101, \"P001\", 100), (101, \"P002\", 200), (101, \"P003\", 150), (102, \"P004\", 300), \n",
    "              (103, \"P005\", 400), (101, \"P006\", 500), (104, \"P007\", 250) ] \n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"store_id\", \"product_id\", \"amount\"]) \n",
    "sales_df.show()\n",
    "\n",
    "store_data = [(101, \"Walmart\"), (102, \"Target\"), (103, \"Costco\"), (104, \"BestBuy\")] \n",
    "store_df = spark.createDataFrame(store_data, [\"store_id\", \"store_name\"]) \n",
    "store_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9acce019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----+---------------+\n",
      "|store_id|product_id|amount|salt|salted_store_id|\n",
      "+--------+----------+------+----+---------------+\n",
      "|     101|      P001|   100|   1|          101_1|\n",
      "|     101|      P002|   200|   0|          101_0|\n",
      "|     101|      P003|   150|   0|          101_0|\n",
      "|     102|      P004|   300|   0|          102_0|\n",
      "|     103|      P005|   400|   0|          103_0|\n",
      "|     101|      P006|   500|   2|          101_2|\n",
      "|     104|      P007|   250|   0|          104_0|\n",
      "+--------+----------+------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Adding Salt to skewed 'sales_df'\n",
    "\n",
    "num_salt_keys = 3  # Define the range of salt keys \n",
    "\n",
    "sales_df_salted = sales_df.withColumn('salt', floor(rand() * num_salt_keys))\\\n",
    "                          .withColumn('salted_store_id', concat_ws(\"_\", col('store_id'), col('salt')))\n",
    "sales_df_salted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c81b3f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----+---------------+\n",
      "|store_id|store_name|salt|salted_store_id|\n",
      "+--------+----------+----+---------------+\n",
      "|     101|   Walmart|   0|          101_0|\n",
      "|     101|   Walmart|   1|          101_1|\n",
      "|     101|   Walmart|   2|          101_2|\n",
      "|     102|    Target|   0|          102_0|\n",
      "|     102|    Target|   1|          102_1|\n",
      "|     102|    Target|   2|          102_2|\n",
      "|     103|    Costco|   0|          103_0|\n",
      "|     103|    Costco|   1|          103_1|\n",
      "|     103|    Costco|   2|          103_2|\n",
      "|     104|   BestBuy|   0|          104_0|\n",
      "|     104|   BestBuy|   1|          104_1|\n",
      "|     104|   BestBuy|   2|          104_2|\n",
      "+--------+----------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Expanding 'store_df' for Join compatibility \n",
    "\n",
    "expanded_store_df = store_df.crossJoin(spark.range(0, num_salt_keys).toDF('salt'))\\\n",
    "                    .withColumn('salted_store_id', concat_ws('_', col('store_id'), col('salt')))\n",
    "expanded_store_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebd7faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+--------+----------+\n",
      "|store_id|product_id|amount|store_id|store_name|\n",
      "+--------+----------+------+--------+----------+\n",
      "|     101|      P002|   200|     101|   Walmart|\n",
      "|     101|      P003|   150|     101|   Walmart|\n",
      "|     101|      P001|   100|     101|   Walmart|\n",
      "|     101|      P006|   500|     101|   Walmart|\n",
      "|     102|      P004|   300|     102|    Target|\n",
      "|     103|      P005|   400|     103|    Costco|\n",
      "|     104|      P007|   250|     104|   BestBuy|\n",
      "+--------+----------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Performing the Optimized Join on Salted Keys\n",
    "\n",
    "joined_df = sales_df_salted.join(expanded_store_df,'salted_store_id', 'inner').drop('salted_store_id', 'salt')\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e8244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9f03fd0",
   "metadata": {},
   "source": [
    "\"\"\" 40\n",
    "\n",
    "You are working as a Data Engineer at a fintech company. Your team is working on integrating two datasets:\n",
    "\n",
    "1. Customer Transactions Data (transactions_df) - Contains customer transactions with columns: customer_id, txn_id, amount, and txn_date. \n",
    "\n",
    "2. Customer Profile Data (profile_df) - Contains customer information with columns: customer_id, name, age, and txn_id (latest transaction ID for reference).\n",
    "\n",
    " The requirement is to merge these two DataFrames on customer_id while keeping track of:\n",
    "\n",
    "Conflicting column names (txn_id) should be renamed properly.\n",
    "\n",
    "If a customer exists in profile_df but not in transactions_df, the row should still be present with NULL values for transaction-related columns.\n",
    "\n",
    "Your task is to write an optimized PySpark code to achieve this.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0224c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----------+\n",
      "|customer_id|txn_id|amount|  txn_date|\n",
      "+-----------+------+------+----------+\n",
      "|        101|  T001|   500|2024-08-10|\n",
      "|        102|  T002|  1200|2024-08-09|\n",
      "|        103|  T003|   300|2024-08-08|\n",
      "|        104|  T004|   450|2024-08-07|\n",
      "+-----------+------+------+----------+\n",
      "\n",
      "+-----------+----+---+------+\n",
      "|customer_id|name|age|txn_id|\n",
      "+-----------+----+---+------+\n",
      "|        101|John| 30|  T001|\n",
      "|        102|Emma| 27|  T005|\n",
      "|        103|Alex| 35|  T003|\n",
      "|        105| Sam| 40|  T006|\n",
      "+-----------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_data = [ (101, \"T001\", 500, \"2024-08-10\"), (102, \"T002\", 1200, \"2024-08-09\"), \n",
    "                     (103, \"T003\", 300, \"2024-08-08\"), (104, \"T004\", 450, \"2024-08-07\"), ] \n",
    "\n",
    "profile_data = [ (101, \"John\", 30, \"T001\"), (102, \"Emma\", 27, \"T005\"), \n",
    "                (103, \"Alex\", 35, \"T003\"), (105, \"Sam\", 40, \"T006\"), ]\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_data, [\"customer_id\", \"txn_id\", \"amount\", \"txn_date\"])\n",
    "transactions_df.show()\n",
    "\n",
    "profile_df = spark.createDataFrame(profile_data, [\"customer_id\", \"name\", \"age\", \"txn_id\"])\n",
    "profile_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764ca0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+-----------+\n",
      "|customer_id|name|age|last_txn_id|\n",
      "+-----------+----+---+-----------+\n",
      "|        101|John| 30|       T001|\n",
      "|        102|Emma| 27|       T005|\n",
      "|        103|Alex| 35|       T003|\n",
      "|        105| Sam| 40|       T006|\n",
      "+-----------+----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df = profile_df.withColumnRenamed('txn_id', 'last_txn_id')\n",
    "profile_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0597d463",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+-----------+------+------+----------+\n",
      "|customer_id|name| age|last_txn_id|txn_id|amount|  txn_date|\n",
      "+-----------+----+----+-----------+------+------+----------+\n",
      "|        101|John|  30|       T001|  T001|   500|2024-08-10|\n",
      "|        102|Emma|  27|       T005|  T002|  1200|2024-08-09|\n",
      "|        103|Alex|  35|       T003|  T003|   300|2024-08-08|\n",
      "|        104|NULL|NULL|       NULL|  T004|   450|2024-08-07|\n",
      "|        105| Sam|  40|       T006|  NULL|  NULL|      NULL|\n",
      "+-----------+----+----+-----------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df.join(transactions_df, on = \"customer_id\", how = \"full_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ab082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5d0009c",
   "metadata": {},
   "source": [
    " 42\n",
    "\n",
    "You are given an employee dataset containing information about employees and their managers. Each employee has a manager_id that refers to another employee in the same table. Your task is to use self-join to find hierarchical relationships between employees, such as finding all employees under a specific manager or the reporting hierarchy of an employee.\n",
    "\n",
    "Interview Task\n",
    "- Write a PySpark self-join query to find the direct reports of each manager. Additionally, extend the logic to find all hierarchical relationships up to any level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd26875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+\n",
      "|employee_id|employee_name|manager_id|\n",
      "+-----------+-------------+----------+\n",
      "|          1|        Alice|      NULL|\n",
      "|          2|          Bob|         1|\n",
      "|          3|      Charlie|         1|\n",
      "|          4|        David|         2|\n",
      "|          5|          Eva|         2|\n",
      "|          6|        Frank|         3|\n",
      "|          7|        Grace|         3|\n",
      "+-----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, \"Alice\", None), (2, \"Bob\", 1),\n",
    "(3, \"Charlie\", 1), (4, \"David\", 2),\n",
    "(5, \"Eva\", 2), (6, \"Frank\", 3), (7, \"Grace\", 3) ]\n",
    "\n",
    "columns = [\"employee_id\", \"employee_name\", \"manager_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4516b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|employee_name|employee_name|\n",
      "+-------------+-------------+\n",
      "|         NULL|        Alice|\n",
      "|        Alice|          Bob|\n",
      "|        Alice|      Charlie|\n",
      "|          Bob|        David|\n",
      "|          Bob|          Eva|\n",
      "|      Charlie|        Frank|\n",
      "|      Charlie|        Grace|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('emp').join(df.alias('mgr'), col('mgr.employee_id') == col('emp.manager_id'), 'left')\\\n",
    "           .select(col('mgr.employee_name'), col('emp.employee_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5dd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c358dda",
   "metadata": {},
   "source": [
    " 43\n",
    "\n",
    "You are working as a Data Engineer, and the company has a log system where timestamps are recorded for every user action (e.g., when the user logs in and logs out). Your manager wants to know how much time each user spends between log in and log out.\n",
    "\n",
    "calculate the difference between the logout_timestamp and login_timestamp in hours, minutes, and seconds. The result should be formatted like \"HH:mm:ss\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6921de45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|\n",
      "+-------+-------------------+-------------------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, \"2025-01-31 08:00:00\", \"2025-01-31 10:30:45\"),\n",
    "(2, \"2025-01-31 09:00:30\", \"2025-01-31 12:15:10\"),\n",
    "(3, \"2025-01-31 07:45:00\", \"2025-01-31 09:00:15\") ]\n",
    "\n",
    "columns = [\"user_id\", \"login_timestamp\", \"logout_timestamp\"] \n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32ab0aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|\n",
      "+-------+-------------------+-------------------+----------+-----------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|\n",
      "+-------+-------------------+-------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('login_time', unix_timestamp('login_timestamp'))\n",
    "df = df.withColumn('logout_time', unix_timestamp('logout_timestamp'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb3c4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+----------------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|duration_seconds|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|            9045|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|           11680|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|            4515|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate difference \n",
    "\n",
    "df = df.withColumn('duration_seconds', col('logout_time')-col('login_time'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b44e132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|duration_seconds|hours|minutes|seconds|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|            9045|    2|     30|     45|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|           11680|    3|     14|     40|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|            4515|    1|     15|     15|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate hours, minutes, and seconds\n",
    "\n",
    "df = df.withColumn('hours', (col('duration_seconds') / 3600).cast('int'))\n",
    "df = df.withColumn('minutes', ((col('duration_seconds') % 3600) / 60).cast('int'))\n",
    "df = df.withColumn('seconds', (col('duration_seconds') % 60).cast('int'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "520e24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+------------------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|duration_seconds|hours|minutes|seconds|formatted_duration|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+------------------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|            9045|    2|     30|     45|          02:30:45|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|           11680|    3|     14|     40|          03:14:40|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|            4515|    1|     15|     15|          01:15:15|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('formatted_duration', expr(\"lpad(hours, 2,'0') ||':' || lpad(minutes, 2,'0') ||':' || lpad(seconds, 2,'0')\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5131354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|user_id|formatted_duration|\n",
      "+-------+------------------+\n",
      "|1      |02:30:45          |\n",
      "|2      |03:14:40          |\n",
      "|3      |01:15:15          |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('user_id', 'formatted_duration').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5a20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93371807",
   "metadata": {},
   "source": [
    " 44 \n",
    "\n",
    "You have a dataset of user activities in an e-commerce application, where each row represents an activity performed by a user. The dataset contains duplicate activity entries (based on user and activity type) and you need to remove the duplicates. Furthermore, you want to keep only the most recent record for each user, based on a timestamp column.\n",
    "\n",
    "Problem\n",
    "- Remove duplicates based on user_id and activity_type.\n",
    "- Keep only the most recent activity_timestamp for each user and activity type combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af70c991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+\n",
      "|user_id|activity_type| activity_timestamp|\n",
      "+-------+-------------+-------------------+\n",
      "|      1|        login|2025-02-01 10:00:00|\n",
      "|      1| view_product|2025-02-01 10:05:00|\n",
      "|      1|        login|2025-02-01 10:30:00|\n",
      "|      2|     purchase|2025-02-01 11:00:00|\n",
      "|      2|        login|2025-02-01 11:15:00|\n",
      "|      2| view_product|2025-02-01 11:30:00|\n",
      "|      3|        login|2025-02-01 12:00:00|\n",
      "|      3|        login|2025-02-01 12:05:00|\n",
      "+-------+-------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- activity_type: string (nullable = true)\n",
      " |-- activity_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, 'login', '2025-02-01 10:00:00'), (1, 'view_product', '2025-02-01 10:05:00'), \n",
    "        (1, 'login', '2025-02-01 10:30:00'), (2, 'purchase', '2025-02-01 11:00:00'), (2, 'login', '2025-02-01 11:15:00'), \n",
    "(2, 'view_product', '2025-02-01 11:30:00'), (3, 'login', '2025-02-01 12:00:00'), (3, 'login', '2025-02-01 12:05:00') ]\n",
    " \n",
    "df = spark.createDataFrame(data, [\"user_id\", \"activity_type\", \"activity_timestamp\"])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8755fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+\n",
      "|user_id|activity_type| activity_timestamp|\n",
      "+-------+-------------+-------------------+\n",
      "|      1|        login|2025-02-01 10:00:00|\n",
      "|      1| view_product|2025-02-01 10:05:00|\n",
      "|      1|        login|2025-02-01 10:30:00|\n",
      "|      2|     purchase|2025-02-01 11:00:00|\n",
      "|      2|        login|2025-02-01 11:15:00|\n",
      "|      2| view_product|2025-02-01 11:30:00|\n",
      "|      3|        login|2025-02-01 12:00:00|\n",
      "|      3|        login|2025-02-01 12:05:00|\n",
      "+-------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('activity_timestamp', col('activity_timestamp').cast('timestamp'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba31b675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+-------+\n",
      "|user_id|activity_type| activity_timestamp|row_num|\n",
      "+-------+-------------+-------------------+-------+\n",
      "|      1|        login|2025-02-01 10:30:00|      1|\n",
      "|      1|        login|2025-02-01 10:00:00|      2|\n",
      "|      1| view_product|2025-02-01 10:05:00|      1|\n",
      "|      2|        login|2025-02-01 11:15:00|      1|\n",
      "|      2|     purchase|2025-02-01 11:00:00|      1|\n",
      "|      2| view_product|2025-02-01 11:30:00|      1|\n",
      "|      3|        login|2025-02-01 12:05:00|      1|\n",
      "|      3|        login|2025-02-01 12:00:00|      2|\n",
      "+-------+-------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id', 'activity_type').orderBy(col('activity_timestamp').desc())\n",
    "# Add a row number to each partition\n",
    "df_with_row_num = df.withColumn('row_num', row_number().over(window_spec))\n",
    "df_with_row_num.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58afa248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+\n",
      "|user_id|activity_type| activity_timestamp|\n",
      "+-------+-------------+-------------------+\n",
      "|      1|        login|2025-02-01 10:30:00|\n",
      "|      1| view_product|2025-02-01 10:05:00|\n",
      "|      2|        login|2025-02-01 11:15:00|\n",
      "|      2|     purchase|2025-02-01 11:00:00|\n",
      "|      2| view_product|2025-02-01 11:30:00|\n",
      "|      3|        login|2025-02-01 12:05:00|\n",
      "+-------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df_with_row_num.filter(col('row_num') == 1).drop('row_num')\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d436d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c602764",
   "metadata": {},
   "source": [
    " 46\n",
    "\n",
    "You have been given a large dataset containing employee salary details. Your goal is to optimize a PySpark job that performs a groupBy operation while minimizing the shuffle.\n",
    "\n",
    "Task:\n",
    "Write a PySpark job to calculate the total salary per department.\n",
    "Optimize the job to reduce shuffle while performing the groupBy operation.\n",
    "Explain why your optimization reduces shuffle and improves performance.\n",
    "\n",
    "Approach:\n",
    "To minimize shuffle during a groupBy operation, we should:\n",
    "Use repartition() efficiently to avoid unnecessary partitions.\n",
    "Use reduceByKey() instead of groupByKey(), as it performs local aggregation before shuffling.\n",
    "If working with a DataFrame, use partitionBy() while writing output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034825fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+------+\n",
      "|emp_id|  name| dept|salary|\n",
      "+------+------+-----+------+\n",
      "|   101| Rahul|   IT| 90000|\n",
      "|   102|  Sita|   HR| 75000|\n",
      "|   103|Vikram|   IT| 85000|\n",
      "|   104| Priya|   HR| 72000|\n",
      "|   105|Anjali|   IT| 88000|\n",
      "|   106|Manish|Sales| 67000|\n",
      "|   107|  Neha|Sales| 70000|\n",
      "+------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (101, \"Rahul\", \"IT\", 90000), (102, \"Sita\", \"HR\", 75000), \n",
    "(103, \"Vikram\", \"IT\", 85000), (104, \"Priya\", \"HR\", 72000), \n",
    "(105, \"Anjali\", \"IT\", 88000), (106, \"Manish\", \"Sales\", 67000), \n",
    "(107, \"Neha\", \"Sales\", 70000) ] \n",
    "\n",
    "columns = [\"emp_id\", \"name\", \"dept\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a06bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IT', 90000),\n",
       " ('HR', 75000),\n",
       " ('IT', 85000),\n",
       " ('HR', 72000),\n",
       " ('IT', 88000),\n",
       " ('Sales', 67000),\n",
       " ('Sales', 70000)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.groupBy('dept').sum('salary').show()\n",
    "\n",
    "#step 1: Convert to RDD and Use reduceByKey (optimized Shuffle)\n",
    "rdd = df.rdd.map(lambda x:(x[2], x[3]))   # (dept, salary)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72bf1315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HR', 147000), ('IT', 263000), ('Sales', 137000)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_result = rdd.reduceByKey(lambda x, y : x+y)  # Aggregation before shuffle\n",
    "optimized_result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335872ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "| dept|total_salary|\n",
      "+-----+------------+\n",
      "|   HR|      147000|\n",
      "|   IT|      263000|\n",
      "|Sales|      137000|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert back to dataframe\n",
    "\n",
    "optimized_df = optimized_result.toDF(['dept', 'total_salary'])\n",
    "optimized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9807cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "| dept|total_salary|\n",
      "+-----+------------+\n",
      "|   IT|      263000|\n",
      "|   HR|      147000|\n",
      "|Sales|      137000|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2:\n",
    "df_optimized =  df.repartition('dept').groupBy('dept').agg(sum('salary').alias('total_salary'))\n",
    "df_optimized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6b6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11a79bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|         1|      Laptop|\n",
      "|         2|      Tablet|\n",
      "|         3|  Smartphone|\n",
      "|         4|     Monitor|\n",
      "|         5|    Keyboard|\n",
      "+----------+------------+\n",
      "\n",
      "+-------+----------+----------+\n",
      "|sale_id|product_id| sale_date|\n",
      "+-------+----------+----------+\n",
      "|    101|         1|2025-01-01|\n",
      "|    102|         3|2025-01-02|\n",
      "|    103|         5|2025-01-03|\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write a PySpark program to identify products that have never been sold.\n",
    "\n",
    "products = spark.createDataFrame([ (1, \"Laptop\"), (2, \"Tablet\"), \n",
    "(3, \"Smartphone\"), (4, \"Monitor\"), \n",
    "(5, \"Keyboard\") ], [\"product_id\", \"product_name\"]) \n",
    "\n",
    "products.show()\n",
    "\n",
    "sales = spark.createDataFrame([ (101, 1, \"2025-01-01\"), (102, 3, \"2025-01-02\"), (103, 5, \"2025-01-03\") ], [\"sale_id\", \"product_id\", \"sale_date\"]) \n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4253915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         2|\n",
      "|         4|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.join(sales, products.product_id == sales.product_id, how='left').filter(sales.product_id.isNull()).select(products.product_id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178a194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ca84ed",
   "metadata": {},
   "source": [
    "- Replace missing values in the price column with the median price instead of the mean.\n",
    "- Drop rows where the product column is null, but if the price column is above 300, replace the null product with \"Unknown\".\n",
    "- Fill missing values in the quantity column with the average quantity rounded to the nearest integer.\n",
    "- Add a new column, total_value, which is the product of price and quantity.\n",
    "- Remove the product_id column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8ae5556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse| NULL|    NULL|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|    NULL|\n",
      "|         5|    NULL|  500|    NULL|\n",
      "+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (1, \"Laptop\", 1000, 5),\n",
    " (2, \"Mouse\", None, None),\n",
    " (3, \"Keyboard\", 50, 2),\n",
    " (4, \"Monitor\", 200, None),\n",
    " (5, None, 500, None),\n",
    "]\n",
    "columns = [\"product_id\", \"product\", \"price\", \"quantity\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ebb93f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse|  200|    NULL|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|    NULL|\n",
      "|         5|    NULL|  500|    NULL|\n",
      "+----------+--------+-----+--------+\n",
      "\n",
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse|  200|    NULL|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|    NULL|\n",
      "|         5|    NULL|  500|    NULL|\n",
      "+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace missing values in the price column with the median price instead of the mean.\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(inputCols = ['price'], outputCols = ['price']).setStrategy('median')\n",
    "df =imputer.fit(df).transform(df)\n",
    "df.show()\n",
    "\n",
    "# OR \n",
    "median_price = df.approxQuantile(\"price\", [0.5], 0.01)[0]\n",
    "df = df.fillna({\"price\": median_price})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1494ad49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse|  200|    NULL|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|    NULL|\n",
      "|         5| unknown|  500|    NULL|\n",
      "+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where the product column is null, but if the price column is above 300, replace the null product with \"Unknown\". \n",
    "\n",
    "df1 = df.withColumn('product', when((col('product').isNull()) & (col('price') > 300), 'unknown').otherwise(col('product'))).select('*')\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "60b191d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse|  200|    NULL|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|    NULL|\n",
      "|         5| unknown|  500|    NULL|\n",
      "+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.filter(col('product').isNotNull())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the quantity column with the average quantity rounded to the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4b6a4b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_quantity = df2.select(round(avg(col('quantity'))).alias('quantity')).collect()[0]['quantity']\n",
    "avg_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6f0768f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse|  200|       4|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|       4|\n",
      "|         5| unknown|  500|       4|\n",
      "+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.fillna({'quantity': avg_quantity})\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "807bf856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+-----------+\n",
      "|product_id| product|price|quantity|total_value|\n",
      "+----------+--------+-----+--------+-----------+\n",
      "|         1|  Laptop| 1000|       5|       5000|\n",
      "|         2|   Mouse|  200|       4|        800|\n",
      "|         3|Keyboard|   50|       2|        100|\n",
      "|         4| Monitor|  200|       4|        800|\n",
      "|         5| unknown|  500|       4|       2000|\n",
      "+----------+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add a new column, total_value, which is the product of price and quantity.\n",
    "\n",
    "df4 = df3.withColumn('total_value', col('price') * col('quantity')).select('*')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d861192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+-----------+\n",
      "| product|price|quantity|total_value|\n",
      "+--------+-----+--------+-----------+\n",
      "|  Laptop| 1000|       5|       5000|\n",
      "|   Mouse|  200|       4|        800|\n",
      "|Keyboard|   50|       2|        100|\n",
      "| Monitor|  200|       4|        800|\n",
      "| unknown|  500|       4|       2000|\n",
      "+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove the product_id column from the DataFrame.\n",
    "df4.drop('product_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed364ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efd665fe",
   "metadata": {},
   "source": [
    "47\n",
    "\n",
    "- Define an explicit schema for this dataset using StructType and StructField.\n",
    "- Load this data into a PySpark DataFrame using the defined schema.\n",
    "- Extract the employees who belong to the \"IT\" department and have a salary greater than 70000.\n",
    "- Split the Address column into two separate columns: City and State.\n",
    "- Save the transformed data into a Parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70464113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+--------------------+\n",
      "|Emp_ID|  Name|Age|Salary|Department|             Address|\n",
      "+------+------+---+------+----------+--------------------+\n",
      "|   101|Rajesh| 30| 60000|        IT| Mumbai, Maharashtra|\n",
      "|   102| Priya| 28| 75000|        HR|Bengaluru, Karnataka|\n",
      "|   103|Suresh| 35| 50000|   Finance| Chennai, Tamil Nadu|\n",
      "|   104|Anjali| 25| 80000|        IT|   Pune, Maharashtra|\n",
      "|   105| Arjun| 40| 90000|Management|Hyderabad, Telangana|\n",
      "+------+------+---+------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ StructField(\"Emp_ID\", IntegerType(), True), \n",
    "                     StructField(\"Name\", StringType(), True), \n",
    "                     StructField(\"Age\", IntegerType(), True), \n",
    "                     StructField(\"Salary\", IntegerType(), True), \n",
    "                     StructField(\"Department\", StringType(), True),\n",
    "                     StructField(\"Address\", StringType(), True) ]) \n",
    "\n",
    "data = [ (101, \"Rajesh\", 30, 60000, \"IT\", \"Mumbai, Maharashtra\"), \n",
    "         (102, \"Priya\", 28, 75000, \"HR\", \"Bengaluru, Karnataka\"), \n",
    "         (103, \"Suresh\", 35, 50000, \"Finance\", \"Chennai, Tamil Nadu\"), \n",
    "         (104, \"Anjali\", 25, 80000, \"IT\", \"Pune, Maharashtra\"), \n",
    "         (105, \"Arjun\", 40, 90000, \"Management\", \"Hyderabad, Telangana\") ]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08a921f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+-----------------+\n",
      "|Emp_ID|  Name|Age|Salary|Department|          Address|\n",
      "+------+------+---+------+----------+-----------------+\n",
      "|   104|Anjali| 25| 80000|        IT|Pune, Maharashtra|\n",
      "+------+------+---+------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the employees who belong to the \"IT\" department and have a salary greater than 70000.\n",
    "\n",
    "df.filter((col('Department') == 'IT') & (col('Salary') > 70000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e35b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+--------------------+---------+-----------+\n",
      "|Emp_ID|  Name|Age|Salary|Department|             Address|     City|      State|\n",
      "+------+------+---+------+----------+--------------------+---------+-----------+\n",
      "|   101|Rajesh| 30| 60000|        IT| Mumbai, Maharashtra|   Mumbai|Maharashtra|\n",
      "|   102| Priya| 28| 75000|        HR|Bengaluru, Karnataka|Bengaluru|  Karnataka|\n",
      "|   103|Suresh| 35| 50000|   Finance| Chennai, Tamil Nadu|  Chennai| Tamil Nadu|\n",
      "|   104|Anjali| 25| 80000|        IT|   Pune, Maharashtra|     Pune|Maharashtra|\n",
      "|   105| Arjun| 40| 90000|Management|Hyderabad, Telangana|Hyderabad|  Telangana|\n",
      "+------+------+---+------+----------+--------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the Address column into two separate columns: City and State.\n",
    "trn_df = df.withColumn('City', split(col('Address'), ', ').getItem(0)).withColumn('State', split(col('Address'), ', ').getItem(1))\n",
    "# OR \n",
    "#trn_df = df.withColumn('City', split(col('Address'), ', ')[0]).withColumn('State', split(col('Address'), ', ')[1])\n",
    "\n",
    "trn_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6963d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed data into a Parquet file.\n",
    "\n",
    "trn_df.write.mode('overwrite').parquet('trn_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31209af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f47605d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+-----+\n",
      "|Salesperson|Region|Month|Sales|\n",
      "+-----------+------+-----+-----+\n",
      "|       Amit| North|  Jan|12000|\n",
      "|     Rajesh| North|  Feb|15000|\n",
      "|     Sunita| North|  Mar|11000|\n",
      "|      Meena| South|  Jan|17000|\n",
      "|       Ravi| South|  Feb|20000|\n",
      "|      Priya| South|  Mar|18000|\n",
      "|     Suresh|  East|  Jan|10000|\n",
      "|     Vishal|  East|  Feb|22000|\n",
      "|      Akash|  East|  Mar|21000|\n",
      "|     Anjali|  West|  Jan|15000|\n",
      "|     Deepak|  West|  Feb|13000|\n",
      "|      Nidhi|  West|  Mar|17000|\n",
      "+-----------+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 48 calculate the month with the highest sales for each region using window functions and the max() function. Ensure that the result includes the region name, month, and sales value. Consider sales fluctuations, and the dataset should contain multiple records for each region to test windowing correctly.\n",
    "\n",
    "data = [ (\"Amit\", \"North\", \"Jan\", 12000), (\"Rajesh\", \"North\", \"Feb\", 15000), (\"Sunita\", \"North\", \"Mar\", 11000), (\"Meena\", \"South\", \"Jan\", 17000), \n",
    "(\"Ravi\", \"South\", \"Feb\", 20000), (\"Priya\", \"South\", \"Mar\", 18000), \n",
    "(\"Suresh\", \"East\", \"Jan\", 10000), (\"Vishal\", \"East\", \"Feb\", 22000), \n",
    "(\"Akash\", \"East\", \"Mar\", 21000), (\"Anjali\", \"West\", \"Jan\", 15000), \n",
    "(\"Deepak\", \"West\", \"Feb\", 13000), (\"Nidhi\", \"West\", \"Mar\", 17000), ] \n",
    "columns = [\"Salesperson\", \"Region\", \"Month\", \"Sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b5b4865a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+-----+----+\n",
      "|Salesperson|Region|Month|Sales|rank|\n",
      "+-----------+------+-----+-----+----+\n",
      "|     Vishal|  East|  Feb|22000|   1|\n",
      "|      Akash|  East|  Mar|21000|   2|\n",
      "|     Suresh|  East|  Jan|10000|   3|\n",
      "|     Rajesh| North|  Feb|15000|   1|\n",
      "|       Amit| North|  Jan|12000|   2|\n",
      "|     Sunita| North|  Mar|11000|   3|\n",
      "|       Ravi| South|  Feb|20000|   1|\n",
      "|      Priya| South|  Mar|18000|   2|\n",
      "|      Meena| South|  Jan|17000|   3|\n",
      "|      Nidhi|  West|  Mar|17000|   1|\n",
      "|     Anjali|  West|  Jan|15000|   2|\n",
      "|     Deepak|  West|  Feb|13000|   3|\n",
      "+-----------+------+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "win = Window.partitionBy('Region').orderBy(col('Sales').desc())\n",
    "df1 = df.withColumn('rank', row_number().over(win))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9ca3740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|Region|Month|Sales|\n",
      "+------+-----+-----+\n",
      "|  East|  Feb|22000|\n",
      "| North|  Feb|15000|\n",
      "| South|  Feb|20000|\n",
      "|  West|  Mar|17000|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(col('rank') == 1).select('Region', 'Month', 'Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06aa0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4b59b3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|salary|\n",
      "+----+------+\n",
      "|John| 50000|\n",
      "|Jane| 60000|\n",
      "| Doe| 55000|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How do you create a new column in a DataFrame?\n",
    "\n",
    "#Task: Given a DataFrame with columns name and salary, create a new column bonus which is 10% of the salary.\n",
    "\n",
    "data = [(\"John\", 50000),\n",
    "(\"Jane\", 60000),\n",
    "(\"Doe\", 55000)]\n",
    "columns = [\"name\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1a484554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+\n",
      "|name|salary| bonus|\n",
      "+----+------+------+\n",
      "|John| 50000|5000.0|\n",
      "|Jane| 60000|6000.0|\n",
      "| Doe| 55000|5500.0|\n",
      "+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('bonus', (col('salary')*10)/100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d02cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd1318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa007c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563fcf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8961d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3358c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f786e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83a2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b1fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b390f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ab9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb3a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "529598a1",
   "metadata": {},
   "source": [
    "##     GeekCoders\n",
    "\n",
    "https://www.youtube.com/watch?v=Rp5LWT4or-o&list=PLxy0DxWEupiODTF_xM5Lw1ghc0XtLCUhC&index=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832691f",
   "metadata": {},
   "source": [
    "### 13 Write a Pyspark query to report the movies with an odd-numbered ID and a description that is not 'boring'. Return the result table in descending order by rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "628768ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "|  2|   Science|    fiction|   8.5|\n",
      "|  3|     irish|     boring|   6.2|\n",
      "|  4|  Ice song|    Fantacy|   8.6|\n",
      "|  5|House card|Interesting|   9.1|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1, 'War','great 3D',8.9) \n",
    ",(2, 'Science','fiction',8.5) \n",
    ",(3, 'irish','boring',6.2)\n",
    ",(4, 'Ice song','Fantacy',8.6)  \n",
    ",(5, 'House card','Interesting',9.1)]   \n",
    "schema=\"ID int,movie string,description string,rating double\"\n",
    "df=spark.createDataFrame(data,schema) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "06f9cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((col('ID') % 2 != 0 ) & (trim(lower(col('description'))) != 'boring')).orderBy(col('rating').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "adbbe630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR \n",
    "df.select('*').filter((col('ID')%2!=0) & (trim(lower(col('description'))) != 'boring')).orderBy(col('rating').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f9e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78d8dbb4",
   "metadata": {},
   "source": [
    "### 14 Collect_list and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d838c8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+\n",
      "|name|  item|weight|\n",
      "+----+------+------+\n",
      "|john|tomato|     2|\n",
      "|bill| apple|     2|\n",
      "|john|banana|     2|\n",
      "|john|tomato|     3|\n",
      "|bill|  taco|     2|\n",
      "|bill| apple|     2|\n",
      "+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"john\", \"tomato\", 2),\n",
    "    (\"bill\", \"apple\", 2),\n",
    "    (\"john\", \"banana\", 2),\n",
    "    (\"john\", \"tomato\", 3),\n",
    "    (\"bill\", \"taco\", 2),\n",
    "    (\"bill\", \"apple\", 2),\n",
    "]\n",
    "schema = \"name string,item string,weight int\"\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e5b9376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----------+\n",
      "|name|  item|sum_weight|\n",
      "+----+------+----------+\n",
      "|john|tomato|         5|\n",
      "|bill| apple|         4|\n",
      "|john|banana|         2|\n",
      "|bill|  taco|         2|\n",
      "+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final =df.groupBy('name', 'item').agg(sum(col('weight')).alias('sum_weight'))\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3cebdd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------------+\n",
      "|name|items                     |\n",
      "+----+--------------------------+\n",
      "|bill|[{apple, 4}, {taco, 2}]   |\n",
      "|john|[{tomato, 5}, {banana, 2}]|\n",
      "+----+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fi = df_final.groupBy('name').agg(collect_list(struct('item', 'sum_weight')).alias('items')).orderBy('name')\n",
    "df_fi.show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6365767",
   "metadata": {},
   "source": [
    "### 15 find all duplicate emails\n",
    "Write a pyspark dataframe query to find all duplicate emails in a table named Person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dc823d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| ID|        email|\n",
      "+---+-------------+\n",
      "|  1|abc@gmail.com|\n",
      "|  2|bcd@gmail.com|\n",
      "|  3|abc@gmail.com|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"abc@gmail.com\"), (2, \"bcd@gmail.com\"), (3, \"abc@gmail.com\")]\n",
    "schema = \"ID int,email string\"\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5ec2176e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        email|\n",
      "+-------------+\n",
      "|abc@gmail.com|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('email').count().filter(col('count')>1).select('email').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1d5e4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        email|\n",
      "+-------------+\n",
      "|abc@gmail.com|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('test')\n",
    "spark.sql(\"\"\"select email from test group by email having count(email)>1\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957bac85",
   "metadata": {},
   "source": [
    "### 18 Write a pyspark query for a report that provides the customer ids from the customer table that bought all the products in the product table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e6b66a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|product_key|\n",
      "+-----------+-----------+\n",
      "|          1|          5|\n",
      "|          2|          6|\n",
      "|          3|          5|\n",
      "|          3|          6|\n",
      "|          1|          6|\n",
      "+-----------+-----------+\n",
      "\n",
      "+-----------+\n",
      "|product_key|\n",
      "+-----------+\n",
      "|          5|\n",
      "|          6|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,5),(2,6),(3,5),(3,6),(1,6)]\n",
    "schema=\"customer_id int,product_key int\"\n",
    "customer_df=spark.createDataFrame(data,schema)\n",
    "\n",
    "data=[(5,),(6,)]\n",
    "schema=\"product_key int\"\n",
    "product_df=spark.createDataFrame(data,schema)\n",
    "\n",
    "customer_df.show()\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "da04ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|cnt_products|\n",
      "+------------+\n",
      "|           2|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take unique key from product table \n",
    "df_product = product_df.agg(countDistinct(col('product_key')).alias('cnt_products'))\n",
    "df_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "91547338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|cnt_products|\n",
      "+-----------+------------+\n",
      "|          1|           2|\n",
      "|          3|           2|\n",
      "|          2|           1|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer = customer_df.groupBy(col('customer_id')).agg(countDistinct(col('product_key')).alias('cnt_products'))\n",
    "df_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9b126fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_product.join(df_customer, df_product.cnt_products == df_customer.cnt_products, 'inner').select('customer_id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd23501",
   "metadata": {},
   "source": [
    "### 19 . Get the employees dept id with maximun and minimum salary in each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "40072fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+\n",
      "| emp_name|dept_id|salary|\n",
      "+---------+-------+------+\n",
      "|   Genece|      2| 75000|\n",
      "|   Jaimin|      2| 80000|\n",
      "|   Pankaj|      2| 80000|\n",
      "| Tarvares|      2| 70000|\n",
      "| Marlania|      4| 70000|\n",
      "|   Briana|      4| 85000|\n",
      "|  Kimberi|      4| 55000|\n",
      "|Gabriella|      4| 55000|\n",
      "|   Lakken|      5| 60000|\n",
      "| Latoynia|      5| 65000|\n",
      "+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[('Genece' , 2 , 75000),\n",
    "('Jaimin' , 2 , 80000 ),\n",
    "('Pankaj' , 2 , 80000 ),\n",
    "('Tarvares' , 2 , 70000),\n",
    "('Marlania' , 4 , 70000),\n",
    "('Briana' , 4 , 85000),\n",
    "('Kimberi' , 4 , 55000),\n",
    "('Gabriella' , 4 , 55000),  \n",
    "('Lakken', 5, 60000),\n",
    "('Latoynia' , 5 , 65000) ]\n",
    "schema=\"emp_name string,dept_id int,salary int\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f0c38c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+\n",
      "|dept_id_1|max_salary|min_salary|\n",
      "+---------+----------+----------+\n",
      "|        2|     80000|     70000|\n",
      "|        4|     85000|     55000|\n",
      "|        5|     65000|     60000|\n",
      "+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = df.groupBy('dept_id').agg(max('salary').alias('max_salary'), min('salary').alias('min_salary')).withColumnRenamed('dept_id', 'dept_id_1')\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c98c5269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+---------+----------+----------+\n",
      "| emp_name|dept_id|salary|dept_id_1|max_salary|min_salary|\n",
      "+---------+-------+------+---------+----------+----------+\n",
      "|   Genece|      2| 75000|        2|     80000|     70000|\n",
      "|   Jaimin|      2| 80000|        2|     80000|     70000|\n",
      "|   Pankaj|      2| 80000|        2|     80000|     70000|\n",
      "| Marlania|      4| 70000|        4|     85000|     55000|\n",
      "| Tarvares|      2| 70000|        2|     80000|     70000|\n",
      "|   Briana|      4| 85000|        4|     85000|     55000|\n",
      "|  Kimberi|      4| 55000|        4|     85000|     55000|\n",
      "|Gabriella|      4| 55000|        4|     85000|     55000|\n",
      "|   Lakken|      5| 60000|        5|     65000|     60000|\n",
      "| Latoynia|      5| 65000|        5|     65000|     60000|\n",
      "+---------+-------+------+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = df.join(df_1, df.dept_id == df_1.dept_id_1, 'inner')\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "774ea93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+---------+----------+----------+\n",
      "| emp_name|dept_id|salary|dept_id_1|max_salary|min_salary|\n",
      "+---------+-------+------+---------+----------+----------+\n",
      "|   Jaimin|      2| 80000|        2|     80000|     70000|\n",
      "|   Pankaj|      2| 80000|        2|     80000|     70000|\n",
      "| Tarvares|      2| 70000|        2|     80000|     70000|\n",
      "|   Briana|      4| 85000|        4|     85000|     55000|\n",
      "|  Kimberi|      4| 55000|        4|     85000|     55000|\n",
      "|Gabriella|      4| 55000|        4|     85000|     55000|\n",
      "|   Lakken|      5| 60000|        5|     65000|     60000|\n",
      "| Latoynia|      5| 65000|        5|     65000|     60000|\n",
      "+---------+-------+------+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3 = df_2.filter((col('max_salary') == col('salary')) | (col('min_salary') == col('salary')))\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e1276b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+\n",
      "|dept_id|salary|           emp_names|\n",
      "+-------+------+--------------------+\n",
      "|      2| 80000|    [Jaimin, Pankaj]|\n",
      "|      2| 70000|          [Tarvares]|\n",
      "|      4| 85000|            [Briana]|\n",
      "|      4| 55000|[Kimberi, Gabriella]|\n",
      "|      5| 60000|            [Lakken]|\n",
      "|      5| 65000|          [Latoynia]|\n",
      "+-------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f = df_3.groupBy('dept_id', 'salary').agg(collect_list('emp_name').alias('emp_names'))\n",
    "df_f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875cad2",
   "metadata": {},
   "source": [
    "### 20 . Cache and Persist DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4d4e2913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|Firstname|Lastname| City|\n",
      "+---------+--------+-----+\n",
      "|      Raj|     Doe| NULL|\n",
      "|     NULL|  Samuel|VIZAG|\n",
      "|    David|   Smith| NULL|\n",
      "|   Samson|    NULL|  HYD|\n",
      "|     Immi|   Steve|  BNG|\n",
      "|     NULL|    NULL| NULL|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Raj\",\"Doe\",None),\n",
    " (None,\"Samuel\",\"VIZAG\"),\n",
    " (\"David\",\"Smith\", None),\n",
    " (\"Samson\",None, \"HYD\"),\n",
    " (\"Immi\", \"Steve\", \"BNG\"),\n",
    " (None, None, None)]\n",
    "\n",
    "columns = [\"Firstname\", \"Lastname\", \"City\"]\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e2e7449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firstname 33.33333333333333\n",
      "Lastname 33.33333333333333\n",
      "City 50.0\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    total_count = df.select(col(i)).count()\n",
    "    null_values = df.filter(col(i).isNull()).count()\n",
    "    percentage = (null_values/total_count)*100\n",
    "    print(i, percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7799371",
   "metadata": {},
   "source": [
    "### 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4cfbc0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "| name|            Hobbies|\n",
      "+-----+-------------------+\n",
      "|Alice|[Badminton, Tennis]|\n",
      "|  Bob|  [Tennis, Cricket]|\n",
      "|Julie| [Cricket, Carroms]|\n",
      "+-----+-------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- Hobbies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Alice' ,['Badminton', 'Tennis']),\n",
    "        ('Bob', ['Tennis', 'Cricket']),\n",
    "        ('Julie', ['Cricket','Carroms'])]\n",
    "schema = ['name', 'Hobbies']\n",
    "df = spark.createDataFrame(data=data, schema= schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "01246b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| name|      col|\n",
      "+-----+---------+\n",
      "|Alice|Badminton|\n",
      "|Alice|   Tennis|\n",
      "|  Bob|   Tennis|\n",
      "|  Bob|  Cricket|\n",
      "|Julie|  Cricket|\n",
      "|Julie|  Carroms|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('name'), explode(col('Hobbies'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7314e593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|City1|City2|City3|\n",
      "+-----+-----+-----+\n",
      "|  Goa|     |   AP|\n",
      "|     |   AP| NULL|\n",
      "| NULL|     |  Blr|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Goa','','AP'), ('','AP',None),(None, '','Blr')]\n",
    "schema = 'City1 string, City2 string, City3 string'\n",
    "df2 = spark.createDataFrame(data, schema)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b2885096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|City1|City2|City3|\n",
      "+-----+-----+-----+\n",
      "|  Goa| NULL|   AP|\n",
      "| NULL|   AP| NULL|\n",
      "| NULL| NULL|  Blr|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = df2.withColumn('City1', when(col('City1')=='',None).otherwise(col('City1')))\\\n",
    "        .withColumn('City2', when(col('City2')=='',None).otherwise(col('City2')))\\\n",
    "        .withColumn('City3', when(col('City3')=='',None).otherwise(col('City3')))\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fb6fa220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Result|\n",
      "+------+\n",
      "|   Goa|\n",
      "|    AP|\n",
      "|   Blr|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff= df_2.withColumn('Result', coalesce(col('City1'), col('City2'), col('City3'))).select('Result')\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "51407bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------+\n",
      "|name      |address                                      |\n",
      "+----------+---------------------------------------------+\n",
      "|John Doe  |{\"street\": \"123 Main St\", \"city\": \"Anytown\"} |\n",
      "|Jane Smith|{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}|\n",
      "+----------+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 26\n",
    "\n",
    "data=[('John Doe','{\"street\": \"123 Main St\", \"city\": \"Anytown\"}'),('Jane Smith','{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}')]\n",
    "df=spark.createDataFrame(data,schema=\"name string,address string\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "26334739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------+-----------------------+\n",
      "|name      |address                                      |parsed_json            |\n",
      "+----------+---------------------------------------------+-----------------------+\n",
      "|John Doe  |{\"street\": \"123 Main St\", \"city\": \"Anytown\"} |{123 Main St, Anytown} |\n",
      "|Jane Smith|{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}|{456 Elm St, Othertown}|\n",
      "+----------+---------------------------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('parsed_json', from_json(col('address'), 'street string,city string' ))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7b3b2c17",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.withColumn() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mwithColumn(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m), col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_json\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstreet\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreet\u001b[39m\u001b[38;5;124m'\u001b[39m),  col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_json\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcity\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      2\u001b[0m df2\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mTypeError\u001b[0m: DataFrame.withColumn() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(col('name'), col('parsed_json').street.alias('street'),  col('parsed_json').city.alias('city'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a1c4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cff363f",
   "metadata": {},
   "source": [
    "### 28 Find missing Numbers in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f27bda58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| Id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, ), (2,), (3,), (6,), (7,), (8,)]\n",
    "schema=\"Id int\"\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "33932d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|min(Id)|max(Id)|\n",
      "+-------+-------+\n",
      "|      1|      8|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list = df.select(min(col('Id')), max(col('Id')))\n",
    "df_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4832d608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(min(Id)=1, max(Id)=8)\n",
      "1\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(df_list.first())\n",
    "print(df_list.first()[0])\n",
    "print(df_list.first()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9129ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = spark.range(df_list.first()[0], df_list.first()[1]+1)\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3cfff4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.subtract(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe274ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "82fee51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| Name| Sub|Marks|\n",
      "+-----+----+-----+\n",
      "|Rudra|math|   79|\n",
      "|Rudra| eng|   60|\n",
      "|Shivu|math|   68|\n",
      "|Shivu| eng|   59|\n",
      "|  Anu|math|   65|\n",
      "|  Anu| eng|   80|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[\n",
    "('Rudra','math',79),\n",
    "('Rudra','eng',60),\n",
    "('Shivu','math', 68),\n",
    "('Shivu','eng', 59),\n",
    "('Anu','math', 65),\n",
    "('Anu','eng',80)\n",
    "]\n",
    "schema=\"Name string,Sub string,Marks int\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a18cfaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "| Name|eng|math|\n",
      "+-----+---+----+\n",
      "|Shivu| 59|  68|\n",
      "|Rudra| 60|  79|\n",
      "|  Anu| 80|  65|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('Name').pivot('Sub').sum('Marks')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "141eea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| Name|Marks_New|\n",
      "+-----+---------+\n",
      "|Rudra| [79, 60]|\n",
      "|Shivu| [68, 59]|\n",
      "|  Anu| [65, 80]|\n",
      "+-----+---------+\n",
      "\n",
      "+-----+----+---+\n",
      "| Name|math|eng|\n",
      "+-----+----+---+\n",
      "|Rudra|  79| 60|\n",
      "|Shivu|  68| 59|\n",
      "|  Anu|  65| 80|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR \n",
    "df2 = df.groupBy(col('Name')).agg(collect_list(col(\"Marks\")).alias(\"Marks_New\"))\n",
    "df2.show()\n",
    "df3 = df2.select(col('Name'), col('Marks_New')[0].alias('math'),col('Marks_New')[1].alias('eng'))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d38f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e219eb6e",
   "metadata": {},
   "source": [
    "### 43 Write a Pyspark code to get all the customer ID along with their final balance amount after calculating their transactions based on transaction type.\n",
    "If any customer has not any transactions then his final balance should remain same as current amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "96308d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------------------+\n",
      "|customer_id|transaction_type|transaction_amount|\n",
      "+-----------+----------------+------------------+\n",
      "|          1|          credit|              30.0|\n",
      "|          1|           debit|              90.0|\n",
      "|          2|          credit|              50.0|\n",
      "|          3|           debit|              57.0|\n",
      "|          2|           debit|              90.0|\n",
      "+-----------+----------------+------------------+\n",
      "\n",
      "+-----------+--------------+\n",
      "|customer_id|current_amount|\n",
      "+-----------+--------------+\n",
      "|          1|        1000.0|\n",
      "|          2|        2000.0|\n",
      "|          3|        3000.0|\n",
      "|          4|        4000.0|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_data = [\n",
    "    (1, \"credit\", 30.0),\n",
    "    (1, \"debit\", 90.0),\n",
    "    (2, \"credit\", 50.0),\n",
    "    (3, \"debit\", 57.0),\n",
    "    (2, \"debit\", 90.0)]\n",
    "transaction_schema = StructType([\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField(\"transaction_type\", StringType(), True),\n",
    "    StructField(\"transaction_amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "trans_df = spark.createDataFrame(transactions_data, schema=transaction_schema)\n",
    "trans_df.show()\n",
    "\n",
    "amounts_data = [\n",
    "    (1, 1000.0),\n",
    "    (2, 2000.0),\n",
    "    (3, 3000.0),\n",
    "    (4, 4000.0)]\n",
    "amount_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"current_amount\", FloatType(), True)])\n",
    "\n",
    "amt_df = spark.createDataFrame(amounts_data, schema=amount_schema)\n",
    "amt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "36f5ecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+--------------------+\n",
      "|customer_id|total_credit|total_debit|total_remaining_amnt|\n",
      "+-----------+------------+-----------+--------------------+\n",
      "|          1|        30.0|       90.0|                60.0|\n",
      "|          2|        50.0|       90.0|                40.0|\n",
      "|          3|         0.0|       57.0|                57.0|\n",
      "+-----------+------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = trans_df.groupBy('customer_id').agg(sum(when(col('transaction_type') == 'credit', col('transaction_amount'))\\\n",
    "                                               .otherwise(0)).alias('total_credit'),\\\n",
    "                                           sum(when(col('transaction_type') == 'debit', col('transaction_amount'))\\\n",
    "                                               .otherwise(0)).alias('total_debit'))\\\n",
    "                                      .withColumn('total_remaining_amnt', col('total_debit')-col('total_credit'))\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f8e75a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|current_amount|\n",
      "+-----------+--------------+\n",
      "|          1|         940.0|\n",
      "|          2|        1960.0|\n",
      "|          3|        2943.0|\n",
      "|          4|        4000.0|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = amt_df.join(df_1, amt_df.customer_id == df_1.customer_id, how='left')\\\n",
    "                          .withColumn('total_remaining_amnt', coalesce(col('total_remaining_amnt'), lit(0)))\\\n",
    "                          .withColumn('current_amount', col('current_amount') - col('total_remaining_amnt'))\\\n",
    "                          .drop(df_1.customer_id)\\\n",
    "                          .select('customer_id', 'current_amount')\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df861022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
