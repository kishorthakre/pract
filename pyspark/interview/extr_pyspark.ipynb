{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faf32c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ba4c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84a89eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20fb389e3d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('pr').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30bf2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------------------+\n",
      "|uid| pid|qunt|              pur_dt|\n",
      "+---+----+----+--------------------+\n",
      "|333|1122|   9|2022-02-06T01:00:...|\n",
      "|333|1122|  10|2022-02-06T02:00:...|\n",
      "|536|1435|  10|2022-03-02T08:40:...|\n",
      "|536|3223|   5|2022-03-02T09:33:...|\n",
      "|536|3223|   6|2022-01-11T12:33:...|\n",
      "|827|2452|  45|2022-03-02T00:00:...|\n",
      "|827|3585|  35|2022-02-20T14:05:...|\n",
      "+---+----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the customers who brought the same product more than once but on different days\n",
    "# ( if same product is purchased multiple times but on same date shouldn't be counted)\n",
    "\n",
    "data = [(333, 1122, 9, '2022-02-06T01:00:00.000+00:00'),\n",
    "        (333,1122,10,'2022-02-06T02:00:00.000+00:00'), \n",
    "        (536,1435, 10,'2022-03-02T08:40:00.000+00:00'),\n",
    "        (536,3223,5,'2022-03-02T09:33:28.000+00:00'),\n",
    "        (536, 3223, 6,'2022-01-11T12:33:44.000+00:00'),\n",
    "        (827, 2452, 45,'2022-03-02T00:00:00.000+00:00'), \n",
    "        (827, 3585, 35,'2022-02-20T14:05:26.000+00:00')]\n",
    "df = spark.createDataFrame(data = data, schema=['uid', 'pid', 'qunt', 'pur_dt'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0882e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+-----+\n",
      "|uid| pid|        dt|count|\n",
      "+---+----+----------+-----+\n",
      "|333|1122|2022-02-06|    2|\n",
      "|536|1435|2022-03-02|    1|\n",
      "|536|3223|2022-03-02|    1|\n",
      "|536|3223|2022-01-11|    1|\n",
      "|827|2452|2022-03-02|    1|\n",
      "|827|3585|2022-02-20|    1|\n",
      "+---+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('dt', to_date('pur_dt')).groupBy('uid', 'pid', 'dt').count()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7f5e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|uid| pid|count|\n",
      "+---+----+-----+\n",
      "|536|3223|    2|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('uid', 'pid').count().filter(col('count') >= 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8fdba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| Id|           name|\n",
      "+---+---------------+\n",
      "|  1|sagar-prajapati|\n",
      "|  2|      alex-john|\n",
      "|  3|      john cena|\n",
      "|  4|        kim joe|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In the given dataset, names contain for some names and space for some names, extract the first name and last name \n",
    "\n",
    "data = [(1, 'sagar-prajapati'), (2, 'alex-john'), (3, 'john cena'), (4, 'kim joe')]\n",
    "schema = ['Id','name']\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d756944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "| Id|           name|              A|\n",
      "+---+---------------+---------------+\n",
      "|  1|sagar-prajapati|sagar prajapati|\n",
      "|  2|      alex-john|      alex john|\n",
      "|  3|      john cena|      john cena|\n",
      "|  4|        kim joe|        kim joe|\n",
      "+---+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_regexp = \"((,)?\\s|[-])\"\n",
    "df1 = df.withColumn(\"A\", regexp_replace(col(\"name\"), replace_regexp, \" \"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41ceb8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------+---------+\n",
      "| Id|           name|First_name|Last_Name|\n",
      "+---+---------------+----------+---------+\n",
      "|  1|sagar-prajapati|     sagar|prajapati|\n",
      "|  2|      alex-john|      alex|     john|\n",
      "|  3|      john cena|      john|     cena|\n",
      "|  4|        kim joe|       kim|      joe|\n",
      "+---+---------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn('First_name', split(df1['A'], ' ').getItem(0)).withColumn('Last_Name', split(df1['A'], ' ').getItem(1))\n",
    "df2.drop('A').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d55ffe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+\n",
      "| Id|           name|                 A|\n",
      "+---+---------------+------------------+\n",
      "|  1|sagar-prajapati|[sagar, prajapati]|\n",
      "|  2|      alex-john|      [alex, john]|\n",
      "|  3|      john cena|      [john, cena]|\n",
      "|  4|        kim joe|        [kim, joe]|\n",
      "+---+---------------+------------------+\n",
      "\n",
      "+---+---------------+------------------+-----+---------+\n",
      "| Id|           name|                 A|fname|    lname|\n",
      "+---+---------------+------------------+-----+---------+\n",
      "|  1|sagar-prajapati|[sagar, prajapati]|sagar|prajapati|\n",
      "|  2|      alex-john|      [alex, john]| alex|     john|\n",
      "|  3|      john cena|      [john, cena]| john|     cena|\n",
      "|  4|        kim joe|        [kim, joe]|  kim|      joe|\n",
      "+---+---------------+------------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_regex = \"((,)?\\s|[-])\"\n",
    "df1 = df.withColumn(\"A\", split(col(\"name\"), split_regex))\n",
    "df1.show()\n",
    "\n",
    "df1.select('*', df1.A.getItem(0).alias('fname'), df1.A.getItem(1).alias('lname')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "792957f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|                 A|array_contains|\n",
      "+------------------+--------------+\n",
      "|[sagar, prajapati]|         false|\n",
      "|      [alex, john]|          true|\n",
      "|      [john, cena]|         false|\n",
      "|        [kim, joe]|         false|\n",
      "+------------------+--------------+\n",
      "\n",
      "+---+---------+------------+\n",
      "| Id|     name|           A|\n",
      "+---+---------+------------+\n",
      "|  2|alex-john|[alex, john]|\n",
      "|  3|john cena|[john, cena]|\n",
      "+---+---------+------------+\n",
      "\n",
      "+---------------+------------------+\n",
      "|           Name|    Sorted_Numbers|\n",
      "+---------------+------------------+\n",
      "|sagar-prajapati|[prajapati, sagar]|\n",
      "|      alex-john|      [alex, john]|\n",
      "|      john cena|      [cena, john]|\n",
      "|        kim joe|        [joe, kim]|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select('A',array_contains('A',\"alex\").alias(\"array_contains\")).show()\n",
    "df1.filter(array_contains(df1.A, 'john')).show()\n",
    "df1.select(\"Name\", array_sort(df1.A).alias(\"Sorted_Numbers\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "112eb487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------+----+\n",
      "| Id|           name|                 A|Size|\n",
      "+---+---------------+------------------+----+\n",
      "|  1|sagar-prajapati|[sagar, prajapati]|   2|\n",
      "|  2|      alex-john|      [alex, john]|   2|\n",
      "|  3|      john cena|      [john, cena]|   2|\n",
      "|  4|        kim joe|        [kim, joe]|   2|\n",
      "+---+---------------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"Size\", size(df1.A)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bf22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "010bc83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|person1|person2|call_duration|\n",
      "+-------+-------+-------------+\n",
      "|     10|     20|           58|\n",
      "|     20|     10|           12|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "|     40|     30|          500|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call_duration \n",
    "\n",
    "data = [(10, 20, 58), (20,10,12), (10,30, 20),(30,40,100),(30, 40, 200), (30, 40, 200), (40, 30, 500)]\n",
    "df = spark.createDataFrame(data = data, schema=['person1', 'person2', 'call_duration'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c678489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|person1|person2|call_duration|\n",
      "+-------+-------+-------------+\n",
      "|     10|     20|           58|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "|     10|     20|           58|\n",
      "|     10|     30|           20|\n",
      "|     30|     40|          100|\n",
      "|     30|     40|          200|\n",
      "|     30|     40|          200|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.alias('t1').unionAll(df.alias('t2')).filter(col('person1') < col('person2'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f662b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------+\n",
      "|person1|person2|call_count|total_duration|\n",
      "+-------+-------+----------+--------------+\n",
      "|     10|     20|         2|           116|\n",
      "|     10|     30|         2|            40|\n",
      "|     30|     40|         6|          1000|\n",
      "+-------+-------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy('person1','person2').agg(count(col('call_duration')).alias('call_count'), sum(col('call_duration')).alias('total_duration'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96b2c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| sub|\n",
      "+---+----+\n",
      "|  1|MATH|\n",
      "|  2|MATH|\n",
      "|  4|CHEM|\n",
      "|  5|MATH|\n",
      "|  2| ENG|\n",
      "|  3| PHY|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the teachers who teaches only math and not any other subject \n",
    "\n",
    "data = [(1, \"MATH\"), (2,'MATH'), (4, 'CHEM'),(5, 'MATH'),(2, 'ENG'), (3, 'PHY')]\n",
    "df = spark.createDataFrame(data = data, schema=['id', 'sub'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3051b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  4|    1|\n",
      "|  5|    1|\n",
      "|  3|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('id').count().filter(col('count') == 1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b512244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| sub|\n",
      "+---+----+\n",
      "|  1|MATH|\n",
      "|  5|MATH|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df1, df.id == df1.id, how = 'inner').filter(df['sub'] == 'MATH').select(df['*']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9dbfc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+\n",
      "|COMPANY|YEAR|REVENUE|\n",
      "+-------+----+-------+\n",
      "|    ABC|2000|    100|\n",
      "|    ABC|2001|    110|\n",
      "|    ABC|2002|    120|\n",
      "|    XYZ|2000|    100|\n",
      "|    XYZ|2001|     90|\n",
      "|    XYZ|2002|    120|\n",
      "|    RXC|2000|    500|\n",
      "|    RXC|2001|    400|\n",
      "|    RXC|2002|    600|\n",
      "|    RXC|2003|    800|\n",
      "+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out the companies where revenue has only increased over the years and there was no decrease at all for any point.\n",
    "\n",
    "data = [('ABC', 2000, 100),\n",
    "('ABC', 2001, 110),\n",
    "('ABC', 2002, 120),\n",
    "('XYZ', 2000, 100),\n",
    "('XYZ', 2001, 90),\n",
    "('XYZ', 2002, 120),\n",
    "('RXC', 2000, 500),\n",
    "('RXC', 2001, 400),\n",
    "('RXC', 2002, 600),\n",
    "('RXC', 2003, 800)]\n",
    "schema = StructType([StructField('COMPANY', StringType(), True),\n",
    "                     StructField('YEAR', IntegerType(), True),\n",
    "                     StructField('REVENUE', IntegerType(), True)]) \n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "167fafd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+----+\n",
      "|COMPANY|YEAR|REVENUE| lag|\n",
      "+-------+----+-------+----+\n",
      "|    ABC|2000|    100| 100|\n",
      "|    ABC|2001|    110|  10|\n",
      "|    ABC|2002|    120|  10|\n",
      "|    RXC|2000|    500| 500|\n",
      "|    RXC|2001|    400|-100|\n",
      "|    RXC|2002|    600| 200|\n",
      "|    RXC|2003|    800| 200|\n",
      "|    XYZ|2000|    100| 100|\n",
      "|    XYZ|2001|     90| -10|\n",
      "|    XYZ|2002|    120|  30|\n",
      "+-------+----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy('COMPANY').orderBy('YEAR')\n",
    "df1 = df.withColumn('lag', col('REVENUE')-lag(col('REVENUE'), 1, 0).over(window))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "396390e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|COMPANY|diff|\n",
      "+-------+----+\n",
      "|    ABC|  10|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy('COMPANY').agg(min(col('lag')).alias('diff')).filter(col('diff') > 0)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d65d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     Movie|       Type|Rating|\n",
      "+---+----------+-----------+------+\n",
      "|  1|       war|   great ed|   8.9|\n",
      "|  2|   science|    fiction|   8.5|\n",
      "|  3|     irish|     boring|   6.2|\n",
      "|  4|  Ice song|    fantacy|   8.6|\n",
      "|  5|house card|interesting|   9.1|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lIst down the movies with an odd ID and which is not boring and order by id desc \n",
    "\n",
    "data = [(1, 'war', 'great ed',8.9),\n",
    "    (2,'science','fiction',8.5),\n",
    "    (3,'irish', 'boring', 6.2),\n",
    "    (4, 'Ice song', 'fantacy', 8.6),\n",
    "    (5, \"house card\", 'interesting', 9.1)]\n",
    "sch = ['ID', 'Movie', 'Type', 'Rating']\n",
    "df = spark.createDataFrame(data=data, schema=sch)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df7f6e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     Movie|       Type|Rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|house card|interesting|   9.1|\n",
      "|  1|       war|   great ed|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.filter(((df['ID'] %2 ) != 0 ) & (col('Type') != 'boring')).orderBy(col('ID').desc())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "302ce2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----+\n",
      "| id| name|salary| mid|\n",
      "+---+-----+------+----+\n",
      "|  1| John|  6000|   4|\n",
      "|  2|Kevin| 11000|   4|\n",
      "|  3|  Bob|  8000|   5|\n",
      "|  4|Laura|  9000|NULL|\n",
      "|  5|Sarah| 10000|NULL|\n",
      "+---+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the employees earning more than managers \n",
    "\n",
    "data = [(1, \"John\", 6000, 4), (2,'Kevin',11000,4), (3, 'Bob',8000, 5),(4, 'Laura',9000,None),(5, 'Sarah',10000, None)]\n",
    "df = spark.createDataFrame(data = data, schema=['id', 'name', 'salary','mid'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6169934e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+---+\n",
      "| id| name|salary|mid|\n",
      "+---+-----+------+---+\n",
      "|  2|Kevin| 11000|  4|\n",
      "+---+-----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('emp').join(df.alias('mgr'), col('emp.mid')  == col('mgr.id'), 'inner').filter(col('emp.salary') > col('mgr.salary'))\\\n",
    ".select(col('emp.id'), col('emp.name'), col('emp.salary'), col('emp.mid')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3cc74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b9d0d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   team|points|\n",
      "+-------+------+\n",
      "|  Mavs^|    18|\n",
      "|  Ne%ts|    33|\n",
      "|Hawk**s|    12|\n",
      "|  Mavs@|    15|\n",
      "| Hawks!|    19|\n",
      "| (Cavs)|    24|\n",
      "|  Magic|    28|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove special charactors\n",
    "\n",
    "data = [['Mavs^', 18], \n",
    "        ['Ne%ts', 33], \n",
    "        ['Hawk**s', 12], \n",
    "        ['Mavs@', 15], \n",
    "        ['Hawks!', 19],\n",
    "        ['(Cavs)', 24],\n",
    "        ['Magic', 28]] \n",
    "columns = ['team', 'points'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "399c47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|   team|points|team2|\n",
      "+-------+------+-----+\n",
      "|  Mavs^|    18| Mavs|\n",
      "|  Ne%ts|    33| Nets|\n",
      "|Hawk**s|    12|Hawks|\n",
      "|  Mavs@|    15| Mavs|\n",
      "| Hawks!|    19|Hawks|\n",
      "| (Cavs)|    24| Cavs|\n",
      "|  Magic|    28|Magic|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('team2', regexp_replace('team', '[^a-zA-Z0-9]', ''))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e03fcce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+-----------------+\n",
      "|   team|points|team2|count_of_spe_char|\n",
      "+-------+------+-----+-----------------+\n",
      "|  Mavs^|    18| Mavs|                1|\n",
      "|  Ne%ts|    33| Nets|                1|\n",
      "|Hawk**s|    12|Hawks|                2|\n",
      "|  Mavs@|    15| Mavs|                1|\n",
      "| Hawks!|    19|Hawks|                1|\n",
      "| (Cavs)|    24| Cavs|                2|\n",
      "|  Magic|    28|Magic|                0|\n",
      "+-------+------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn('count_of_spe_char',length(col('team')) - length(col('team2'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698d1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc9b8e6",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/company/seekho-bigdata-institute/posts/?feedView=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1eee32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffee7795",
   "metadata": {},
   "source": [
    "9\n",
    "The dataset has duplicate entries, and some entries are missing values. You are required to:\n",
    "    \n",
    "-- Deduplicate the dataset.\n",
    "- Handle any missing values appropriately.\n",
    "- Determine the top 3 most frequent activity_type for each user_id.\n",
    "- Calculate the time spent by each user on each activity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "aa42a870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+\n",
      "|user_id|          timestamp|activity_type|\n",
      "+-------+-------------------+-------------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|   NULL|2024-12-30 12:00:00|        LOGIN|\n",
      "|     U3|               NULL|       LOGOUT|\n",
      "+-------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"U1\", \"2024-12-30 10:00:00\", \"LOGIN\"), \n",
    "        (\"U1\", \"2024-12-30 10:05:00\", \"BROWSE\"),\n",
    "        (\"U1\", \"2024-12-30 10:20:00\", \"LOGOUT\"),\n",
    "        (\"U2\", \"2024-12-30 11:00:00\", \"LOGIN\"),\n",
    "        (\"U2\", \"2024-12-30 11:15:00\", \"BROWSE\"),\n",
    "        (\"U2\", \"2024-12-30 11:30:00\", \"LOGOUT\"),\n",
    "        (\"U1\", \"2024-12-30 10:20:00\", \"LOGOUT\"),  \n",
    "        (None, \"2024-12-30 12:00:00\", \"LOGIN\"),   \n",
    "        (\"U3\", None, \"LOGOUT\")           ]\n",
    "\n",
    "schema = StructType([ StructField(\"user_id\", StringType(), True),\n",
    "                     StructField(\"timestamp\", StringType(), True),\n",
    "                     StructField(\"activity_type\", StringType(), True) ])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d882e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+\n",
      "|user_id|          timestamp|activity_type|\n",
      "+-------+-------------------+-------------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|   NULL|2024-12-30 12:00:00|        LOGIN|\n",
      "|     U3|               NULL|       LOGOUT|\n",
      "+-------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert timestamp to TimesstampType\n",
    "df = df.withColumn('timestamp', col('timestamp').cast(TimestampType()))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8bba8c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+\n",
      "|user_id|          timestamp|activity_type|\n",
      "+-------+-------------------+-------------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|\n",
      "|     U3|               NULL|       LOGOUT|\n",
      "|   NULL|2024-12-30 12:00:00|        LOGIN|\n",
      "+-------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate the dataset \n",
    "df_deduplicated = df.dropDuplicates()\n",
    "df_deduplicated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c504b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+\n",
      "|user_id|          timestamp|activity_type|\n",
      "+-------+-------------------+-------------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|\n",
      "+-------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2 Handle missing values \n",
    "df_cleaned = df_deduplicated.dropna(subset=['user_id', 'timestamp', 'activity_type'])\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "94627899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+\n",
      "|user_id|activity_type|count|\n",
      "+-------+-------------+-----+\n",
      "|     U1|       BROWSE|    1|\n",
      "|     U2|       BROWSE|    1|\n",
      "|     U1|       LOGOUT|    1|\n",
      "|     U2|        LOGIN|    1|\n",
      "|     U2|       LOGOUT|    1|\n",
      "|     U1|        LOGIN|    1|\n",
      "+-------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Determine the top 3 most frequent activity_type for each user_id.\n",
    "\n",
    "activity_count = df_cleaned.groupBy('user_id', 'activity_type').count()\n",
    "activity_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f4a702fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+----+\n",
      "|user_id|activity_type|count|rank|\n",
      "+-------+-------------+-----+----+\n",
      "|     U1|       BROWSE|    1|   1|\n",
      "|     U1|       LOGOUT|    1|   2|\n",
      "|     U1|        LOGIN|    1|   3|\n",
      "|     U2|       BROWSE|    1|   1|\n",
      "|     U2|        LOGIN|    1|   2|\n",
      "|     U2|       LOGOUT|    1|   3|\n",
      "+-------+-------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_count= activity_count.withColumn('rank', row_number().over(Window.partitionBy('user_id').orderBy(desc('count'))))\n",
    "activity_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "299ecf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+\n",
      "|user_id|activity_type|count|\n",
      "+-------+-------------+-----+\n",
      "|     U1|       BROWSE|    1|\n",
      "|     U1|       LOGOUT|    1|\n",
      "|     U1|        LOGIN|    1|\n",
      "|     U2|       BROWSE|    1|\n",
      "|     U2|        LOGIN|    1|\n",
      "|     U2|       LOGOUT|    1|\n",
      "+-------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_activities = activity_count.filter(col('rank') <= 3).select('user_id', 'activity_type', 'count')\n",
    "top_activities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1bb18dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+-------------------+\n",
      "|user_id|          timestamp|activity_type|     next_timestamp|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|               NULL|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|2024-12-30 10:00:00|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|2024-12-30 10:05:00|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|               NULL|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|2024-12-30 11:00:00|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|2024-12-30 11:15:00|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the time spent by each user on each activity_type\n",
    "\n",
    "win_spec = Window.partitionBy('user_id').orderBy('timestamp')\n",
    "df_with_lag = df_cleaned.withColumn('next_timestamp', lag('timestamp').over(win_spec))\n",
    "df_with_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f6c88192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+-------------------+----------+\n",
      "|user_id|          timestamp|activity_type|     next_timestamp|time_spent|\n",
      "+-------+-------------------+-------------+-------------------+----------+\n",
      "|     U1|2024-12-30 10:00:00|        LOGIN|               NULL|      NULL|\n",
      "|     U1|2024-12-30 10:05:00|       BROWSE|2024-12-30 10:00:00|      -5.0|\n",
      "|     U1|2024-12-30 10:20:00|       LOGOUT|2024-12-30 10:05:00|     -15.0|\n",
      "|     U2|2024-12-30 11:00:00|        LOGIN|               NULL|      NULL|\n",
      "|     U2|2024-12-30 11:15:00|       BROWSE|2024-12-30 11:00:00|     -15.0|\n",
      "|     U2|2024-12-30 11:30:00|       LOGOUT|2024-12-30 11:15:00|     -15.0|\n",
      "+-------+-------------------+-------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_time_spent = df_with_lag.withColumn('time_spent', (col('next_timestamp').cast('long') - col('timestamp').cast('long')).cast('double') / 60)\n",
    "df_with_time_spent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9be40899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+---------------+\n",
      "|user_id|activity_type|sum(time_spent)|\n",
      "+-------+-------------+---------------+\n",
      "|     U1|        LOGIN|           NULL|\n",
      "|     U1|       BROWSE|           -5.0|\n",
      "|     U1|       LOGOUT|          -15.0|\n",
      "|     U2|        LOGIN|           NULL|\n",
      "|     U2|       BROWSE|          -15.0|\n",
      "|     U2|       LOGOUT|          -15.0|\n",
      "+-------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_spent_per_activity = df_with_time_spent.groupBy('user_id', 'activity_type').sum('time_spent')\n",
    "time_spent_per_activity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d0176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3cbff72",
   "metadata": {},
   "source": [
    "16\n",
    "- Calculate the average grade for each student across all semesters.\n",
    "- Filter out students with an average grade lower than 75.\n",
    "- Find the top 3 students with the highest grades in each subject for the latest semester.\n",
    "- Sort the final results by subject and then by grade in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5e8b8b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+-----+\n",
      "|Student|Subject| Semester|Grade|\n",
      "+-------+-------+---------+-----+\n",
      "|  Alice|   Math|Semester1|   85|\n",
      "|  Alice|   Math|Semester2|   90|\n",
      "|  Alice|English|Semester1|   78|\n",
      "|  Alice|English|Semester2|   82|\n",
      "|    Bob|   Math|Semester1|   65|\n",
      "|    Bob|   Math|Semester2|   70|\n",
      "|    Bob|English|Semester1|   60|\n",
      "|    Bob|English|Semester2|   65|\n",
      "|Charlie|   Math|Semester1|   95|\n",
      "|Charlie|   Math|Semester2|   98|\n",
      "|Charlie|English|Semester1|   88|\n",
      "|Charlie|English|Semester2|   90|\n",
      "|  David|   Math|Semester1|   78|\n",
      "|  David|   Math|Semester2|   80|\n",
      "|  David|English|Semester1|   75|\n",
      "|  David|English|Semester2|   72|\n",
      "|    Eve|   Math|Semester1|   88|\n",
      "|    Eve|   Math|Semester2|   85|\n",
      "|    Eve|English|Semester1|   80|\n",
      "|    Eve|English|Semester2|   83|\n",
      "+-------+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (\"Alice\", \"Math\", \"Semester1\", 85),\n",
    " (\"Alice\", \"Math\", \"Semester2\", 90),\n",
    " (\"Alice\", \"English\", \"Semester1\", 78),\n",
    " (\"Alice\", \"English\", \"Semester2\", 82),\n",
    " (\"Bob\", \"Math\", \"Semester1\", 65),\n",
    " (\"Bob\", \"Math\", \"Semester2\", 70),\n",
    " (\"Bob\", \"English\", \"Semester1\", 60),\n",
    " (\"Bob\", \"English\", \"Semester2\", 65),\n",
    " (\"Charlie\", \"Math\", \"Semester1\", 95),\n",
    " (\"Charlie\", \"Math\", \"Semester2\", 98),\n",
    " (\"Charlie\", \"English\", \"Semester1\", 88),\n",
    " (\"Charlie\", \"English\", \"Semester2\", 90),\n",
    " (\"David\", \"Math\", \"Semester1\", 78),\n",
    " (\"David\", \"Math\", \"Semester2\", 80),\n",
    " (\"David\", \"English\", \"Semester1\", 75),\n",
    " (\"David\", \"English\", \"Semester2\", 72),\n",
    " (\"Eve\", \"Math\", \"Semester1\", 88),\n",
    " (\"Eve\", \"Math\", \"Semester2\", 85),\n",
    " (\"Eve\", \"English\", \"Semester1\", 80),\n",
    " (\"Eve\", \"English\", \"Semester2\", 83)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Student\", \"Subject\", \"Semester\", \"Grade\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1a44a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "| Semester|avg(Grade)|\n",
      "+---------+----------+\n",
      "|Semester1|      79.2|\n",
      "|Semester2|      81.5|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average grade for each student across all semesters.\n",
    "std_avg_grd = df.groupBy('Semester').agg(avg(col('Grade')))\n",
    "std_avg_grd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2134dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+-----+\n",
      "|Student|Subject| Semester|Grade|\n",
      "+-------+-------+---------+-----+\n",
      "|  Alice|   Math|Semester1|   85|\n",
      "|  Alice|   Math|Semester2|   90|\n",
      "|  Alice|English|Semester1|   78|\n",
      "|  Alice|English|Semester2|   82|\n",
      "|Charlie|   Math|Semester1|   95|\n",
      "|Charlie|   Math|Semester2|   98|\n",
      "|Charlie|English|Semester1|   88|\n",
      "|Charlie|English|Semester2|   90|\n",
      "|  David|   Math|Semester1|   78|\n",
      "|  David|   Math|Semester2|   80|\n",
      "|    Eve|   Math|Semester1|   88|\n",
      "|    Eve|   Math|Semester2|   85|\n",
      "|    Eve|English|Semester1|   80|\n",
      "|    Eve|English|Semester2|   83|\n",
      "+-------+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter out students with an average grade lower than 75. \n",
    "\n",
    "df.filter(col('Grade') > 75).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "837d0e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|Subject|Latest_Semester|\n",
      "+-------+---------------+\n",
      "|English|      Semester2|\n",
      "|   Math|      Semester2|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the top 3 students with the highest grades in each subject for the latest semester. \n",
    "# Step 1: Determine the latest semester for each subject.\n",
    "\n",
    "latest_semester_df = df.groupBy('Subject').agg({'Semester':'max'}).withColumnRenamed('max(Semester)', 'Latest_Semester')\n",
    "latest_semester_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ae0d294b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Student|Subject|Grade|\n",
      "+-------+-------+-----+\n",
      "|  Alice|   Math|   90|\n",
      "|  Alice|English|   82|\n",
      "|    Bob|   Math|   70|\n",
      "|    Bob|English|   65|\n",
      "|Charlie|   Math|   98|\n",
      "|Charlie|English|   90|\n",
      "|  David|   Math|   80|\n",
      "|  David|English|   72|\n",
      "|    Eve|English|   83|\n",
      "|    Eve|   Math|   85|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the final results by subject and then by grade in descending order. \n",
    "\n",
    "df1 = df.alias('main')\n",
    "df2 = latest_semester_df.alias('latest')\n",
    "\n",
    "latest_grades_df = df1.join(df2, (df1.Subject == df2.Subject) & (df1.Semester == df2.Latest_Semester), how='inner')\\\n",
    "                   .select(df1.Student, df1.Subject, df1.Grade)\n",
    "latest_grades_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ad4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8eb11de",
   "metadata": {},
   "source": [
    "19\n",
    "- Display the schema of the created DataFrame.\n",
    "- Filter the DataFrame to show only transactions where the amount is greater than 100.\n",
    "- Add a new column discounted_amount that applies a 10% discount to all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8c5f0518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+--------------+\n",
      "|amount|customer_id|      date|transaction_id|\n",
      "+------+-----------+----------+--------------+\n",
      "|   150|        101|2025-01-01|             1|\n",
      "|    90|        102|2025-01-02|             2|\n",
      "|   200|        103|2025-01-03|             3|\n",
      "|    50|        104|2025-01-04|             4|\n",
      "|   120|        105|2025-01-05|             5|\n",
      "+------+-----------+----------+--------------+\n",
      "\n",
      "root\n",
      " |-- amount: long (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- transaction_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_data = [\n",
    " {\"transaction_id\": 1, \"customer_id\": 101, \"amount\": 150, \"date\": \"2025-01-01\"},\n",
    " {\"transaction_id\": 2, \"customer_id\": 102, \"amount\": 90, \"date\": \"2025-01-02\"},\n",
    " {\"transaction_id\": 3, \"customer_id\": 103, \"amount\": 200, \"date\": \"2025-01-03\"},\n",
    " {\"transaction_id\": 4, \"customer_id\": 104, \"amount\": 50, \"date\": \"2025-01-04\"},\n",
    " {\"transaction_id\": 5, \"customer_id\": 105, \"amount\": 120, \"date\": \"2025-01-05\"}\n",
    "]\n",
    "df = spark.createDataFrame(transaction_data)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dcecd16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+--------------+\n",
      "|amount|customer_id|      date|transaction_id|\n",
      "+------+-----------+----------+--------------+\n",
      "|   150|        101|2025-01-01|             1|\n",
      "|   200|        103|2025-01-03|             3|\n",
      "|   120|        105|2025-01-05|             5|\n",
      "+------+-----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.amount > 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dcf1df05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+--------------+-----------------+\n",
      "|amount|customer_id|      date|transaction_id|discounted_amount|\n",
      "+------+-----------+----------+--------------+-----------------+\n",
      "|   150|        101|2025-01-01|             1|            135.0|\n",
      "|    90|        102|2025-01-02|             2|             81.0|\n",
      "|   200|        103|2025-01-03|             3|            180.0|\n",
      "|    50|        104|2025-01-04|             4|             45.0|\n",
      "|   120|        105|2025-01-05|             5|            108.0|\n",
      "+------+-----------+----------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column discounted_amount that applies a 10% discount to all transactions.\n",
    "\n",
    "df.withColumn('discounted_amount', col('amount') * 0.9).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb724e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a20890",
   "metadata": {},
   "source": [
    "20 \n",
    "\n",
    "The sales data is collected daily, and the company wants to analyze the performance of each product across different regions. The data is stored in a PySpark DataFrame with the following schema:\n",
    "The management has requested a report where each region becomes a column and the values represent the total sales for each product in that region. Your task is to write PySpark code to generate this pivot table.\n",
    "\n",
    "Task:\n",
    "\n",
    "1. Load the sample data into a PySpark DataFrame.\n",
    "2. Use PySpark's pivot functionality to create a table where:\n",
    "Each region is a column.\n",
    "- The rows represent the products.\n",
    "- The values are the total sales for each product in each region.\n",
    "3. Provide the output DataFrame in a user-friendly format for the stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "389cefcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|Product|Region|Sales|\n",
      "+-------+------+-----+\n",
      "|      A| North|  100|\n",
      "|      B|  East|  200|\n",
      "|      A|  East|  150|\n",
      "|      C| North|  300|\n",
      "|      B| South|  400|\n",
      "|      C|  East|  250|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data =([ (\"A\", \"North\", 100), \n",
    "        (\"B\", \"East\", 200), \n",
    "        (\"A\", \"East\", 150),\n",
    "        (\"C\", \"North\", 300), \n",
    "        (\"B\", \"South\", 400), \n",
    "        (\"C\", \"East\", 250) ] )\n",
    "columns = [\"Product\", \"Region\", \"Sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae17ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+\n",
      "|Product|East|North|South|\n",
      "+-------+----+-----+-----+\n",
      "|      B| 200| NULL|  400|\n",
      "|      C| 250|  300| NULL|\n",
      "|      A| 150|  100| NULL|\n",
      "+-------+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Product').pivot('Region').agg(sum('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2b981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0896aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "944197eb",
   "metadata": {},
   "source": [
    "21 \n",
    "\n",
    "calculate the ROW_NUMBER() partitioned by department and ordered by salary in descending order. Additionally, the employees should be ranked within each department based on their hiring date if their salaries are the same. Add a new column called rank to the DataFrame that contains the calculated row numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c796d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+----------+\n",
      "|name| department|salary| hire_date|\n",
      "+----+-----------+------+----------+\n",
      "|John|         HR|  5000|2021-05-01|\n",
      "|Jane|         HR|  6000|2022-03-15|\n",
      "| Sam|Engineering|  7000|2021-06-01|\n",
      "|Anna|Engineering|  8000|2020-07-01|\n",
      "|Paul|         HR|  4500|2021-05-01|\n",
      "|Sara|Engineering|  7000|2020-08-01|\n",
      "| Tom|Engineering|  7500|2021-07-01|\n",
      "+----+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"John\", \"HR\", 5000, \"2021-05-01\"), \n",
    "(\"Jane\", \"HR\", 6000, \"2022-03-15\"), \n",
    "(\"Sam\", \"Engineering\", 7000, \"2021-06-01\"), \n",
    "(\"Anna\", \"Engineering\", 8000, \"2020-07-01\"), \n",
    "(\"Paul\", \"HR\", 4500, \"2021-05-01\"), \n",
    "(\"Sara\", \"Engineering\", 7000, \"2020-08-01\"), \n",
    "(\"Tom\", \"Engineering\", 7500, \"2021-07-01\") ]\n",
    "\n",
    "columns = [\"name\", \"department\", \"salary\", \"hire_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9a8fd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+----------+---+\n",
      "|name| department|salary| hire_date| rn|\n",
      "+----+-----------+------+----------+---+\n",
      "|Anna|Engineering|  8000|2020-07-01|  1|\n",
      "| Tom|Engineering|  7500|2021-07-01|  2|\n",
      "| Sam|Engineering|  7000|2021-06-01|  3|\n",
      "|Sara|Engineering|  7000|2020-08-01|  4|\n",
      "|Jane|         HR|  6000|2022-03-15|  1|\n",
      "|John|         HR|  5000|2021-05-01|  2|\n",
      "|Paul|         HR|  4500|2021-05-01|  3|\n",
      "+----+-----------+------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "win = Window.partitionBy('department').orderBy(col('salary').desc())\n",
    "df.withColumn('rn', row_number().over(win)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb7f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2567e981",
   "metadata": {},
   "source": [
    "## 22 :\n",
    "    \n",
    "You have the following dataset containing sales information for different products and regions. Reshape the data using PySpark's pivot() method to calculate the total sales for each product across regions, and then optimize it further by applying specific transformations.\n",
    "\n",
    "Task 1: Use pivot() to create a table showing the total sales for each product by region.\n",
    "\n",
    "Task 2: Add a column calculating the percentage contribution of each region to the total sales for that product.\n",
    "\n",
    "Task 3: Sort the data in descending order by total sales for each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5856b18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+-------+\n",
      "|Region|Product|Sales|Quarter|\n",
      "+------+-------+-----+-------+\n",
      "| North| Laptop| 2000|     Q1|\n",
      "| South| Laptop| 3000|     Q1|\n",
      "|  East| Laptop| 2500|     Q1|\n",
      "| North|  Phone| 1500|     Q1|\n",
      "| South|  Phone| 1000|     Q1|\n",
      "|  East|  Phone| 2000|     Q1|\n",
      "| North| Laptop| 3000|     Q2|\n",
      "| South| Laptop| 4000|     Q2|\n",
      "|  East| Laptop| 3500|     Q2|\n",
      "| North|  Phone| 2500|     Q2|\n",
      "| South|  Phone| 1500|     Q2|\n",
      "|  East|  Phone| 3000|     Q2|\n",
      "+------+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"North\", \"Laptop\", 2000, \"Q1\"),\n",
    "        (\"South\", \"Laptop\", 3000, \"Q1\"), \n",
    "        (\"East\", \"Laptop\", 2500, \"Q1\"), \n",
    "        (\"North\", \"Phone\", 1500, \"Q1\"),\n",
    "        (\"South\", \"Phone\", 1000, \"Q1\"), \n",
    "        (\"East\", \"Phone\", 2000, \"Q1\"),\n",
    "        (\"North\", \"Laptop\", 3000, \"Q2\"), \n",
    "        (\"South\", \"Laptop\", 4000, \"Q2\"),\n",
    "        (\"East\", \"Laptop\", 3500, \"Q2\"),\n",
    "        (\"North\", \"Phone\", 2500, \"Q2\"),\n",
    "        (\"South\", \"Phone\", 1500, \"Q2\"), \n",
    "        (\"East\", \"Phone\", 3000, \"Q2\") ]\n",
    "\n",
    "columns = [\"Region\", \"Product\", \"Sales\", \"Quarter\"] \n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b41fbc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+\n",
      "|Product|East|North|South|\n",
      "+-------+----+-----+-----+\n",
      "|  Phone|5000| 4000| 2500|\n",
      "| Laptop|6000| 5000| 7000|\n",
      "+-------+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot to calculate total sales by region\n",
    "pivot_df = df.groupBy('Product').pivot('Region').sum('Sales')\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9c2de97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "|Product|East|North|South|Total_Sales|North_%|South_%|East_%|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "|  Phone|5000| 4000| 2500|      11500|  34.78|  21.74| 43.48|\n",
      "| Laptop|6000| 5000| 7000|      18000|  27.78|  38.89| 33.33|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column calculating the percentage contribution of each region to the total sales for that product.\n",
    "\n",
    "total_seles_df = pivot_df\\\n",
    "    .withColumn('Total_Sales', col('North') + col('South') + col('East'))\\\n",
    "    .withColumn('North_%', round(col('North') / col('Total_Sales') * 100, 2))\\\n",
    "    .withColumn('South_%', round(col('South') / col('Total_Sales') * 100, 2))\\\n",
    "    .withColumn('East_%', round(col('East') / col('Total_Sales') * 100, 2))\n",
    "\n",
    "total_seles_df.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6059836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "|Product|East|North|South|Total_Sales|North_%|South_%|East_%|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "| Laptop|6000| 5000| 7000|      18000|  27.78|  38.89| 33.33|\n",
      "|  Phone|5000| 4000| 2500|      11500|  34.78|  21.74| 43.48|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "|Product|East|North|South|Total_Sales|North_%|South_%|East_%|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "| Laptop|6000| 5000| 7000|      18000|  27.78|  38.89| 33.33|\n",
      "|  Phone|5000| 4000| 2500|      11500|  34.78|  21.74| 43.48|\n",
      "+-------+----+-----+-----+-----------+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data in descending order by total sales for each product.\n",
    "\n",
    "total_seles_df.sort(col('Total_Sales').desc()).show()\n",
    "# OR \n",
    "total_seles_df.orderBy(col('Total_Sales').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0251e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "523e2952",
   "metadata": {},
   "source": [
    " 24\n",
    "\n",
    "You are given a nested JSON file named sample_data.json stored in an S3 bucket at s3://your-bucket/sample_data.json. The JSON file contains details about employees, including their names, departments, and address details (nested fields).\n",
    "\n",
    "Write a PySpark program to:\n",
    "- Load the JSON file into a DataFrame.\n",
    "- Flatten the nested structure to create a tabular format.\n",
    "- Write the resulting DataFrame as a Parquet file to the output path s3://your-bucket/output/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "73f9c366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+---+-------+\n",
      "|            address|department| id|   name|\n",
      "+-------------------+----------+---+-------+\n",
      "|     {New York, NY}|        HR|  1|  Alice|\n",
      "|{San Francisco, CA}|        IT|  2|    Bob|\n",
      "|      {Chicago, IL}|   Finance|  3|Charlie|\n",
      "+-------------------+----------+---+-------+\n",
      "\n",
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from google.cloud import storage\n",
    "storage_client = storage.client(project = project_id)\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blobs = bucket.list_blobs(prefix = subdirectory_name)\n",
    "for obj in blobs:\n",
    "    blob1 = obj.name\n",
    "    blob = bucket.blob(blob1)\n",
    "    blob.download_to_filename(blob1)\n",
    "    \n",
    "#upload file \n",
    "path = (\"{0}/output/{1}\".format(subdirectory_name, f_name))\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(path)\n",
    "blob.upload_from_filename(f_name)\"\"\"\n",
    "\n",
    "#df = spark.read.json('sample_data.json')\n",
    "\n",
    "f_df = spark.read.format('json')\\\n",
    "           .option('inferschema', True)\\\n",
    "           .option('multiline', True)\\\n",
    "           .load('sample_data.json')\n",
    "f_df.show()     \n",
    "f_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74f26655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------------+-------------+\n",
      "| id|   name|department| address_city|address_state|\n",
      "+---+-------+----------+-------------+-------------+\n",
      "|  1|  Alice|        HR|     New York|           NY|\n",
      "|  2|    Bob|        IT|San Francisco|           CA|\n",
      "|  3|Charlie|   Finance|      Chicago|           IL|\n",
      "+---+-------+----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = f_df.select('id','name','department', 'address.city','address.state')\\\n",
    "                                           .withColumnRenamed('city','address_city')\\\n",
    "                                           .withColumnRenamed('state','address_state')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "efda0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.mode('overwrite').parquet('path')\n",
    "\"\"\"df.write.format('json')\\\n",
    "        .option('header', True)\\\n",
    "        .option('mode', 'overwrite')\\\n",
    "        .option('path','newJson/')\\\n",
    "        .save()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba0ec1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48b11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd9221cb",
   "metadata": {},
   "source": [
    "26 split the values in this column into multiple rows to make the dataset suitable for downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "beb9227d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| ID|               Tags|\n",
      "+---+-------------------+\n",
      "|  1|apple,banana,orange|\n",
      "|  2|       mango,grapes|\n",
      "|  3|          pineapple|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, \"apple,banana,orange\"), \n",
    " (2, \"mango,grapes\"), (3, \"pineapple\") ] \n",
    "\n",
    "columns = [\"ID\", \"Tags\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21ea1c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| ID|      Tag|\n",
      "+---+---------+\n",
      "|  1|    apple|\n",
      "|  1|   banana|\n",
      "|  1|   orange|\n",
      "|  2|    mango|\n",
      "|  2|   grapes|\n",
      "|  3|pineapple|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Tag', explode(split(df['Tags'], ','))).drop('Tags').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101ceb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfe5c39e",
   "metadata": {},
   "source": [
    " 27 \n",
    "\n",
    "Imagine you're analyzing the monthly sales performance of a company across different regions. You want to calculate:\n",
    "\n",
    "- The cumulative sales for each region over months.\n",
    "- The rank of each month based on sales within the same region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfe905ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|Region|Month|Sales|\n",
      "+------+-----+-----+\n",
      "|  East|  Jan|  200|\n",
      "|  East|  Feb|  300|\n",
      "|  East|  Mar|  250|\n",
      "|  West|  Jan|  400|\n",
      "|  West|  Feb|  350|\n",
      "|  West|  Mar|  450|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"East\", \"Jan\", 200), (\"East\", \"Feb\", 300), \n",
    "(\"East\", \"Mar\", 250), (\"West\", \"Jan\", 400), \n",
    "(\"West\", \"Feb\", 350), (\"West\", \"Mar\", 450) ]\n",
    "\n",
    "columns = [\"Region\", \"Month\", \"Sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70ec4b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+----------------+\n",
      "|Region|Month|Sales|Cumulative_Sales|\n",
      "+------+-----+-----+----------------+\n",
      "|  East|  Jan|  200|             200|\n",
      "|  East|  Mar|  250|             450|\n",
      "|  East|  Feb|  300|             750|\n",
      "|  West|  Feb|  350|             350|\n",
      "|  West|  Jan|  400|             750|\n",
      "|  West|  Mar|  450|            1200|\n",
      "+------+-----+-----+----------------+\n",
      "\n",
      "+------+-----+-----+----------------+----+\n",
      "|Region|Month|Sales|Cumulative_Sales|Rank|\n",
      "+------+-----+-----+----------------+----+\n",
      "|  East|  Jan|  200|             200|   1|\n",
      "|  East|  Mar|  250|             450|   2|\n",
      "|  East|  Feb|  300|             750|   3|\n",
      "|  West|  Feb|  350|             350|   1|\n",
      "|  West|  Jan|  400|             750|   2|\n",
      "|  West|  Mar|  450|            1200|   3|\n",
      "+------+-----+-----+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a window partitionBy by Region and ordered by Sales\n",
    "window_spec = Window.partitionBy('Region').orderBy('Sales')\n",
    "\n",
    "# Add cumulative sum and rank columns:\n",
    "res_df = df.withColumn('Cumulative_Sales', sum('Sales').over(window_spec))\n",
    "res_df.show()\n",
    "res_df = res_df.withColumn('Rank', rank().over(window_spec))\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1abad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309178f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2180875a",
   "metadata": {},
   "source": [
    " 28\n",
    "\n",
    "You are working as a Data Engineer and need to clean up a dataset that contains customer order information. The dataset includes details such as the customer ID, order ID, order date, and the total amount. Due to a data processing issue, some rows are duplicated, and you need to remove duplicates based on a composite key of customer_id and order_id, keeping only the latest order (based on the order_date).\n",
    "\n",
    "You need to remove the duplicate rows based on the composite key (customer_id, order_id) and retain only the row with the latest order_date for each combination of customer_id and order_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9046ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+\n",
      "|customer_id|order_id|order_date|total_amount|\n",
      "+-----------+--------+----------+------------+\n",
      "|        101|    1001|2025-01-15|       500.0|\n",
      "|        102|    1002|2025-01-14|       300.0|\n",
      "|        101|    1001|2025-01-17|       550.0|\n",
      "|        103|    1003|2025-01-16|       450.0|\n",
      "|        102|    1002|2025-01-18|       320.0|\n",
      "|        103|    1003|2025-01-19|       460.0|\n",
      "+-----------+--------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (101, 1001, \"2025-01-15\", 500.00), (102, 1002, \"2025-01-14\", 300.00), (101, 1001, \"2025-01-17\", 550.00), (103, 1003, \"2025-01-16\", 450.00),\n",
    "(102, 1002, \"2025-01-18\", 320.00), (103, 1003, \"2025-01-19\", 460.00) ]\n",
    "\n",
    "schema = [\"customer_id\", \"order_id\", \"order_date\", \"total_amount\"] \n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad4b125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+\n",
      "|customer_id|order_id|order_date|total_amount|\n",
      "+-----------+--------+----------+------------+\n",
      "|        101|    1001|2025-01-15|       500.0|\n",
      "|        102|    1002|2025-01-14|       300.0|\n",
      "|        101|    1001|2025-01-17|       550.0|\n",
      "|        103|    1003|2025-01-16|       450.0|\n",
      "|        102|    1002|2025-01-18|       320.0|\n",
      "|        103|    1003|2025-01-19|       460.0|\n",
      "+-----------+--------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.drop_duplicates(['customer_id','order_id']).show()\n",
    "\n",
    "# Cast the order_date to DateType for proper compasion\n",
    "df = df.withColumn('order_date', to_date(df['order_date'], 'yyyy-MM-dd'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "204612ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+-------+\n",
      "|customer_id|order_id|order_date|total_amount|row_num|\n",
      "+-----------+--------+----------+------------+-------+\n",
      "|        101|    1001|2025-01-17|       550.0|      1|\n",
      "|        101|    1001|2025-01-15|       500.0|      2|\n",
      "|        102|    1002|2025-01-18|       320.0|      1|\n",
      "|        102|    1002|2025-01-14|       300.0|      2|\n",
      "|        103|    1003|2025-01-19|       460.0|      1|\n",
      "|        103|    1003|2025-01-16|       450.0|      2|\n",
      "+-----------+--------+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window specification to get the latest order per customer_id and order_id\n",
    "\n",
    "window_spec = Window.partitionBy('customer_id', 'order_id').orderBy(col('order_date').desc())\n",
    "\n",
    "# Add row_number to identify the latest record\n",
    "df_with_row_number = df.withColumn('row_num', row_number().over(window_spec))\n",
    "df_with_row_number.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58613696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+\n",
      "|customer_id|order_id|order_date|total_amount|\n",
      "+-----------+--------+----------+------------+\n",
      "|        101|    1001|2025-01-17|       550.0|\n",
      "|        102|    1002|2025-01-18|       320.0|\n",
      "|        103|    1003|2025-01-19|       460.0|\n",
      "+-----------+--------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to get onnly the latest record \n",
    "\n",
    "res_df = df_with_row_number.filter(col('row_num') == 1).drop('row_num')\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c743c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65426db5",
   "metadata": {},
   "source": [
    "29 The columns contain different types of data, including numeric, categorical, and string values. Your objective is to:\n",
    "\n",
    "1. Fill numeric columns with the median value.\n",
    "2. Fill categorical columns with the most frequent value.\n",
    "3. Fill string columns with \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3b6282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Customer_ID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Last_Visit: string (nullable = true)\n",
      " |-- Purchase_Amount: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+-----------+----+------+------+----------+---------------+\n",
      "|Customer_ID| Age|Region|Gender|Last_Visit|Purchase_Amount|\n",
      "+-----------+----+------+------+----------+---------------+\n",
      "|          1|  25| North|     M|2025-01-01|            150|\n",
      "|          2|NULL|  East|  NULL|2025-01-02|           NULL|\n",
      "|          3|  30| South|     F|      NULL|            200|\n",
      "|          4|  22|  NULL|     M|2025-01-03|            180|\n",
      "|          5|  28|  West|     F|      NULL|           NULL|\n",
      "+-----------+----+------+------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, 25, 'North', 'M', '2025-01-01', 150),\n",
    "(2, None, 'East', None, '2025-01-02', None),\n",
    "(3, 30, 'South', 'F', None, 200),\n",
    "(4, 22, None, 'M', '2025-01-03', 180),\n",
    "(5, 28, 'West', 'F', None, None), ]\n",
    "schema = StructType([StructField('Customer_ID', IntegerType\n",
    "                                 (), True),\n",
    "                    StructField('Age', IntegerType(), True),\n",
    "                    StructField('Region', StringType(), True),\n",
    "                    StructField('Gender', StringType(), True),\n",
    "                     StructField('Last_Visit', StringType(), True),\n",
    "                     StructField('Purchase_Amount', IntegerType(), True),\n",
    "                    ])\n",
    "\n",
    "#columns = ['Customer_ID', 'Age', 'Region', 'Gender', 'Last_Visit', 'Purchase_Amount'] \n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(df.printSchema())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82d07d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_types:  [('Customer_ID', 'int'), ('Age', 'int'), ('Region', 'string'), ('Gender', 'string'), ('Last_Visit', 'string'), ('Purchase_Amount', 'int')]\n",
      "+-----------+---+-------+-------+----------+---------------+\n",
      "|Customer_ID|Age| Region| Gender|Last_Visit|Purchase_Amount|\n",
      "+-----------+---+-------+-------+----------+---------------+\n",
      "|          1| 25|  North|      M|2025-01-01|            150|\n",
      "|          2| 25|   East|Unknown|2025-01-02|            180|\n",
      "|          3| 30|  South|      F|   Unknown|            200|\n",
      "|          4| 22|Unknown|      M|2025-01-03|            180|\n",
      "|          5| 28|   West|      F|   Unknown|            180|\n",
      "+-----------+---+-------+-------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to fill missing values dynamically\n",
    "def fill_missing_values(df):\n",
    "    \n",
    "    column_types = df.dtypes\n",
    "    print('column_types: ', column_types)\n",
    "    \n",
    "    # loop through each column based on type\n",
    "    for column, dtype in column_types:\n",
    "        \n",
    "        if dtype == 'int' or dtype == 'double' or dtype == 'long':\n",
    "            median_value = df.approxQuantile(column, [0.5], 0)[0]\n",
    "            df = df.fillna({column: median_value})\n",
    "            \n",
    "        elif dtype == 'string':\n",
    "            df = df.fillna({column: 'Unknown'})\n",
    "            \n",
    "        else:\n",
    "            df = df.fillna({column: 'Unknown'})\n",
    "        \n",
    "    return df\n",
    "filled_df = fill_missing_values(df)\n",
    "filled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8def4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e13499d",
   "metadata": {},
   "source": [
    "30 Validate the date format and filter rows where input_date matches the format \"yyyy-MM-dd\".\n",
    "\n",
    "- Transform valid dates into the format \"MM/dd/yyyy\".\n",
    "- For invalid dates, replace them with the string \"Invalid Date\".\n",
    "- Output the transformed DataFrame with a new column named validated_date.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5d7806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|input_date|\n",
      "+----------+\n",
      "|2023-12-31|\n",
      "|31-12-2023|\n",
      "|2023/12/31|\n",
      "|2024-01-01|\n",
      "|13-01-2023|\n",
      "|   invalid|\n",
      "|2022-02-28|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"2023-12-31\",), (\"31-12-2023\",),\n",
    "(\"2023/12/31\",), (\"2024-01-01\",),\n",
    "(\"13-01-2023\",), (\"invalid\",), (\"2022-02-28\",) ]\n",
    "columns = [\"input_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40796307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|input_date|validated_date|\n",
      "+----------+--------------+\n",
      "|2023-12-31|    12/31/2023|\n",
      "|31-12-2023|  Invalid_date|\n",
      "|2023/12/31|  Invalid_date|\n",
      "|2024-01-01|    01/01/2024|\n",
      "|13-01-2023|  Invalid_date|\n",
      "|   invalid|  Invalid_date|\n",
      "|2022-02-28|    02/28/2022|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df.withColumn('validated_date', when(to_date(col('input_date'), 'yyyy-MM-dd').isNotNull(), date_format(to_date(col('input_date'), 'yyyy-MM-dd'), 'MM/dd/yyyy')).otherwise('Invalid_date'))\n",
    "result_df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee862f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4bc1191",
   "metadata": {},
   "source": [
    " 34\n",
    "\n",
    "You are given a dataset of sales transactions for multiple stores and products.\n",
    "- Calculate the percentage contribution of each product's sales to the total sales of its store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee0516c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|StoreID|Product|Sales|\n",
      "+-------+-------+-----+\n",
      "|     S1|     P1|  100|\n",
      "|     S1|     P2|  200|\n",
      "|     S1|     P3|  300|\n",
      "|     S2|     P1|  400|\n",
      "|     S2|     P2|  100|\n",
      "|     S2|     P3|  500|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"S1\", \"P1\", 100), (\"S1\", \"P2\", 200),\n",
    "(\"S1\", \"P3\", 300), (\"S2\", \"P1\", 400),\n",
    "(\"S2\", \"P2\", 100), (\"S2\", \"P3\", 500) ]\n",
    "columns = [\"StoreID\", \"Product\", \"Sales\"] \n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9220d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|StoreID|Total_Sales|\n",
      "+-------+-----------+\n",
      "|     S1|        600|\n",
      "|     S2|       1000|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('StoreID').agg(sum('Sales').alias('Total_Sales'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "062a90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----------+\n",
      "|StoreID|Product|Sales|Total_Sales|\n",
      "+-------+-------+-----+-----------+\n",
      "|     S1|     P1|  100|        600|\n",
      "|     S1|     P2|  200|        600|\n",
      "|     S1|     P3|  300|        600|\n",
      "|     S2|     P1|  400|       1000|\n",
      "|     S2|     P2|  100|       1000|\n",
      "|     S2|     P3|  500|       1000|\n",
      "+-------+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = df.join(df1, on='StoreID', how= 'inner')\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284210dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-----------+------+\n",
      "|StoreID|Product|Sales|Total_Sales|percnt|\n",
      "+-------+-------+-----+-----------+------+\n",
      "|     S1|     P1|  100|        600| 16.67|\n",
      "|     S1|     P2|  200|        600| 33.33|\n",
      "|     S1|     P3|  300|        600|  50.0|\n",
      "|     S2|     P1|  400|       1000|  40.0|\n",
      "|     S2|     P2|  100|       1000|  10.0|\n",
      "|     S2|     P3|  500|       1000|  50.0|\n",
      "+-------+-------+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.withColumn('percnt', round((col('Sales') / col('Total_sales'))*100,2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d5967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ff55a6f",
   "metadata": {},
   "source": [
    " 35\n",
    "\n",
    "You are working as a Data Engineer at a retail company. The marketing team has provided a dataset of customer purchases to analyze the relationship between the amount spent on advertisements and the revenue generated. \n",
    "- Using PySpark, compute the correlation between the \"Ad_Spend\" and \"Revenue\" columns to determine if there's a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df8646ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+\n",
      "|Customer_ID|Ad_Spend|Revenue|\n",
      "+-----------+--------+-------+\n",
      "|       C001|    2000|  25000|\n",
      "|       C002|    1500|  23000|\n",
      "|       C003|    3000|  40000|\n",
      "|       C004|    1200|  18000|\n",
      "|       C005|    2500|  30000|\n",
      "+-----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ StructField(\"Customer_ID\", StringType(), True), StructField(\"Ad_Spend\", IntegerType(), True), StructField(\"Revenue\", IntegerType(), True) ])\n",
    "data = [ (\"C001\", 2000, 25000), (\"C002\", 1500, 23000),\n",
    "(\"C003\", 3000, 40000), (\"C004\", 1200, 18000),\n",
    "(\"C005\", 2500, 30000) ] \n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3febbf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between ad_spend and revenue is: 0.9704535552410213\n"
     ]
    }
   ],
   "source": [
    "correlation = df.stat.corr('Ad_Spend', 'Revenue')\n",
    "print('The correlation between ad_spend and revenue is:' ,correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40323a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8911103",
   "metadata": {},
   "source": [
    " 36\n",
    "\n",
    "You are given a large e-commerce transaction dataset stored in a partitioned format based on country. \n",
    "- Count the distinct number of products purchased (product_id) for each customer_id in every country. The result should include the country, customer ID, and the distinct product count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56fad9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+\n",
      "|country|customer_id|product_id|\n",
      "+-------+-----------+----------+\n",
      "|    USA|        101|      P001|\n",
      "|    USA|        101|      P002|\n",
      "|    USA|        101|      P001|\n",
      "|    USA|        102|      P003|\n",
      "|    USA|        102|      P003|\n",
      "|     UK|        201|      P004|\n",
      "|     UK|        201|      P005|\n",
      "|     UK|        202|      P004|\n",
      "|     UK|        202|      P005|\n",
      "|     UK|        202|      P004|\n",
      "+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"USA\", 101, \"P001\"), \n",
    "(\"USA\", 101, \"P002\"), (\"USA\", 101, \"P001\"), \n",
    "(\"USA\", 102, \"P003\"), (\"USA\", 102, \"P003\"), \n",
    "(\"UK\", 201, \"P004\"), (\"UK\", 201, \"P005\"), \n",
    "(\"UK\", 202, \"P004\"), (\"UK\", 202, \"P005\"), (\"UK\", 202, \"P004\") ]\n",
    "\n",
    "columns = [\"country\", \"customer_id\", \"product_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "575b4c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------------+\n",
      "|country|customer_id|count(DISTINCT product_id)|\n",
      "+-------+-----------+--------------------------+\n",
      "|    USA|        101|                         2|\n",
      "|     UK|        202|                         2|\n",
      "|     UK|        201|                         2|\n",
      "|    USA|        102|                         1|\n",
      "+-------+-----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('country', 'customer_id').agg(countDistinct(col('product_id'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94eb9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63194096",
   "metadata": {},
   "source": [
    "$Broadcast$ the smaller DataFrame (product_data). 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9400748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----------+\n",
      "|sale_id|product_id|quantity| sale_date|\n",
      "+-------+----------+--------+----------+\n",
      "|      1|       101|       5|2025-01-01|\n",
      "|      2|       102|       3|2025-01-02|\n",
      "|      3|       103|       2|2025-01-03|\n",
      "|      4|       101|       1|2025-01-04|\n",
      "|      5|       104|       4|2025-01-05|\n",
      "|      6|       105|       6|2025-01-06|\n",
      "+-------+----------+--------+----------+\n",
      "\n",
      "+----------+------------+-----------+-----+\n",
      "|product_id|product_name|   category|price|\n",
      "+----------+------------+-----------+-----+\n",
      "|       101|      Laptop|Electronics| 1000|\n",
      "|       102|       Phone|Electronics|  500|\n",
      "|       103|  Headphones|Accessories|  150|\n",
      "|       104|      Tablet|Electronics|  600|\n",
      "|       105|  Smartwatch|Accessories|  200|\n",
      "+----------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = [ (1, 101, 5, '2025-01-01'), (2, 102, 3, '2025-01-02'),\n",
    "(3, 103, 2, '2025-01-03'), (4, 101, 1, '2025-01-04'),\n",
    "(5, 104, 4, '2025-01-05'), (6, 105, 6, '2025-01-06'), ]\n",
    "\n",
    "# product_data (Small DataFrame)\n",
    "\n",
    "product_data = [ (101, 'Laptop', 'Electronics', 1000),\n",
    "(102, 'Phone', 'Electronics', 500),\n",
    "(103, 'Headphones', 'Accessories', 150),\n",
    "(104, 'Tablet', 'Electronics', 600),\n",
    "(105, 'Smartwatch', 'Accessories', 200), ] \n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, ['sale_id', 'product_id','quantity', 'sale_date'])\n",
    "product_df= spark.createDataFrame(product_data, ['product_id', 'product_name', 'category', 'price'])\n",
    "sales_df.show()\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2738e9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+----------+------------+-----------+-----+\n",
      "|product_id|sale_id|quantity|sale_date |product_name|category   |price|\n",
      "+----------+-------+--------+----------+------------+-----------+-----+\n",
      "|101       |1      |5       |2025-01-01|Laptop      |Electronics|1000 |\n",
      "|102       |2      |3       |2025-01-02|Phone       |Electronics|500  |\n",
      "|103       |3      |2       |2025-01-03|Headphones  |Accessories|150  |\n",
      "|101       |4      |1       |2025-01-04|Laptop      |Electronics|1000 |\n",
      "|104       |5      |4       |2025-01-05|Tablet      |Electronics|600  |\n",
      "|105       |6      |6       |2025-01-06|Smartwatch  |Accessories|200  |\n",
      "+----------+-------+--------+----------+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "prod_df_broadcast = broadcast(product_df)\n",
    "\n",
    "joined_df = sales_df.join(prod_df_broadcast, on='product_id', how='inner')\n",
    "joined_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa7402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "170ccafe",
   "metadata": {},
   "source": [
    " 39\n",
    "\n",
    "You are working with large datasets in PySpark and need to join two DataFrames. However, one of the tables has highly skewed data, causing performance issues due to data shuffling. How would you optimize this join using salting techniques?\n",
    "You are given the following sample datasets:\n",
    "\n",
    "sales_df (Fact Table - Large Dataset, Highly Skewed on store_id)\n",
    "Your task is to perform an optimized join between sales_df and store_df on store_id, ensuring that the skewness does not degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d478d53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|store_id|product_id|amount|\n",
      "+--------+----------+------+\n",
      "|     101|      P001|   100|\n",
      "|     101|      P002|   200|\n",
      "|     101|      P003|   150|\n",
      "|     102|      P004|   300|\n",
      "|     103|      P005|   400|\n",
      "|     101|      P006|   500|\n",
      "|     104|      P007|   250|\n",
      "+--------+----------+------+\n",
      "\n",
      "+--------+----------+\n",
      "|store_id|store_name|\n",
      "+--------+----------+\n",
      "|     101|   Walmart|\n",
      "|     102|    Target|\n",
      "|     103|    Costco|\n",
      "|     104|   BestBuy|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = [ (101, \"P001\", 100), (101, \"P002\", 200), (101, \"P003\", 150), (102, \"P004\", 300), \n",
    "              (103, \"P005\", 400), (101, \"P006\", 500), (104, \"P007\", 250) ] \n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"store_id\", \"product_id\", \"amount\"]) \n",
    "sales_df.show()\n",
    "\n",
    "store_data = [(101, \"Walmart\"), (102, \"Target\"), (103, \"Costco\"), (104, \"BestBuy\")] \n",
    "store_df = spark.createDataFrame(store_data, [\"store_id\", \"store_name\"]) \n",
    "store_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9acce019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----+---------------+\n",
      "|store_id|product_id|amount|salt|salted_store_id|\n",
      "+--------+----------+------+----+---------------+\n",
      "|     101|      P001|   100|   1|          101_1|\n",
      "|     101|      P002|   200|   0|          101_0|\n",
      "|     101|      P003|   150|   0|          101_0|\n",
      "|     102|      P004|   300|   0|          102_0|\n",
      "|     103|      P005|   400|   0|          103_0|\n",
      "|     101|      P006|   500|   2|          101_2|\n",
      "|     104|      P007|   250|   0|          104_0|\n",
      "+--------+----------+------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Adding Salt to skewed 'sales_df'\n",
    "\n",
    "num_salt_keys = 3  # Define the range of salt keys \n",
    "\n",
    "sales_df_salted = sales_df.withColumn('salt', floor(rand() * num_salt_keys))\\\n",
    "                          .withColumn('salted_store_id', concat_ws(\"_\", col('store_id'), col('salt')))\n",
    "sales_df_salted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c81b3f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----+---------------+\n",
      "|store_id|store_name|salt|salted_store_id|\n",
      "+--------+----------+----+---------------+\n",
      "|     101|   Walmart|   0|          101_0|\n",
      "|     101|   Walmart|   1|          101_1|\n",
      "|     101|   Walmart|   2|          101_2|\n",
      "|     102|    Target|   0|          102_0|\n",
      "|     102|    Target|   1|          102_1|\n",
      "|     102|    Target|   2|          102_2|\n",
      "|     103|    Costco|   0|          103_0|\n",
      "|     103|    Costco|   1|          103_1|\n",
      "|     103|    Costco|   2|          103_2|\n",
      "|     104|   BestBuy|   0|          104_0|\n",
      "|     104|   BestBuy|   1|          104_1|\n",
      "|     104|   BestBuy|   2|          104_2|\n",
      "+--------+----------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Expanding 'store_df' for Join compatibility \n",
    "\n",
    "expanded_store_df = store_df.crossJoin(spark.range(0, num_salt_keys).toDF('salt'))\\\n",
    "                    .withColumn('salted_store_id', concat_ws('_', col('store_id'), col('salt')))\n",
    "expanded_store_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebd7faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+--------+----------+\n",
      "|store_id|product_id|amount|store_id|store_name|\n",
      "+--------+----------+------+--------+----------+\n",
      "|     101|      P002|   200|     101|   Walmart|\n",
      "|     101|      P003|   150|     101|   Walmart|\n",
      "|     101|      P001|   100|     101|   Walmart|\n",
      "|     101|      P006|   500|     101|   Walmart|\n",
      "|     102|      P004|   300|     102|    Target|\n",
      "|     103|      P005|   400|     103|    Costco|\n",
      "|     104|      P007|   250|     104|   BestBuy|\n",
      "+--------+----------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Performing the Optimized Join on Salted Keys\n",
    "\n",
    "joined_df = sales_df_salted.join(expanded_store_df,'salted_store_id', 'inner').drop('salted_store_id', 'salt')\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e8244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9f03fd0",
   "metadata": {},
   "source": [
    "\"\"\" 40\n",
    "\n",
    "You are working as a Data Engineer at a fintech company. Your team is working on integrating two datasets:\n",
    "\n",
    "1. Customer Transactions Data (transactions_df) - Contains customer transactions with columns: customer_id, txn_id, amount, and txn_date. \n",
    "\n",
    "2. Customer Profile Data (profile_df) - Contains customer information with columns: customer_id, name, age, and txn_id (latest transaction ID for reference).\n",
    "\n",
    " The requirement is to merge these two DataFrames on customer_id while keeping track of:\n",
    "\n",
    "Conflicting column names (txn_id) should be renamed properly.\n",
    "\n",
    "If a customer exists in profile_df but not in transactions_df, the row should still be present with NULL values for transaction-related columns.\n",
    "\n",
    "Your task is to write an optimized PySpark code to achieve this.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0224c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----------+\n",
      "|customer_id|txn_id|amount|  txn_date|\n",
      "+-----------+------+------+----------+\n",
      "|        101|  T001|   500|2024-08-10|\n",
      "|        102|  T002|  1200|2024-08-09|\n",
      "|        103|  T003|   300|2024-08-08|\n",
      "|        104|  T004|   450|2024-08-07|\n",
      "+-----------+------+------+----------+\n",
      "\n",
      "+-----------+----+---+------+\n",
      "|customer_id|name|age|txn_id|\n",
      "+-----------+----+---+------+\n",
      "|        101|John| 30|  T001|\n",
      "|        102|Emma| 27|  T005|\n",
      "|        103|Alex| 35|  T003|\n",
      "|        105| Sam| 40|  T006|\n",
      "+-----------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_data = [ (101, \"T001\", 500, \"2024-08-10\"), (102, \"T002\", 1200, \"2024-08-09\"), \n",
    "                     (103, \"T003\", 300, \"2024-08-08\"), (104, \"T004\", 450, \"2024-08-07\"), ] \n",
    "\n",
    "profile_data = [ (101, \"John\", 30, \"T001\"), (102, \"Emma\", 27, \"T005\"), \n",
    "                (103, \"Alex\", 35, \"T003\"), (105, \"Sam\", 40, \"T006\"), ]\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_data, [\"customer_id\", \"txn_id\", \"amount\", \"txn_date\"])\n",
    "transactions_df.show()\n",
    "\n",
    "profile_df = spark.createDataFrame(profile_data, [\"customer_id\", \"name\", \"age\", \"txn_id\"])\n",
    "profile_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764ca0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+-----------+\n",
      "|customer_id|name|age|last_txn_id|\n",
      "+-----------+----+---+-----------+\n",
      "|        101|John| 30|       T001|\n",
      "|        102|Emma| 27|       T005|\n",
      "|        103|Alex| 35|       T003|\n",
      "|        105| Sam| 40|       T006|\n",
      "+-----------+----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df = profile_df.withColumnRenamed('txn_id', 'last_txn_id')\n",
    "profile_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0597d463",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+-----------+------+------+----------+\n",
      "|customer_id|name| age|last_txn_id|txn_id|amount|  txn_date|\n",
      "+-----------+----+----+-----------+------+------+----------+\n",
      "|        101|John|  30|       T001|  T001|   500|2024-08-10|\n",
      "|        102|Emma|  27|       T005|  T002|  1200|2024-08-09|\n",
      "|        103|Alex|  35|       T003|  T003|   300|2024-08-08|\n",
      "|        104|NULL|NULL|       NULL|  T004|   450|2024-08-07|\n",
      "|        105| Sam|  40|       T006|  NULL|  NULL|      NULL|\n",
      "+-----------+----+----+-----------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df.join(transactions_df, on = \"customer_id\", how = \"full_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ab082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5d0009c",
   "metadata": {},
   "source": [
    " 42\n",
    "\n",
    "You are given an employee dataset containing information about employees and their managers. Each employee has a manager_id that refers to another employee in the same table. Your task is to use self-join to find hierarchical relationships between employees, such as finding all employees under a specific manager or the reporting hierarchy of an employee.\n",
    "\n",
    "Interview Task\n",
    "- Write a PySpark self-join query to find the direct reports of each manager. Additionally, extend the logic to find all hierarchical relationships up to any level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd26875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+\n",
      "|employee_id|employee_name|manager_id|\n",
      "+-----------+-------------+----------+\n",
      "|          1|        Alice|      NULL|\n",
      "|          2|          Bob|         1|\n",
      "|          3|      Charlie|         1|\n",
      "|          4|        David|         2|\n",
      "|          5|          Eva|         2|\n",
      "|          6|        Frank|         3|\n",
      "|          7|        Grace|         3|\n",
      "+-----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, \"Alice\", None), (2, \"Bob\", 1),\n",
    "(3, \"Charlie\", 1), (4, \"David\", 2),\n",
    "(5, \"Eva\", 2), (6, \"Frank\", 3), (7, \"Grace\", 3) ]\n",
    "\n",
    "columns = [\"employee_id\", \"employee_name\", \"manager_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4516b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|employee_name|employee_name|\n",
      "+-------------+-------------+\n",
      "|         NULL|        Alice|\n",
      "|        Alice|          Bob|\n",
      "|        Alice|      Charlie|\n",
      "|          Bob|        David|\n",
      "|          Bob|          Eva|\n",
      "|      Charlie|        Frank|\n",
      "|      Charlie|        Grace|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('emp').join(df.alias('mgr'), col('mgr.employee_id') == col('emp.manager_id'), 'left')\\\n",
    "           .select(col('mgr.employee_name'), col('emp.employee_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5dd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c358dda",
   "metadata": {},
   "source": [
    " 43\n",
    "\n",
    "You are working as a Data Engineer, and the company has a log system where timestamps are recorded for every user action (e.g., when the user logs in and logs out). Your manager wants to know how much time each user spends between log in and log out.\n",
    "\n",
    "calculate the difference between the logout_timestamp and login_timestamp in hours, minutes, and seconds. The result should be formatted like \"HH:mm:ss\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6921de45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|\n",
      "+-------+-------------------+-------------------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, \"2025-01-31 08:00:00\", \"2025-01-31 10:30:45\"),\n",
    "(2, \"2025-01-31 09:00:30\", \"2025-01-31 12:15:10\"),\n",
    "(3, \"2025-01-31 07:45:00\", \"2025-01-31 09:00:15\") ]\n",
    "\n",
    "columns = [\"user_id\", \"login_timestamp\", \"logout_timestamp\"] \n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32ab0aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|\n",
      "+-------+-------------------+-------------------+----------+-----------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|\n",
      "+-------+-------------------+-------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('login_time', unix_timestamp('login_timestamp'))\n",
    "df = df.withColumn('logout_time', unix_timestamp('logout_timestamp'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb3c4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+----------------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|duration_seconds|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|            9045|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|           11680|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|            4515|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate difference \n",
    "\n",
    "df = df.withColumn('duration_seconds', col('logout_time')-col('login_time'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b44e132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|duration_seconds|hours|minutes|seconds|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|            9045|    2|     30|     45|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|           11680|    3|     14|     40|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|            4515|    1|     15|     15|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate hours, minutes, and seconds\n",
    "\n",
    "df = df.withColumn('hours', (col('duration_seconds') / 3600).cast('int'))\n",
    "df = df.withColumn('minutes', ((col('duration_seconds') % 3600) / 60).cast('int'))\n",
    "df = df.withColumn('seconds', (col('duration_seconds') % 60).cast('int'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "520e24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+------------------+\n",
      "|user_id|    login_timestamp|   logout_timestamp|login_time|logout_time|duration_seconds|hours|minutes|seconds|formatted_duration|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+------------------+\n",
      "|      1|2025-01-31 08:00:00|2025-01-31 10:30:45|1738290600| 1738299645|            9045|    2|     30|     45|          02:30:45|\n",
      "|      2|2025-01-31 09:00:30|2025-01-31 12:15:10|1738294230| 1738305910|           11680|    3|     14|     40|          03:14:40|\n",
      "|      3|2025-01-31 07:45:00|2025-01-31 09:00:15|1738289700| 1738294215|            4515|    1|     15|     15|          01:15:15|\n",
      "+-------+-------------------+-------------------+----------+-----------+----------------+-----+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('formatted_duration', expr(\"lpad(hours, 2,'0') ||':' || lpad(minutes, 2,'0') ||':' || lpad(seconds, 2,'0')\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5131354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|user_id|formatted_duration|\n",
      "+-------+------------------+\n",
      "|1      |02:30:45          |\n",
      "|2      |03:14:40          |\n",
      "|3      |01:15:15          |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('user_id', 'formatted_duration').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5a20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93371807",
   "metadata": {},
   "source": [
    " 44 \n",
    "\n",
    "You have a dataset of user activities in an e-commerce application, where each row represents an activity performed by a user. The dataset contains duplicate activity entries (based on user and activity type) and you need to remove the duplicates. Furthermore, you want to keep only the most recent record for each user, based on a timestamp column.\n",
    "\n",
    "Problem\n",
    "- Remove duplicates based on user_id and activity_type.\n",
    "- Keep only the most recent activity_timestamp for each user and activity type combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af70c991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+\n",
      "|user_id|activity_type| activity_timestamp|\n",
      "+-------+-------------+-------------------+\n",
      "|      1|        login|2025-02-01 10:00:00|\n",
      "|      1| view_product|2025-02-01 10:05:00|\n",
      "|      1|        login|2025-02-01 10:30:00|\n",
      "|      2|     purchase|2025-02-01 11:00:00|\n",
      "|      2|        login|2025-02-01 11:15:00|\n",
      "|      2| view_product|2025-02-01 11:30:00|\n",
      "|      3|        login|2025-02-01 12:00:00|\n",
      "|      3|        login|2025-02-01 12:05:00|\n",
      "+-------+-------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- activity_type: string (nullable = true)\n",
      " |-- activity_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, 'login', '2025-02-01 10:00:00'), (1, 'view_product', '2025-02-01 10:05:00'), \n",
    "        (1, 'login', '2025-02-01 10:30:00'), (2, 'purchase', '2025-02-01 11:00:00'), (2, 'login', '2025-02-01 11:15:00'), \n",
    "(2, 'view_product', '2025-02-01 11:30:00'), (3, 'login', '2025-02-01 12:00:00'), (3, 'login', '2025-02-01 12:05:00') ]\n",
    " \n",
    "df = spark.createDataFrame(data, [\"user_id\", \"activity_type\", \"activity_timestamp\"])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8755fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+\n",
      "|user_id|activity_type| activity_timestamp|\n",
      "+-------+-------------+-------------------+\n",
      "|      1|        login|2025-02-01 10:00:00|\n",
      "|      1| view_product|2025-02-01 10:05:00|\n",
      "|      1|        login|2025-02-01 10:30:00|\n",
      "|      2|     purchase|2025-02-01 11:00:00|\n",
      "|      2|        login|2025-02-01 11:15:00|\n",
      "|      2| view_product|2025-02-01 11:30:00|\n",
      "|      3|        login|2025-02-01 12:00:00|\n",
      "|      3|        login|2025-02-01 12:05:00|\n",
      "+-------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('activity_timestamp', col('activity_timestamp').cast('timestamp'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba31b675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+-------+\n",
      "|user_id|activity_type| activity_timestamp|row_num|\n",
      "+-------+-------------+-------------------+-------+\n",
      "|      1|        login|2025-02-01 10:30:00|      1|\n",
      "|      1|        login|2025-02-01 10:00:00|      2|\n",
      "|      1| view_product|2025-02-01 10:05:00|      1|\n",
      "|      2|        login|2025-02-01 11:15:00|      1|\n",
      "|      2|     purchase|2025-02-01 11:00:00|      1|\n",
      "|      2| view_product|2025-02-01 11:30:00|      1|\n",
      "|      3|        login|2025-02-01 12:05:00|      1|\n",
      "|      3|        login|2025-02-01 12:00:00|      2|\n",
      "+-------+-------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('user_id', 'activity_type').orderBy(col('activity_timestamp').desc())\n",
    "# Add a row number to each partition\n",
    "df_with_row_num = df.withColumn('row_num', row_number().over(window_spec))\n",
    "df_with_row_num.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58afa248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------+\n",
      "|user_id|activity_type| activity_timestamp|\n",
      "+-------+-------------+-------------------+\n",
      "|      1|        login|2025-02-01 10:30:00|\n",
      "|      1| view_product|2025-02-01 10:05:00|\n",
      "|      2|        login|2025-02-01 11:15:00|\n",
      "|      2|     purchase|2025-02-01 11:00:00|\n",
      "|      2| view_product|2025-02-01 11:30:00|\n",
      "|      3|        login|2025-02-01 12:05:00|\n",
      "+-------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df_with_row_num.filter(col('row_num') == 1).drop('row_num')\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d436d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c602764",
   "metadata": {},
   "source": [
    " 46\n",
    "\n",
    "You have been given a large dataset containing employee salary details. Your goal is to optimize a PySpark job that performs a groupBy operation while minimizing the shuffle.\n",
    "\n",
    "Task:\n",
    "Write a PySpark job to calculate the total salary per department.\n",
    "Optimize the job to reduce shuffle while performing the groupBy operation.\n",
    "Explain why your optimization reduces shuffle and improves performance.\n",
    "\n",
    "Approach:\n",
    "To minimize shuffle during a groupBy operation, we should:\n",
    "Use repartition() efficiently to avoid unnecessary partitions.\n",
    "Use reduceByKey() instead of groupByKey(), as it performs local aggregation before shuffling.\n",
    "If working with a DataFrame, use partitionBy() while writing output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034825fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+------+\n",
      "|emp_id|  name| dept|salary|\n",
      "+------+------+-----+------+\n",
      "|   101| Rahul|   IT| 90000|\n",
      "|   102|  Sita|   HR| 75000|\n",
      "|   103|Vikram|   IT| 85000|\n",
      "|   104| Priya|   HR| 72000|\n",
      "|   105|Anjali|   IT| 88000|\n",
      "|   106|Manish|Sales| 67000|\n",
      "|   107|  Neha|Sales| 70000|\n",
      "+------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (101, \"Rahul\", \"IT\", 90000), (102, \"Sita\", \"HR\", 75000), \n",
    "(103, \"Vikram\", \"IT\", 85000), (104, \"Priya\", \"HR\", 72000), \n",
    "(105, \"Anjali\", \"IT\", 88000), (106, \"Manish\", \"Sales\", 67000), \n",
    "(107, \"Neha\", \"Sales\", 70000) ] \n",
    "\n",
    "columns = [\"emp_id\", \"name\", \"dept\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a06bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IT', 90000),\n",
       " ('HR', 75000),\n",
       " ('IT', 85000),\n",
       " ('HR', 72000),\n",
       " ('IT', 88000),\n",
       " ('Sales', 67000),\n",
       " ('Sales', 70000)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.groupBy('dept').sum('salary').show()\n",
    "\n",
    "#step 1: Convert to RDD and Use reduceByKey (optimized Shuffle)\n",
    "rdd = df.rdd.map(lambda x:(x[2], x[3]))   # (dept, salary)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72bf1315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HR', 147000), ('IT', 263000), ('Sales', 137000)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_result = rdd.reduceByKey(lambda x, y : x+y)  # Aggregation before shuffle\n",
    "optimized_result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335872ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "| dept|total_salary|\n",
      "+-----+------------+\n",
      "|   HR|      147000|\n",
      "|   IT|      263000|\n",
      "|Sales|      137000|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert back to dataframe\n",
    "\n",
    "optimized_df = optimized_result.toDF(['dept', 'total_salary'])\n",
    "optimized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9807cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "| dept|total_salary|\n",
      "+-----+------------+\n",
      "|   IT|      263000|\n",
      "|   HR|      147000|\n",
      "|Sales|      137000|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2:\n",
    "df_optimized =  df.repartition('dept').groupBy('dept').agg(sum('salary').alias('total_salary'))\n",
    "df_optimized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6b6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11a79bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|         1|      Laptop|\n",
      "|         2|      Tablet|\n",
      "|         3|  Smartphone|\n",
      "|         4|     Monitor|\n",
      "|         5|    Keyboard|\n",
      "+----------+------------+\n",
      "\n",
      "+-------+----------+----------+\n",
      "|sale_id|product_id| sale_date|\n",
      "+-------+----------+----------+\n",
      "|    101|         1|2025-01-01|\n",
      "|    102|         3|2025-01-02|\n",
      "|    103|         5|2025-01-03|\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write a PySpark program to identify products that have never been sold.\n",
    "\n",
    "products = spark.createDataFrame([ (1, \"Laptop\"), (2, \"Tablet\"), \n",
    "(3, \"Smartphone\"), (4, \"Monitor\"), \n",
    "(5, \"Keyboard\") ], [\"product_id\", \"product_name\"]) \n",
    "\n",
    "products.show()\n",
    "\n",
    "sales = spark.createDataFrame([ (101, 1, \"2025-01-01\"), (102, 3, \"2025-01-02\"), (103, 5, \"2025-01-03\") ], [\"sale_id\", \"product_id\", \"sale_date\"]) \n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4253915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         2|\n",
      "|         4|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.join(sales, products.product_id == sales.product_id, how='left').filter(sales.product_id.isNull()).select(products.product_id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178a194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ca84ed",
   "metadata": {},
   "source": [
    "- Replace missing values in the price column with the median price instead of the mean.\n",
    "- Drop rows where the product column is null, but if the price column is above 300, replace the null product with \"Unknown\".\n",
    "- Fill missing values in the quantity column with the average quantity rounded to the nearest integer.\n",
    "- Add a new column, total_value, which is the product of price and quantity.\n",
    "- Remove the product_id column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8ae5556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse| NULL|    NULL|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|    NULL|\n",
      "|         5|    NULL|  500|    NULL|\n",
      "+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (1, \"Laptop\", 1000, 5),\n",
    " (2, \"Mouse\", None, None),\n",
    " (3, \"Keyboard\", 50, 2),\n",
    " (4, \"Monitor\", 200, None),\n",
    " (5, None, 500, None),\n",
    "]\n",
    "columns = [\"product_id\", \"product\", \"price\", \"quantity\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ebb93f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse|  200|    NULL|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|    NULL|\n",
      "|         5|    NULL|  500|    NULL|\n",
      "+----------+--------+-----+--------+\n",
      "\n",
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse|  200|    NULL|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|    NULL|\n",
      "|         5|    NULL|  500|    NULL|\n",
      "+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace missing values in the price column with the median price instead of the mean.\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(inputCols = ['price'], outputCols = ['price']).setStrategy('median')\n",
    "df =imputer.fit(df).transform(df)\n",
    "df.show()\n",
    "\n",
    "# OR \n",
    "median_price = df.approxQuantile(\"price\", [0.5], 0.01)[0]\n",
    "df = df.fillna({\"price\": median_price})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1494ad49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse|  200|    NULL|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|    NULL|\n",
      "|         5| unknown|  500|    NULL|\n",
      "+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where the product column is null, but if the price column is above 300, replace the null product with \"Unknown\". \n",
    "\n",
    "df1 = df.withColumn('product', when((col('product').isNull()) & (col('price') > 300), 'unknown').otherwise(col('product'))).select('*')\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "60b191d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse|  200|    NULL|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|    NULL|\n",
      "|         5| unknown|  500|    NULL|\n",
      "+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.filter(col('product').isNotNull())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the quantity column with the average quantity rounded to the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4b6a4b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_quantity = df2.select(round(avg(col('quantity'))).alias('quantity')).collect()[0]['quantity']\n",
    "avg_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6f0768f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+\n",
      "|product_id| product|price|quantity|\n",
      "+----------+--------+-----+--------+\n",
      "|         1|  Laptop| 1000|       5|\n",
      "|         2|   Mouse|  200|       4|\n",
      "|         3|Keyboard|   50|       2|\n",
      "|         4| Monitor|  200|       4|\n",
      "|         5| unknown|  500|       4|\n",
      "+----------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.fillna({'quantity': avg_quantity})\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "807bf856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+--------+-----------+\n",
      "|product_id| product|price|quantity|total_value|\n",
      "+----------+--------+-----+--------+-----------+\n",
      "|         1|  Laptop| 1000|       5|       5000|\n",
      "|         2|   Mouse|  200|       4|        800|\n",
      "|         3|Keyboard|   50|       2|        100|\n",
      "|         4| Monitor|  200|       4|        800|\n",
      "|         5| unknown|  500|       4|       2000|\n",
      "+----------+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add a new column, total_value, which is the product of price and quantity.\n",
    "\n",
    "df4 = df3.withColumn('total_value', col('price') * col('quantity')).select('*')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d861192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+-----------+\n",
      "| product|price|quantity|total_value|\n",
      "+--------+-----+--------+-----------+\n",
      "|  Laptop| 1000|       5|       5000|\n",
      "|   Mouse|  200|       4|        800|\n",
      "|Keyboard|   50|       2|        100|\n",
      "| Monitor|  200|       4|        800|\n",
      "| unknown|  500|       4|       2000|\n",
      "+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove the product_id column from the DataFrame.\n",
    "df4.drop('product_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed364ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efd665fe",
   "metadata": {},
   "source": [
    "47\n",
    "\n",
    "- Define an explicit schema for this dataset using StructType and StructField.\n",
    "- Load this data into a PySpark DataFrame using the defined schema.\n",
    "- Extract the employees who belong to the \"IT\" department and have a salary greater than 70000.\n",
    "- Split the Address column into two separate columns: City and State.\n",
    "- Save the transformed data into a Parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70464113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+--------------------+\n",
      "|Emp_ID|  Name|Age|Salary|Department|             Address|\n",
      "+------+------+---+------+----------+--------------------+\n",
      "|   101|Rajesh| 30| 60000|        IT| Mumbai, Maharashtra|\n",
      "|   102| Priya| 28| 75000|        HR|Bengaluru, Karnataka|\n",
      "|   103|Suresh| 35| 50000|   Finance| Chennai, Tamil Nadu|\n",
      "|   104|Anjali| 25| 80000|        IT|   Pune, Maharashtra|\n",
      "|   105| Arjun| 40| 90000|Management|Hyderabad, Telangana|\n",
      "+------+------+---+------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ StructField(\"Emp_ID\", IntegerType(), True), \n",
    "                     StructField(\"Name\", StringType(), True), \n",
    "                     StructField(\"Age\", IntegerType(), True), \n",
    "                     StructField(\"Salary\", IntegerType(), True), \n",
    "                     StructField(\"Department\", StringType(), True),\n",
    "                     StructField(\"Address\", StringType(), True) ]) \n",
    "\n",
    "data = [ (101, \"Rajesh\", 30, 60000, \"IT\", \"Mumbai, Maharashtra\"), \n",
    "         (102, \"Priya\", 28, 75000, \"HR\", \"Bengaluru, Karnataka\"), \n",
    "         (103, \"Suresh\", 35, 50000, \"Finance\", \"Chennai, Tamil Nadu\"), \n",
    "         (104, \"Anjali\", 25, 80000, \"IT\", \"Pune, Maharashtra\"), \n",
    "         (105, \"Arjun\", 40, 90000, \"Management\", \"Hyderabad, Telangana\") ]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08a921f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+-----------------+\n",
      "|Emp_ID|  Name|Age|Salary|Department|          Address|\n",
      "+------+------+---+------+----------+-----------------+\n",
      "|   104|Anjali| 25| 80000|        IT|Pune, Maharashtra|\n",
      "+------+------+---+------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the employees who belong to the \"IT\" department and have a salary greater than 70000.\n",
    "\n",
    "df.filter((col('Department') == 'IT') & (col('Salary') > 70000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e35b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+------+----------+--------------------+---------+-----------+\n",
      "|Emp_ID|  Name|Age|Salary|Department|             Address|     City|      State|\n",
      "+------+------+---+------+----------+--------------------+---------+-----------+\n",
      "|   101|Rajesh| 30| 60000|        IT| Mumbai, Maharashtra|   Mumbai|Maharashtra|\n",
      "|   102| Priya| 28| 75000|        HR|Bengaluru, Karnataka|Bengaluru|  Karnataka|\n",
      "|   103|Suresh| 35| 50000|   Finance| Chennai, Tamil Nadu|  Chennai| Tamil Nadu|\n",
      "|   104|Anjali| 25| 80000|        IT|   Pune, Maharashtra|     Pune|Maharashtra|\n",
      "|   105| Arjun| 40| 90000|Management|Hyderabad, Telangana|Hyderabad|  Telangana|\n",
      "+------+------+---+------+----------+--------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the Address column into two separate columns: City and State.\n",
    "trn_df = df.withColumn('City', split(col('Address'), ', ').getItem(0)).withColumn('State', split(col('Address'), ', ').getItem(1))\n",
    "# OR \n",
    "#trn_df = df.withColumn('City', split(col('Address'), ', ')[0]).withColumn('State', split(col('Address'), ', ')[1])\n",
    "\n",
    "trn_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6963d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed data into a Parquet file.\n",
    "\n",
    "trn_df.write.mode('overwrite').parquet('trn_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31209af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f47605d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4865a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
