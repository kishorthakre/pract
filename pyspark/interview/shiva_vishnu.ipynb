{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989f7a5d",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/in/sivavishnu1/recent-activity/all/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d3f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46579161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96a9eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PartitionManagement</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2578ed2f850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize spark session\n",
    "spark = SparkSession.builder.appName(\"PartitionManagement\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb05cc7",
   "metadata": {},
   "source": [
    "ğˆğ¦ğšğ ğ¢ğ§ğ ğ²ğ¨ğ® ğ¡ğšğ¯ğ ğš ğ¥ğšğ«ğ ğ ğğšğ­ğšğ¬ğğ­ ğ¥ğ¨ğšğğğ ğ¢ğ§ğ­ğ¨ ğ’ğ©ğšğ«ğ¤ ğšğ§ğ ğšğŸğ­ğğ« ğ©ğğ«ğŸğ¨ğ«ğ¦ğ¢ğ§ğ  ğ¬ğ¨ğ¦ğ ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğšğ­ğ¢ğ¨ğ§ğ¬, ğ­ğ¡ğ ğğšğ­ğš ğ¢ğ¬ ğ«ğğ©ğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ğğ ğ¢ğ§ğ­ğ¨ ğŸğŸ ğ©ğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ğ¬. ğ˜ğ¨ğ® ğ§ğ¨ğ° ğ°ğšğ§ğ­ ğ­ğ¨ ğ©ğ«ğ¢ğ§ğ­ ğ­ğ¡ğ ğ©ğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ ğˆğƒğ¬ ğšğ¥ğ¨ğ§ğ  ğ°ğ¢ğ­ğ¡ ğ­ğ¡ğğ¢ğ« ğœğ¨ğ«ğ«ğğ¬ğ©ğ¨ğ§ğğ¢ğ§ğ  ğğšğ­ğš. ğ€ğğğ¢ğ­ğ¢ğ¨ğ§ğšğ¥ğ¥ğ², ğ²ğ¨ğ® ğ§ğğğ ğ­ğ¨ ğ«ğğ¦ğ¨ğ¯ğ ğ­ğ¡ğ ğğšğ­ğš ğŸğ«ğ¨ğ¦ ğ©ğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ğ¬ ğŸğŸ” ğ­ğ¨ ğŸğŸ.\n",
    "ğˆğ¬ ğ­ğ¡ğ¢ğ¬ ğ©ğ¨ğ¬ğ¬ğ¢ğ›ğ¥ğ ğ¢ğ§ ğ’ğ©ğšğ«ğ¤? ğˆğŸ ğ¬ğ¨, ğ¡ğ¨ğ° ğ°ğ¨ğ®ğ¥ğ ğ²ğ¨ğ® ğ©ğ«ğ¢ğ§ğ­ ğ­ğ¡ğ ğ©ğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ ğˆğƒğ¬ ğšğ§ğ ğ«ğğ¦ğ¨ğ¯ğ ğ­ğ¡ğ ğğšğ­ğš ğŸğ«ğ¨ğ¦ ğ©ğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ğ¬ ğŸğŸ” ğ­ğ¨ ğŸğŸ ğ®ğ¬ğ¢ğ§ğ  ğğ²ğ’ğ©ğšğ«ğ¤? ğğ¥ğğšğ¬ğ ğğ±ğ©ğ¥ğšğ¢ğ§ ğ­ğ¡ğ ğ©ğ«ğ¨ğœğğ¬ğ¬ ğšğ§ğ ğ©ğ«ğ¨ğ¯ğ¢ğğ ğš ğ¬ğšğ¦ğ©ğ¥ğ ğœğ¨ğğ ğ­ğ¨ ğšğœğ¡ğ¢ğğ¯ğ ğ­ğ¡ğ¢ğ¬.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa7bf5",
   "metadata": {},
   "source": [
    "# ğ’ğ­ğğ©ğ¬ ğŸğ¨ğ« ğğ«ğ¢ğ§ğ­ğ¢ğ§ğ  ğğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ ğˆğƒğ¬ ğšğ§ğ ğƒğğ¥ğğ­ğ¢ğ§ğ  ğƒğšğ­ğš ğ¢ğ§ ğğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ğ¬ ğŸğŸ” ğ­ğ¨ ğŸğŸ:\n",
    "\n",
    "\"\"\"1. View Partition IDs:\n",
    "In order to view the partition IDs and the data within each partition, we can use the glom() method that groups the data within each partition into a list.\n",
    "\n",
    "2. Deleting Data from Partitions 16 to 20:\n",
    "Spark doesn't provide direct APIs for deleting specific partitions. However, we can manipulate the data by filtering out the data from the desired partitions (in this case, partitions 16-20). Here's the step-by-step approach:\n",
    "Read and View Data Partitions: First, read the data into Spark and check how it is partitioned.\n",
    "\n",
    "Use mapPartitionsWithIndex: This is a transformation that allows you to access the index of each partition and the corresponding data.\n",
    "Filter out Partitions 16 to 20: Based on the partition index, we can filter out the unwanted partitions.\n",
    "\n",
    "Repartition: If necessary, repartition the data after filtering out the partitions you don't need.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec4496e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "|  5|    Eve| 22|\n",
      "|  6|  Frank| 28|\n",
      "|  7|  Grace| 32|\n",
      "|  8| Hannah| 26|\n",
      "|  9|    Ivy| 24|\n",
      "| 10|   Jack| 27|\n",
      "| 11|   Kara| 29|\n",
      "| 12|   Lian| 31|\n",
      "| 13|    Mia| 33|\n",
      "| 14|   Nina| 34|\n",
      "| 15| Oliver| 36|\n",
      "| 16|   Paul| 37|\n",
      "| 17|  Quinn| 38|\n",
      "| 18|   Rita| 39|\n",
      "| 19|  Steve| 41|\n",
      "| 20|    Tom| 42|\n",
      "+---+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Alice', 30),\n",
    "       (2, 'Bob', 25),\n",
    "       (3, 'Charlie', 35),\n",
    "       (4, 'David', 40),\n",
    "       (5, 'Eve', 22),\n",
    "       (6, 'Frank', 28),\n",
    "       (7, 'Grace', 32),\n",
    "       (8, 'Hannah', 26),\n",
    "       (9, 'Ivy', 24),\n",
    "       (10, 'Jack', 27),\n",
    "       (11, 'Kara', 29),\n",
    "       (12, 'Lian', 31),\n",
    "       (13, 'Mia', 33),\n",
    "       (14, 'Nina', 34),\n",
    "       (15, 'Oliver', 36),\n",
    "       (16, 'Paul', 37),\n",
    "       (17, 'Quinn', 38),\n",
    "       (18, 'Rita', 39),\n",
    "       (19, 'Steve', 41),\n",
    "       (20, 'Tom', 42),\n",
    "       (21, 'Nina', 34),\n",
    "       (22, 'Oliver', 36),\n",
    "       (23, 'Paul', 37),\n",
    "       (24, 'Quinn', 38),\n",
    "       (25, 'Rita', 39),\n",
    "       (26, 'Steve', 41),\n",
    "       (27, 'Tom', 42),\n",
    "       (28, 'Tom', 42),\n",
    "       (29, 'Nina', 34),\n",
    "       (30, 'Oliver', 36),\n",
    "       (31, 'Paul', 37),\n",
    "       (32, 'Quinn', 38),\n",
    "       (33, 'Rita', 39),\n",
    "       (34, 'Steve', 41),\n",
    "       (35, 'Tom', 42)]\n",
    "# Add more data to simulate large dataset\n",
    "\n",
    "# Columns for the DataFrame\n",
    "columns = ['id', 'name', 'age']\n",
    "\n",
    "# Create DataFrame from the sample data \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cad1787f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, age: bigint]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition DataFrame into 20 partitions\n",
    "df_repartitioned = df.repartition(20)\n",
    "df_repartitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65e50c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the number of partitions in the DataFrame\n",
    "num_partitions = df_repartitioned.rdd.getNumPartitions()\n",
    "num_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac142daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [Row(id=30, name='Oliver', age=36)],\n",
       " [Row(id=34, name='Steve', age=41)],\n",
       " [Row(id=29, name='Nina', age=34)],\n",
       " [Row(id=8, name='Hannah', age=26), Row(id=33, name='Rita', age=39)],\n",
       " [Row(id=6, name='Frank', age=28), Row(id=35, name='Tom', age=42)],\n",
       " [Row(id=7, name='Grace', age=32),\n",
       "  Row(id=21, name='Nina', age=34),\n",
       "  Row(id=31, name='Paul', age=37)],\n",
       " [Row(id=5, name='Eve', age=22),\n",
       "  Row(id=23, name='Paul', age=37),\n",
       "  Row(id=32, name='Quinn', age=38)],\n",
       " [Row(id=14, name='Nina', age=34), Row(id=24, name='Quinn', age=38)],\n",
       " [Row(id=2, name='Bob', age=25),\n",
       "  Row(id=11, name='Kara', age=29),\n",
       "  Row(id=13, name='Mia', age=33),\n",
       "  Row(id=22, name='Oliver', age=36)],\n",
       " [Row(id=3, name='Charlie', age=35),\n",
       "  Row(id=9, name='Ivy', age=24),\n",
       "  Row(id=15, name='Oliver', age=36)],\n",
       " [Row(id=1, name='Alice', age=30),\n",
       "  Row(id=12, name='Lian', age=31),\n",
       "  Row(id=16, name='Paul', age=37),\n",
       "  Row(id=27, name='Tom', age=42)],\n",
       " [Row(id=4, name='David', age=40),\n",
       "  Row(id=10, name='Jack', age=27),\n",
       "  Row(id=26, name='Steve', age=41)],\n",
       " [Row(id=28, name='Tom', age=42)],\n",
       " [Row(id=19, name='Steve', age=41), Row(id=25, name='Rita', age=39)],\n",
       " [Row(id=17, name='Quinn', age=38)],\n",
       " [Row(id=18, name='Rita', age=39)],\n",
       " [Row(id=20, name='Tom', age=42)],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the data in each partition using glom()\n",
    "# glom() groups the data in each patition as a list\n",
    "\n",
    "partitions_data = df_repartitioned.rdd.glom().collect()\n",
    "partitions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db544404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition ID: 0, Data: []\n",
      "Partition ID: 1, Data: [Row(id=30, name='Oliver', age=36)]\n",
      "Partition ID: 2, Data: [Row(id=34, name='Steve', age=41)]\n",
      "Partition ID: 3, Data: [Row(id=29, name='Nina', age=34)]\n",
      "Partition ID: 4, Data: [Row(id=8, name='Hannah', age=26), Row(id=33, name='Rita', age=39)]\n",
      "Partition ID: 5, Data: [Row(id=6, name='Frank', age=28), Row(id=35, name='Tom', age=42)]\n",
      "Partition ID: 6, Data: [Row(id=7, name='Grace', age=32), Row(id=21, name='Nina', age=34), Row(id=31, name='Paul', age=37)]\n",
      "Partition ID: 7, Data: [Row(id=5, name='Eve', age=22), Row(id=23, name='Paul', age=37), Row(id=32, name='Quinn', age=38)]\n",
      "Partition ID: 8, Data: [Row(id=14, name='Nina', age=34), Row(id=24, name='Quinn', age=38)]\n",
      "Partition ID: 9, Data: [Row(id=2, name='Bob', age=25), Row(id=11, name='Kara', age=29), Row(id=13, name='Mia', age=33), Row(id=22, name='Oliver', age=36)]\n",
      "Partition ID: 10, Data: [Row(id=3, name='Charlie', age=35), Row(id=9, name='Ivy', age=24), Row(id=15, name='Oliver', age=36)]\n",
      "Partition ID: 11, Data: [Row(id=1, name='Alice', age=30), Row(id=12, name='Lian', age=31), Row(id=16, name='Paul', age=37), Row(id=27, name='Tom', age=42)]\n",
      "Partition ID: 12, Data: [Row(id=4, name='David', age=40), Row(id=10, name='Jack', age=27), Row(id=26, name='Steve', age=41)]\n",
      "Partition ID: 13, Data: [Row(id=28, name='Tom', age=42)]\n",
      "Partition ID: 14, Data: [Row(id=19, name='Steve', age=41), Row(id=25, name='Rita', age=39)]\n",
      "Partition ID: 15, Data: [Row(id=17, name='Quinn', age=38)]\n",
      "Partition ID: 16, Data: [Row(id=18, name='Rita', age=39)]\n",
      "Partition ID: 17, Data: [Row(id=20, name='Tom', age=42)]\n",
      "Partition ID: 18, Data: []\n",
      "Partition ID: 19, Data: []\n"
     ]
    }
   ],
   "source": [
    "# Print partition IDs and their corresponding data \n",
    "for idx, partition in enumerate(partitions_data):\n",
    "    print(f\"Partition ID: {idx}, Data: {partition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4861d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's filter out partitions 16 to 20 using mapPartitionsWithIndex\n",
    "#This will allow us to access the partition index and filter out unwanted partitions. \n",
    "\n",
    "def filter_partition_data(index, iterator):\n",
    "    # keep data only for partitions 0 to 15 (excluding partitions 16 to 20)\n",
    "    if index < 16:\n",
    "        yield from iterator\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da528428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter functions to remove data from partitions 10 to 20 \n",
    "\n",
    "filtered_rdd = df_repartitioned.rdd.mapPartitionsWithIndex(filter_partition_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "101878eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "| 30| Oliver| 36|\n",
      "| 34|  Steve| 41|\n",
      "| 29|   Nina| 34|\n",
      "|  8| Hannah| 26|\n",
      "| 33|   Rita| 39|\n",
      "|  6|  Frank| 28|\n",
      "| 35|    Tom| 42|\n",
      "|  7|  Grace| 32|\n",
      "| 21|   Nina| 34|\n",
      "| 31|   Paul| 37|\n",
      "|  5|    Eve| 22|\n",
      "| 23|   Paul| 37|\n",
      "| 32|  Quinn| 38|\n",
      "| 14|   Nina| 34|\n",
      "| 24|  Quinn| 38|\n",
      "|  2|    Bob| 25|\n",
      "| 11|   Kara| 29|\n",
      "| 13|    Mia| 33|\n",
      "| 22| Oliver| 36|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the filtered RDD back to DataFrame \n",
    "filtered_df = spark.createDataFrame(filtered_rdd, columns) \n",
    "filtered_df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0599b3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9550d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "10f0f7cc",
   "metadata": {},
   "source": [
    "ğ‡ğ¨ğ° ğ°ğ¨ğ®ğ¥ğ ğ²ğ¨ğ® ğ¡ğšğ§ğğ¥ğ ğ¦ğ¢ğ¬ğ¬ğ¢ğ§ğ  ğ¨ğ« ğ¢ğ§ğ¯ğšğ¥ğ¢ğ ğğšğ­ğš ğ¢ğ§ ğš ğğšğ­ğš ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ ğ®ğ¬ğ¢ğ§ğ  ğ©ğ²ğ¬ğ©ğšğ«ğ¤?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86f063",
   "metadata": {},
   "source": [
    "ğŠğğ² ğ‡ğšğ§ğğ¥ğ¢ğ§ğ  ğ“ğğœğ¡ğ§ğ¢ğªğ®ğğ¬:\n",
    "\n",
    "1, Dropping Missing Values\n",
    "\n",
    "2, Filling Missing Values\n",
    "\n",
    "3, Replacing Invalid Data\n",
    "\n",
    "4, Handling Invalid Data Types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a944c",
   "metadata": {},
   "source": [
    "Handling null values is a crucial part of data preprocessing in PySpark. Null values can arise due to missing data or errors during data collection, and they can impact the quality of your analysis and processing. PySpark provides several functions to manage null values, such as dropna() to remove rows with nulls, fillna() to replace null values with specified defaults, and filter() to select rows based on the presence of nulls. You can also use conditional expressions like when() to replace null values with different values depending on certain conditions. Proper handling of null values ensures better data integrity and more accurate results in your Spark processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34b956d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+\n",
      "| id|   name| age|salary|\n",
      "+---+-------+----+------+\n",
      "|  1|  Alice|  25|  NULL|\n",
      "|  2|    Bob|NULL|  4000|\n",
      "|  3|Charlie|  30|  4500|\n",
      "|  4|   NULL|NULL|  5000|\n",
      "|  5|    Eve|  28| -1000|\n",
      "+---+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Alice', 25, None), (2, 'Bob', None, 4000), (3, 'Charlie', 30, 4500), (4, None, None, 5000), (5, 'Eve', 28, -1000)] \n",
    "columns = ['id', 'name', 'age', 'salary'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41d76bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows with missing or null values in any column\n",
    "\n",
    "df_dropped = df.dropna() \n",
    "df_dropped.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb46e5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  3000|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|Unknown|  0|  5000|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values (null) with a specified value\n",
    "\n",
    "df_filled = df.fillna({'age':0, 'salary':3000, 'name':'Unknown'})\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d8a1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  3000|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|Unknown|  0|  5000|\n",
      "|  5|    Eve| 28|     0|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replacing invalid data (e.g., salary <0) with a valid value (e.g., 0)\n",
    "\n",
    "df_invallid_replaced = df_filled.withColumn('salary', when(col('salary') <0, 0).otherwise(col('salary')))\n",
    "df_invallid_replaced.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67acd57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  NULL|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|   NULL|  0|  5000|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling invalid data types (e.g., if age is not an integer) \n",
    "# For this, we would attempt to cast age to an integer and filter out rows with invalid age data\n",
    "\n",
    "df_invalid_age_handled = df.withColumn('age', when(col('age').cast('int').isNotNull(), col('age')).otherwise(0))\n",
    "df_invalid_age_handled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d34f104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  NULL|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|   NULL|  0|  5000|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling missing values in a specific column (e.g., age), without affecting other columns\n",
    "\n",
    "df_age_filled = df.fillna({'age':0})\n",
    "df_age_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d07a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|age|salary|\n",
      "+---+----+---+------+\n",
      "|  0|   1|  2|     1|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking how many nulls are present in each column before and after cleaning\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4192dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|age|salary|\n",
      "+---+----+---+------+\n",
      "|  0|   0|  0|     0|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count of Null values after handling.\n",
    "\n",
    "df_invallid_replaced.select([count(when(col(c).isNull(), c)).alias(c) for c in df_invallid_replaced.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e70bb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee5d1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b0431ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+----------+\n",
      "|  Name|Subject|Mark|Attendance|\n",
      "+------+-------+----+----------+\n",
      "|  John|   Math|  75|   Present|\n",
      "|Martin|Science|NULL|    Absent|\n",
      "| Sarah|History|  85|      NULL|\n",
      "|  Alex|   Math|NULL|   Present|\n",
      "|   Eve|   Math|  90|    Absent|\n",
      "+------+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", 'Math', 75, \"Present\"),\n",
    "        ('Martin', 'Science', None, \"Absent\"),\n",
    "        ('Sarah', 'History', 85, None),\n",
    "        ('Alex', \"Math\", None, \"Present\"),\n",
    "        ('Eve', 'Math', 90, 'Absent')] \n",
    "columns = ['Name', 'Subject', 'Mark', 'Attendance'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5b6b77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+----------+\n",
      "| Name|Subject|Mark|Attendance|\n",
      "+-----+-------+----+----------+\n",
      "| John|   Math|  75|   Present|\n",
      "|Sarah|History|  85|      NULL|\n",
      "|  Eve|   Math|  90|    Absent|\n",
      "+-----+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter using isNotNull\n",
    "df_not_null = df.filter(col('Mark').isNotNull())\n",
    "df_not_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ffd7fd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+----------+\n",
      "|  Name|Subject|Mark|Attendance|\n",
      "+------+-------+----+----------+\n",
      "|  John|   Math|  75|   Present|\n",
      "|Martin|Science|NULL|    Absent|\n",
      "| Sarah|History|  85|      NULL|\n",
      "|  Alex|   Math|NULL|   Present|\n",
      "|   Eve|   Math|  90|    Absent|\n",
      "+------+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter using 'OR'\n",
    "df_not_null_either = df.filter(col('Mark').isNotNull() | col('Attendance').isNotNull())\n",
    "df_not_null_either.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2d1d8e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+----------+\n",
      "| Name|Subject|Mark|Attendance|\n",
      "+-----+-------+----+----------+\n",
      "| John|   Math|  75|   Present|\n",
      "|Sarah|History|  85|      NULL|\n",
      "|  Eve|   Math|  90|    Absent|\n",
      "+-----+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_drp = df.dropna(subset= ['Mark'])\n",
    "df_drp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a578ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae31aed",
   "metadata": {},
   "source": [
    "# ğ‘ğğ©ğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ ğ¯ğ¬. ğ‚ğ¨ğšğ¥ğğ¬ğœğ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "187dc6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  Alice|  1|\n",
      "|    Bob|  2|\n",
      "|Charlie|  3|\n",
      "|  David|  4|\n",
      "|    Eva|  5|\n",
      "|  Frank|  6|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Alice', 1), ('Bob',2), ('Charlie', 3), ('David', 4), ('Eva', 5), ('Frank', 6)] \n",
    "columns = ['name', 'id'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "416f25ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions before repartitioning: 8\n"
     ]
    }
   ],
   "source": [
    "print('Number of partitions before repartitioning:', df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50a9ecd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, id: bigint]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartitioning the dataframe into 4 partitions for parallelism \n",
    "df_repartitioned = df.repartition(4)\n",
    "df_repartitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1de36573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repartitioned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc27eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  David|  4|\n",
      "|    Eva|  5|\n",
      "|  Frank|  6|\n",
      "|Charlie|  3|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a transformation (filtering) to check data\n",
    "df_filtered = df_repartitioned.filter(col('id') > 2)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "31448169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  David|  4|\n",
      "|    Eva|  5|\n",
      "|Charlie|  3|\n",
      "|  Frank|  6|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Coalescing the partitions after filtering (reducing to 2 partitions for optimization)\n",
    "df_coaleasced = df_filtered.coalesce(2)\n",
    "print(df_coaleasced.rdd.getNumPartitions())\n",
    "df_coaleasced.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5f4e4213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   name| department|\n",
      "+-------+-----------+\n",
      "|  Alice|         HR|\n",
      "|    Bob|    Finance|\n",
      "|Charlie|Engineering|\n",
      "|  David|         HR|\n",
      "|    Eva|Engineering|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a join with another Dataframe to demonstrate partitioning in action\n",
    "\n",
    "data2 = [('Alice', 'HR'), (\"Bob\", \"Finance\"), (\"Charlie\",\"Engineering\"), ('David', 'HR'), (\"Eva\", \"Engineering\")]\n",
    "column2 = ['name', 'department']\n",
    "df2 = spark.createDataFrame(data2, column2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b3813dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, department: string]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition the second dataframe by 'name' to align partitioning for a more efficient join\n",
    "\n",
    "df2_partitioned = df2.repartition('name')\n",
    "df2_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57def181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------+\n",
      "|   name| id| department|\n",
      "+-------+---+-----------+\n",
      "|  Alice|  1|         HR|\n",
      "|    Bob|  2|    Finance|\n",
      "|Charlie|  3|Engineering|\n",
      "|  David|  4|         HR|\n",
      "|    Eva|  5|Engineering|\n",
      "+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined = df_repartitioned.join(df2_partitioned, on='name', how='inner')\n",
    "df_joined.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f6205",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb38f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "|  5|    Eve| 22|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Alice', 30), (2, 'Bob', 25), (3, 'Charlie', 35), (4, 'David', 40), (5, 'Eve', 22)] \n",
    "columns = ['id', 'name', 'age'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1e58d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the checkpoint directory \n",
    "checkpoint_dir = 'file:///temp/spark_checkpoints' # Local file system checkpoint directory\n",
    "spark.sparkContext.setCheckpointDir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99327f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------------+\n",
      "| id|   name|age|age_plus_ten|\n",
      "+---+-------+---+------------+\n",
      "|  1|  Alice| 30|          40|\n",
      "|  3|Charlie| 35|          45|\n",
      "|  4|  David| 40|          50|\n",
      "+---+-------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform transformations \n",
    "df_trns = df.filter(col('age') > 25).withColumn('age_plus_ten', col('age') + 10)\n",
    "df_trns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "af4009b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint the traansformed dataframe\n",
    "df_trns = df_trns.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "641c1bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------------+\n",
      "| id|   name|age|age_plus_ten|\n",
      "+---+-------+---+------------+\n",
      "|  1|  Alice| 30|          40|\n",
      "|  3|Charlie| 35|          45|\n",
      "|  4|  David| 40|          50|\n",
      "+---+-------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5e8c2c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------------+-------------+\n",
      "| id|   name|age|age_plus_ten|age_times_two|\n",
      "+---+-------+---+------------+-------------+\n",
      "|  1|  Alice| 30|          40|           60|\n",
      "|  3|Charlie| 35|          45|           70|\n",
      "|  4|  David| 40|          50|           80|\n",
      "+---+-------+---+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform another transformation on the checkpointed dataframe\n",
    "df_final = df_trns.withColumn('age_times_two', col('age') *2)\n",
    "df_final.show()\n",
    "\n",
    "# Simulating a failure to see checkpoint recovery\n",
    "# Uncommetn the line below to simulate a failure after checkpointing\n",
    "# raise Exception ('Simulating a job failure after checkpointing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b057a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec3a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_data = [(1, 'Alice', 'alice@example.com'),\n",
    "                 (2, 'Bob', 'bob@example.com'),\n",
    "                 (3, 'Charlie', 'charlie@example.com')]\n",
    "customers_columns = ['customer_id', 'name', 'email']\n",
    "\n",
    "orders_data = [(101, 1, 1001, '2023-01-01'),\n",
    "              (102, 2, 1002, '2023-01-05'),\n",
    "              (103, 3, 1001, '2023-01-10')]\n",
    "orders_columns = ['order_id', 'customer_id', 'product_id', 'order_date']\n",
    "\n",
    "products_data = [(1001, 'Laptop', 'Electronics'),\n",
    "                (1002, 'Shoes', 'Apparel')]\n",
    "products_columns = ['product_id', 'product_name', 'category']\n",
    "\n",
    "payments_data = [ (1, 101, 1200.50, 'Completed'),\n",
    "                (2, 102, 50.00, 'Pending'),\n",
    "                (3, 103, 1200.50, 'Completed')]\n",
    "payments_columns = ['payment_id', 'order_id', 'amount', 'payment_status']\n",
    "\n",
    "\n",
    "df_customers = spark.createDataFrame(customers_data, customers_columns)\n",
    "df_orders = spark.createDataFrame(orders_data, orders_columns)\n",
    "df_products = spark.createDataFrame(products_data, products_columns)\n",
    "df_payments = spark.createDataFrame(payments_data, payments_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437935a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_orders.join(df_customers, on='customer_id', how='inner')\n",
    "df_joined = df_joined.join(df_products, on='product_id', how='inner')\n",
    "df_joined = df_joined.join(df_payments, on = 'order_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a94988d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------+----------+------------+-----------+------+--------------+\n",
      "|   name|              email|order_id|order_date|product_name|   category|amount|payment_status|\n",
      "+-------+-------------------+--------+----------+------------+-----------+------+--------------+\n",
      "|  Alice|  alice@example.com|     101|2023-01-01|      Laptop|Electronics|1200.5|     Completed|\n",
      "|    Bob|    bob@example.com|     102|2023-01-05|       Shoes|    Apparel|  50.0|       Pending|\n",
      "|Charlie|charlie@example.com|     103|2023-01-10|      Laptop|Electronics|1200.5|     Completed|\n",
      "+-------+-------------------+--------+----------+------------+-----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.select(\n",
    "    'name', 'email', 'order_id', 'order_date', 'product_name', 'category', 'amount', 'payment_status').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c62b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
