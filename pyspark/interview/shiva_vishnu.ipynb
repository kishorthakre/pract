{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a0e7648e",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/in/sivavishnu1/recent-activity/all/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21d3f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46579161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b96a9eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PartitionManagement</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1548310b090>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize spark session\n",
    "spark = SparkSession.builder.appName(\"PartitionManagement\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec4496e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "|  5|    Eve| 22|\n",
      "|  6|  Frank| 28|\n",
      "|  7|  Grace| 32|\n",
      "|  8| Hannah| 26|\n",
      "|  9|    Ivy| 24|\n",
      "| 10|   Jack| 27|\n",
      "| 11|   Kara| 29|\n",
      "| 12|   Lian| 31|\n",
      "| 13|    Mia| 33|\n",
      "| 14|   Nina| 34|\n",
      "| 15| Oliver| 36|\n",
      "| 16|   Paul| 37|\n",
      "| 17|  Quinn| 38|\n",
      "| 18|   Rita| 39|\n",
      "| 19|  Steve| 41|\n",
      "| 20|    Tom| 42|\n",
      "+---+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Alice', 30),\n",
    "       (2, 'Bob', 25),\n",
    "       (3, 'Charlie', 35),\n",
    "       (4, 'David', 40),\n",
    "       (5, 'Eve', 22),\n",
    "       (6, 'Frank', 28),\n",
    "       (7, 'Grace', 32),\n",
    "       (8, 'Hannah', 26),\n",
    "       (9, 'Ivy', 24),\n",
    "       (10, 'Jack', 27),\n",
    "       (11, 'Kara', 29),\n",
    "       (12, 'Lian', 31),\n",
    "       (13, 'Mia', 33),\n",
    "       (14, 'Nina', 34),\n",
    "       (15, 'Oliver', 36),\n",
    "       (16, 'Paul', 37),\n",
    "       (17, 'Quinn', 38),\n",
    "       (18, 'Rita', 39),\n",
    "       (19, 'Steve', 41),\n",
    "       (20, 'Tom', 42),\n",
    "       (21, 'Nina', 34),\n",
    "       (22, 'Oliver', 36),\n",
    "       (23, 'Paul', 37),\n",
    "       (24, 'Quinn', 38),\n",
    "       (25, 'Rita', 39),\n",
    "       (26, 'Steve', 41),\n",
    "       (27, 'Tom', 42),\n",
    "       (28, 'Tom', 42),\n",
    "       (29, 'Nina', 34),\n",
    "       (30, 'Oliver', 36),\n",
    "       (31, 'Paul', 37),\n",
    "       (32, 'Quinn', 38),\n",
    "       (33, 'Rita', 39),\n",
    "       (34, 'Steve', 41),\n",
    "       (35, 'Tom', 42)]\n",
    "# Add more data to simulate large dataset\n",
    "\n",
    "# Columns for the DataFrame\n",
    "columns = ['id', 'name', 'age']\n",
    "\n",
    "# Create DataFrame from the sample data \n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cad1787f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, age: bigint]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition DataFrame into 20 partitions\n",
    "df_repartitioned = df.repartition(20)\n",
    "df_repartitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65e50c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the number of partitions in the DataFrame\n",
    "num_partitions = df_repartitioned.rdd.getNumPartitions()\n",
    "num_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac142daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [Row(id=30, name='Oliver', age=36)],\n",
       " [Row(id=34, name='Steve', age=41)],\n",
       " [Row(id=29, name='Nina', age=34)],\n",
       " [Row(id=8, name='Hannah', age=26), Row(id=33, name='Rita', age=39)],\n",
       " [Row(id=6, name='Frank', age=28), Row(id=35, name='Tom', age=42)],\n",
       " [Row(id=7, name='Grace', age=32),\n",
       "  Row(id=21, name='Nina', age=34),\n",
       "  Row(id=31, name='Paul', age=37)],\n",
       " [Row(id=5, name='Eve', age=22),\n",
       "  Row(id=23, name='Paul', age=37),\n",
       "  Row(id=32, name='Quinn', age=38)],\n",
       " [Row(id=14, name='Nina', age=34), Row(id=24, name='Quinn', age=38)],\n",
       " [Row(id=2, name='Bob', age=25),\n",
       "  Row(id=11, name='Kara', age=29),\n",
       "  Row(id=13, name='Mia', age=33),\n",
       "  Row(id=22, name='Oliver', age=36)],\n",
       " [Row(id=3, name='Charlie', age=35),\n",
       "  Row(id=9, name='Ivy', age=24),\n",
       "  Row(id=15, name='Oliver', age=36)],\n",
       " [Row(id=1, name='Alice', age=30),\n",
       "  Row(id=12, name='Lian', age=31),\n",
       "  Row(id=16, name='Paul', age=37),\n",
       "  Row(id=27, name='Tom', age=42)],\n",
       " [Row(id=4, name='David', age=40),\n",
       "  Row(id=10, name='Jack', age=27),\n",
       "  Row(id=26, name='Steve', age=41)],\n",
       " [Row(id=28, name='Tom', age=42)],\n",
       " [Row(id=19, name='Steve', age=41), Row(id=25, name='Rita', age=39)],\n",
       " [Row(id=17, name='Quinn', age=38)],\n",
       " [Row(id=18, name='Rita', age=39)],\n",
       " [Row(id=20, name='Tom', age=42)],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the data in each partition using glom()\n",
    "# glom() groups the data in each patition as a list\n",
    "\n",
    "partitions_data = df_repartitioned.rdd.glom().collect()\n",
    "partitions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db544404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition ID: 0, Data: []\n",
      "Partition ID: 1, Data: [Row(id=30, name='Oliver', age=36)]\n",
      "Partition ID: 2, Data: [Row(id=34, name='Steve', age=41)]\n",
      "Partition ID: 3, Data: [Row(id=29, name='Nina', age=34)]\n",
      "Partition ID: 4, Data: [Row(id=8, name='Hannah', age=26), Row(id=33, name='Rita', age=39)]\n",
      "Partition ID: 5, Data: [Row(id=6, name='Frank', age=28), Row(id=35, name='Tom', age=42)]\n",
      "Partition ID: 6, Data: [Row(id=7, name='Grace', age=32), Row(id=21, name='Nina', age=34), Row(id=31, name='Paul', age=37)]\n",
      "Partition ID: 7, Data: [Row(id=5, name='Eve', age=22), Row(id=23, name='Paul', age=37), Row(id=32, name='Quinn', age=38)]\n",
      "Partition ID: 8, Data: [Row(id=14, name='Nina', age=34), Row(id=24, name='Quinn', age=38)]\n",
      "Partition ID: 9, Data: [Row(id=2, name='Bob', age=25), Row(id=11, name='Kara', age=29), Row(id=13, name='Mia', age=33), Row(id=22, name='Oliver', age=36)]\n",
      "Partition ID: 10, Data: [Row(id=3, name='Charlie', age=35), Row(id=9, name='Ivy', age=24), Row(id=15, name='Oliver', age=36)]\n",
      "Partition ID: 11, Data: [Row(id=1, name='Alice', age=30), Row(id=12, name='Lian', age=31), Row(id=16, name='Paul', age=37), Row(id=27, name='Tom', age=42)]\n",
      "Partition ID: 12, Data: [Row(id=4, name='David', age=40), Row(id=10, name='Jack', age=27), Row(id=26, name='Steve', age=41)]\n",
      "Partition ID: 13, Data: [Row(id=28, name='Tom', age=42)]\n",
      "Partition ID: 14, Data: [Row(id=19, name='Steve', age=41), Row(id=25, name='Rita', age=39)]\n",
      "Partition ID: 15, Data: [Row(id=17, name='Quinn', age=38)]\n",
      "Partition ID: 16, Data: [Row(id=18, name='Rita', age=39)]\n",
      "Partition ID: 17, Data: [Row(id=20, name='Tom', age=42)]\n",
      "Partition ID: 18, Data: []\n",
      "Partition ID: 19, Data: []\n"
     ]
    }
   ],
   "source": [
    "# Print partition IDs and their corresponding data \n",
    "for idx, partition in enumerate(partitions_data):\n",
    "    print(f\"Partition ID: {idx}, Data: {partition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4861d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's filter out partitions 16 to 20 using mapPartitionsWithIndex\n",
    "#This will allow us to access the partition index and filter out unwanted partitions. \n",
    "\n",
    "def filter_partition_data(index, iterator):\n",
    "    # keep data only for partitions 0 to 15 (excluding partitions 16 to 20)\n",
    "    if index < 16:\n",
    "        yield from iterator\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da528428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter functions to remove data from partitions 10 to 20 \n",
    "\n",
    "filtered_rdd = df_repartitioned.rdd.mapPartitionsWithIndex(filter_partition_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "101878eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "| 30| Oliver| 36|\n",
      "| 34|  Steve| 41|\n",
      "| 29|   Nina| 34|\n",
      "|  8| Hannah| 26|\n",
      "| 33|   Rita| 39|\n",
      "|  6|  Frank| 28|\n",
      "| 35|    Tom| 42|\n",
      "|  7|  Grace| 32|\n",
      "| 21|   Nina| 34|\n",
      "| 31|   Paul| 37|\n",
      "|  5|    Eve| 22|\n",
      "| 23|   Paul| 37|\n",
      "| 32|  Quinn| 38|\n",
      "| 14|   Nina| 34|\n",
      "| 24|  Quinn| 38|\n",
      "|  2|    Bob| 25|\n",
      "| 11|   Kara| 29|\n",
      "| 13|    Mia| 33|\n",
      "| 22| Oliver| 36|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the filtered RDD back to DataFrame\n",
    "filtered_df = spark.createDataFrame(filtered_rdd, columns)\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9550d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "10f0f7cc",
   "metadata": {},
   "source": [
    "𝐇𝐨𝐰 𝐰𝐨𝐮𝐥𝐝 𝐲𝐨𝐮 𝐡𝐚𝐧𝐝𝐥𝐞 𝐦𝐢𝐬𝐬𝐢𝐧𝐠 𝐨𝐫 𝐢𝐧𝐯𝐚𝐥𝐢𝐝 𝐝𝐚𝐭𝐚 𝐢𝐧 𝐚 𝐝𝐚𝐭𝐚 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 𝐮𝐬𝐢𝐧𝐠 𝐩𝐲𝐬𝐩𝐚𝐫𝐤?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34b956d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+\n",
      "| id|   name| age|salary|\n",
      "+---+-------+----+------+\n",
      "|  1|  Alice|  25|  NULL|\n",
      "|  2|    Bob|NULL|  4000|\n",
      "|  3|Charlie|  30|  4500|\n",
      "|  4|   NULL|NULL|  5000|\n",
      "|  5|    Eve|  28| -1000|\n",
      "+---+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Alice', 25, None), (2, 'Bob', None, 4000), (3, 'Charlie', 30, 4500), (4, None, None, 5000), (5, 'Eve', 28, -1000)]\n",
    "columns = ['id', 'name', 'age', 'salary']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41d76bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows with missing or null values in any column\n",
    "\n",
    "df_dropped = df.dropna()\n",
    "df_dropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb46e5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  3000|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|Unknown|  0|  5000|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values (null) with a specified value\n",
    "\n",
    "df_filled = df.fillna({'age':0, 'salary':3000, 'name':'Unknown'})\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d8a1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  3000|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|Unknown|  0|  5000|\n",
      "|  5|    Eve| 28|     0|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replacing invalid data (e.g., salary <0) with a valid value (e.g., 0)\n",
    "\n",
    "df_invallid_replaced = df_filled.withColumn('salary', when(col('salary') <0, 0).otherwise(col('salary')))\n",
    "df_invallid_replaced.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67acd57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  NULL|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|   NULL|  0|  5000|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling invalid data types (e.g., if age is not an integer) \n",
    "# For this, we would attempt to cast age to an integer and filter out rows with invalid age data\n",
    "\n",
    "df_invalid_age_handled = df.withColumn('age', when(col('age').cast('int').isNotNull(), col('age')).otherwise(0))\n",
    "df_invalid_age_handled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d34f104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  NULL|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|   NULL|  0|  5000|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling missing values in a specific column (e.g., age), without affecting other columns\n",
    "\n",
    "df_age_filled = df.fillna({'age':0})\n",
    "df_age_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1d07a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|age|salary|\n",
      "+---+----+---+------+\n",
      "|  0|   1|  2|     1|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking how many nulls are present in each column before and after cleaning\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4192dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|age|salary|\n",
      "+---+----+---+------+\n",
      "|  0|   0|  0|     0|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count of Null values after handling.\n",
    "\n",
    "df_invallid_replaced.select([count(when(col(c).isNull(), c)).alias(c) for c in df_invallid_replaced.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70bb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
