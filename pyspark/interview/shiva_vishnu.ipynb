{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989f7a5d",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/in/sivavishnu1/recent-activity/all/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d3f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46579161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96a9eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PartitionManagement</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2578ed2f850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize spark session\n",
    "spark = SparkSession.builder.appName(\"PartitionManagement\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb05cc7",
   "metadata": {},
   "source": [
    "𝐈𝐦𝐚𝐠𝐢𝐧𝐞 𝐲𝐨𝐮 𝐡𝐚𝐯𝐞 𝐚 𝐥𝐚𝐫𝐠𝐞 𝐝𝐚𝐭𝐚𝐬𝐞𝐭 𝐥𝐨𝐚𝐝𝐞𝐝 𝐢𝐧𝐭𝐨 𝐒𝐩𝐚𝐫𝐤 𝐚𝐧𝐝 𝐚𝐟𝐭𝐞𝐫 𝐩𝐞𝐫𝐟𝐨𝐫𝐦𝐢𝐧𝐠 𝐬𝐨𝐦𝐞 𝐭𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧𝐬, 𝐭𝐡𝐞 𝐝𝐚𝐭𝐚 𝐢𝐬 𝐫𝐞𝐩𝐚𝐫𝐭𝐢𝐭𝐢𝐨𝐧𝐞𝐝 𝐢𝐧𝐭𝐨 𝟐𝟎 𝐩𝐚𝐫𝐭𝐢𝐭𝐢𝐨𝐧𝐬. 𝐘𝐨𝐮 𝐧𝐨𝐰 𝐰𝐚𝐧𝐭 𝐭𝐨 𝐩𝐫𝐢𝐧𝐭 𝐭𝐡𝐞 𝐩𝐚𝐫𝐭𝐢𝐭𝐢𝐨𝐧 𝐈𝐃𝐬 𝐚𝐥𝐨𝐧𝐠 𝐰𝐢𝐭𝐡 𝐭𝐡𝐞𝐢𝐫 𝐜𝐨𝐫𝐫𝐞𝐬𝐩𝐨𝐧𝐝𝐢𝐧𝐠 𝐝𝐚𝐭𝐚. 𝐀𝐝𝐝𝐢𝐭𝐢𝐨𝐧𝐚𝐥𝐥𝐲, 𝐲𝐨𝐮 𝐧𝐞𝐞𝐝 𝐭𝐨 𝐫𝐞𝐦𝐨𝐯𝐞 𝐭𝐡𝐞 𝐝𝐚𝐭𝐚 𝐟𝐫𝐨𝐦 𝐩𝐚𝐫𝐭𝐢𝐭𝐢𝐨𝐧𝐬 𝟏𝟔 𝐭𝐨 𝟐𝟎.\n",
    "𝐈𝐬 𝐭𝐡𝐢𝐬 𝐩𝐨𝐬𝐬𝐢𝐛𝐥𝐞 𝐢𝐧 𝐒𝐩𝐚𝐫𝐤? 𝐈𝐟 𝐬𝐨, 𝐡𝐨𝐰 𝐰𝐨𝐮𝐥𝐝 𝐲𝐨𝐮 𝐩𝐫𝐢𝐧𝐭 𝐭𝐡𝐞 𝐩𝐚𝐫𝐭𝐢𝐭𝐢𝐨𝐧 𝐈𝐃𝐬 𝐚𝐧𝐝 𝐫𝐞𝐦𝐨𝐯𝐞 𝐭𝐡𝐞 𝐝𝐚𝐭𝐚 𝐟𝐫𝐨𝐦 𝐩𝐚𝐫𝐭𝐢𝐭𝐢𝐨𝐧𝐬 𝟏𝟔 𝐭𝐨 𝟐𝟎 𝐮𝐬𝐢𝐧𝐠 𝐏𝐲𝐒𝐩𝐚𝐫𝐤? 𝐏𝐥𝐞𝐚𝐬𝐞 𝐞𝐱𝐩𝐥𝐚𝐢𝐧 𝐭𝐡𝐞 𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐚𝐧𝐝 𝐩𝐫𝐨𝐯𝐢𝐝𝐞 𝐚 𝐬𝐚𝐦𝐩𝐥𝐞 𝐜𝐨𝐝𝐞 𝐭𝐨 𝐚𝐜𝐡𝐢𝐞𝐯𝐞 𝐭𝐡𝐢𝐬.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa7bf5",
   "metadata": {},
   "source": [
    "# 𝐒𝐭𝐞𝐩𝐬 𝐟𝐨𝐫 𝐏𝐫𝐢𝐧𝐭𝐢𝐧𝐠 𝐏𝐚𝐫𝐭𝐢𝐭𝐢𝐨𝐧 𝐈𝐃𝐬 𝐚𝐧𝐝 𝐃𝐞𝐥𝐞𝐭𝐢𝐧𝐠 𝐃𝐚𝐭𝐚 𝐢𝐧 𝐏𝐚𝐫𝐭𝐢𝐭𝐢𝐨𝐧𝐬 𝟏𝟔 𝐭𝐨 𝟐𝟎:\n",
    "\n",
    "\"\"\"1. View Partition IDs:\n",
    "In order to view the partition IDs and the data within each partition, we can use the glom() method that groups the data within each partition into a list.\n",
    "\n",
    "2. Deleting Data from Partitions 16 to 20:\n",
    "Spark doesn't provide direct APIs for deleting specific partitions. However, we can manipulate the data by filtering out the data from the desired partitions (in this case, partitions 16-20). Here's the step-by-step approach:\n",
    "Read and View Data Partitions: First, read the data into Spark and check how it is partitioned.\n",
    "\n",
    "Use mapPartitionsWithIndex: This is a transformation that allows you to access the index of each partition and the corresponding data.\n",
    "Filter out Partitions 16 to 20: Based on the partition index, we can filter out the unwanted partitions.\n",
    "\n",
    "Repartition: If necessary, repartition the data after filtering out the partitions you don't need.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec4496e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "|  5|    Eve| 22|\n",
      "|  6|  Frank| 28|\n",
      "|  7|  Grace| 32|\n",
      "|  8| Hannah| 26|\n",
      "|  9|    Ivy| 24|\n",
      "| 10|   Jack| 27|\n",
      "| 11|   Kara| 29|\n",
      "| 12|   Lian| 31|\n",
      "| 13|    Mia| 33|\n",
      "| 14|   Nina| 34|\n",
      "| 15| Oliver| 36|\n",
      "| 16|   Paul| 37|\n",
      "| 17|  Quinn| 38|\n",
      "| 18|   Rita| 39|\n",
      "| 19|  Steve| 41|\n",
      "| 20|    Tom| 42|\n",
      "+---+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Alice', 30),\n",
    "       (2, 'Bob', 25),\n",
    "       (3, 'Charlie', 35),\n",
    "       (4, 'David', 40),\n",
    "       (5, 'Eve', 22),\n",
    "       (6, 'Frank', 28),\n",
    "       (7, 'Grace', 32),\n",
    "       (8, 'Hannah', 26),\n",
    "       (9, 'Ivy', 24),\n",
    "       (10, 'Jack', 27),\n",
    "       (11, 'Kara', 29),\n",
    "       (12, 'Lian', 31),\n",
    "       (13, 'Mia', 33),\n",
    "       (14, 'Nina', 34),\n",
    "       (15, 'Oliver', 36),\n",
    "       (16, 'Paul', 37),\n",
    "       (17, 'Quinn', 38),\n",
    "       (18, 'Rita', 39),\n",
    "       (19, 'Steve', 41),\n",
    "       (20, 'Tom', 42),\n",
    "       (21, 'Nina', 34),\n",
    "       (22, 'Oliver', 36),\n",
    "       (23, 'Paul', 37),\n",
    "       (24, 'Quinn', 38),\n",
    "       (25, 'Rita', 39),\n",
    "       (26, 'Steve', 41),\n",
    "       (27, 'Tom', 42),\n",
    "       (28, 'Tom', 42),\n",
    "       (29, 'Nina', 34),\n",
    "       (30, 'Oliver', 36),\n",
    "       (31, 'Paul', 37),\n",
    "       (32, 'Quinn', 38),\n",
    "       (33, 'Rita', 39),\n",
    "       (34, 'Steve', 41),\n",
    "       (35, 'Tom', 42)]\n",
    "# Add more data to simulate large dataset\n",
    "\n",
    "# Columns for the DataFrame\n",
    "columns = ['id', 'name', 'age']\n",
    "\n",
    "# Create DataFrame from the sample data \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cad1787f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, age: bigint]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition DataFrame into 20 partitions\n",
    "df_repartitioned = df.repartition(20)\n",
    "df_repartitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65e50c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the number of partitions in the DataFrame\n",
    "num_partitions = df_repartitioned.rdd.getNumPartitions()\n",
    "num_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac142daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [Row(id=30, name='Oliver', age=36)],\n",
       " [Row(id=34, name='Steve', age=41)],\n",
       " [Row(id=29, name='Nina', age=34)],\n",
       " [Row(id=8, name='Hannah', age=26), Row(id=33, name='Rita', age=39)],\n",
       " [Row(id=6, name='Frank', age=28), Row(id=35, name='Tom', age=42)],\n",
       " [Row(id=7, name='Grace', age=32),\n",
       "  Row(id=21, name='Nina', age=34),\n",
       "  Row(id=31, name='Paul', age=37)],\n",
       " [Row(id=5, name='Eve', age=22),\n",
       "  Row(id=23, name='Paul', age=37),\n",
       "  Row(id=32, name='Quinn', age=38)],\n",
       " [Row(id=14, name='Nina', age=34), Row(id=24, name='Quinn', age=38)],\n",
       " [Row(id=2, name='Bob', age=25),\n",
       "  Row(id=11, name='Kara', age=29),\n",
       "  Row(id=13, name='Mia', age=33),\n",
       "  Row(id=22, name='Oliver', age=36)],\n",
       " [Row(id=3, name='Charlie', age=35),\n",
       "  Row(id=9, name='Ivy', age=24),\n",
       "  Row(id=15, name='Oliver', age=36)],\n",
       " [Row(id=1, name='Alice', age=30),\n",
       "  Row(id=12, name='Lian', age=31),\n",
       "  Row(id=16, name='Paul', age=37),\n",
       "  Row(id=27, name='Tom', age=42)],\n",
       " [Row(id=4, name='David', age=40),\n",
       "  Row(id=10, name='Jack', age=27),\n",
       "  Row(id=26, name='Steve', age=41)],\n",
       " [Row(id=28, name='Tom', age=42)],\n",
       " [Row(id=19, name='Steve', age=41), Row(id=25, name='Rita', age=39)],\n",
       " [Row(id=17, name='Quinn', age=38)],\n",
       " [Row(id=18, name='Rita', age=39)],\n",
       " [Row(id=20, name='Tom', age=42)],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the data in each partition using glom()\n",
    "# glom() groups the data in each patition as a list\n",
    "\n",
    "partitions_data = df_repartitioned.rdd.glom().collect()\n",
    "partitions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db544404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition ID: 0, Data: []\n",
      "Partition ID: 1, Data: [Row(id=30, name='Oliver', age=36)]\n",
      "Partition ID: 2, Data: [Row(id=34, name='Steve', age=41)]\n",
      "Partition ID: 3, Data: [Row(id=29, name='Nina', age=34)]\n",
      "Partition ID: 4, Data: [Row(id=8, name='Hannah', age=26), Row(id=33, name='Rita', age=39)]\n",
      "Partition ID: 5, Data: [Row(id=6, name='Frank', age=28), Row(id=35, name='Tom', age=42)]\n",
      "Partition ID: 6, Data: [Row(id=7, name='Grace', age=32), Row(id=21, name='Nina', age=34), Row(id=31, name='Paul', age=37)]\n",
      "Partition ID: 7, Data: [Row(id=5, name='Eve', age=22), Row(id=23, name='Paul', age=37), Row(id=32, name='Quinn', age=38)]\n",
      "Partition ID: 8, Data: [Row(id=14, name='Nina', age=34), Row(id=24, name='Quinn', age=38)]\n",
      "Partition ID: 9, Data: [Row(id=2, name='Bob', age=25), Row(id=11, name='Kara', age=29), Row(id=13, name='Mia', age=33), Row(id=22, name='Oliver', age=36)]\n",
      "Partition ID: 10, Data: [Row(id=3, name='Charlie', age=35), Row(id=9, name='Ivy', age=24), Row(id=15, name='Oliver', age=36)]\n",
      "Partition ID: 11, Data: [Row(id=1, name='Alice', age=30), Row(id=12, name='Lian', age=31), Row(id=16, name='Paul', age=37), Row(id=27, name='Tom', age=42)]\n",
      "Partition ID: 12, Data: [Row(id=4, name='David', age=40), Row(id=10, name='Jack', age=27), Row(id=26, name='Steve', age=41)]\n",
      "Partition ID: 13, Data: [Row(id=28, name='Tom', age=42)]\n",
      "Partition ID: 14, Data: [Row(id=19, name='Steve', age=41), Row(id=25, name='Rita', age=39)]\n",
      "Partition ID: 15, Data: [Row(id=17, name='Quinn', age=38)]\n",
      "Partition ID: 16, Data: [Row(id=18, name='Rita', age=39)]\n",
      "Partition ID: 17, Data: [Row(id=20, name='Tom', age=42)]\n",
      "Partition ID: 18, Data: []\n",
      "Partition ID: 19, Data: []\n"
     ]
    }
   ],
   "source": [
    "# Print partition IDs and their corresponding data \n",
    "for idx, partition in enumerate(partitions_data):\n",
    "    print(f\"Partition ID: {idx}, Data: {partition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4861d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's filter out partitions 16 to 20 using mapPartitionsWithIndex\n",
    "#This will allow us to access the partition index and filter out unwanted partitions. \n",
    "\n",
    "def filter_partition_data(index, iterator):\n",
    "    # keep data only for partitions 0 to 15 (excluding partitions 16 to 20)\n",
    "    if index < 16:\n",
    "        yield from iterator\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da528428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter functions to remove data from partitions 10 to 20 \n",
    "\n",
    "filtered_rdd = df_repartitioned.rdd.mapPartitionsWithIndex(filter_partition_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "101878eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "| 30| Oliver| 36|\n",
      "| 34|  Steve| 41|\n",
      "| 29|   Nina| 34|\n",
      "|  8| Hannah| 26|\n",
      "| 33|   Rita| 39|\n",
      "|  6|  Frank| 28|\n",
      "| 35|    Tom| 42|\n",
      "|  7|  Grace| 32|\n",
      "| 21|   Nina| 34|\n",
      "| 31|   Paul| 37|\n",
      "|  5|    Eve| 22|\n",
      "| 23|   Paul| 37|\n",
      "| 32|  Quinn| 38|\n",
      "| 14|   Nina| 34|\n",
      "| 24|  Quinn| 38|\n",
      "|  2|    Bob| 25|\n",
      "| 11|   Kara| 29|\n",
      "| 13|    Mia| 33|\n",
      "| 22| Oliver| 36|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the filtered RDD back to DataFrame \n",
    "filtered_df = spark.createDataFrame(filtered_rdd, columns) \n",
    "filtered_df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0599b3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9550d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "10f0f7cc",
   "metadata": {},
   "source": [
    "𝐇𝐨𝐰 𝐰𝐨𝐮𝐥𝐝 𝐲𝐨𝐮 𝐡𝐚𝐧𝐝𝐥𝐞 𝐦𝐢𝐬𝐬𝐢𝐧𝐠 𝐨𝐫 𝐢𝐧𝐯𝐚𝐥𝐢𝐝 𝐝𝐚𝐭𝐚 𝐢𝐧 𝐚 𝐝𝐚𝐭𝐚 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 𝐮𝐬𝐢𝐧𝐠 𝐩𝐲𝐬𝐩𝐚𝐫𝐤?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86f063",
   "metadata": {},
   "source": [
    "𝐊𝐞𝐲 𝐇𝐚𝐧𝐝𝐥𝐢𝐧𝐠 𝐓𝐞𝐜𝐡𝐧𝐢𝐪𝐮𝐞𝐬:\n",
    "\n",
    "1, Dropping Missing Values\n",
    "\n",
    "2, Filling Missing Values\n",
    "\n",
    "3, Replacing Invalid Data\n",
    "\n",
    "4, Handling Invalid Data Types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a944c",
   "metadata": {},
   "source": [
    "Handling null values is a crucial part of data preprocessing in PySpark. Null values can arise due to missing data or errors during data collection, and they can impact the quality of your analysis and processing. PySpark provides several functions to manage null values, such as dropna() to remove rows with nulls, fillna() to replace null values with specified defaults, and filter() to select rows based on the presence of nulls. You can also use conditional expressions like when() to replace null values with different values depending on certain conditions. Proper handling of null values ensures better data integrity and more accurate results in your Spark processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34b956d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+\n",
      "| id|   name| age|salary|\n",
      "+---+-------+----+------+\n",
      "|  1|  Alice|  25|  NULL|\n",
      "|  2|    Bob|NULL|  4000|\n",
      "|  3|Charlie|  30|  4500|\n",
      "|  4|   NULL|NULL|  5000|\n",
      "|  5|    Eve|  28| -1000|\n",
      "+---+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Alice', 25, None), (2, 'Bob', None, 4000), (3, 'Charlie', 30, 4500), (4, None, None, 5000), (5, 'Eve', 28, -1000)] \n",
    "columns = ['id', 'name', 'age', 'salary'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41d76bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows with missing or null values in any column\n",
    "\n",
    "df_dropped = df.dropna() \n",
    "df_dropped.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb46e5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  3000|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|Unknown|  0|  5000|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values (null) with a specified value\n",
    "\n",
    "df_filled = df.fillna({'age':0, 'salary':3000, 'name':'Unknown'})\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d8a1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  3000|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|Unknown|  0|  5000|\n",
      "|  5|    Eve| 28|     0|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replacing invalid data (e.g., salary <0) with a valid value (e.g., 0)\n",
    "\n",
    "df_invallid_replaced = df_filled.withColumn('salary', when(col('salary') <0, 0).otherwise(col('salary')))\n",
    "df_invallid_replaced.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67acd57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  NULL|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|   NULL|  0|  5000|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling invalid data types (e.g., if age is not an integer) \n",
    "# For this, we would attempt to cast age to an integer and filter out rows with invalid age data\n",
    "\n",
    "df_invalid_age_handled = df.withColumn('age', when(col('age').cast('int').isNotNull(), col('age')).otherwise(0))\n",
    "df_invalid_age_handled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d34f104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25|  NULL|\n",
      "|  2|    Bob|  0|  4000|\n",
      "|  3|Charlie| 30|  4500|\n",
      "|  4|   NULL|  0|  5000|\n",
      "|  5|    Eve| 28| -1000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling missing values in a specific column (e.g., age), without affecting other columns\n",
    "\n",
    "df_age_filled = df.fillna({'age':0})\n",
    "df_age_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d07a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|age|salary|\n",
      "+---+----+---+------+\n",
      "|  0|   1|  2|     1|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking how many nulls are present in each column before and after cleaning\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4192dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|age|salary|\n",
      "+---+----+---+------+\n",
      "|  0|   0|  0|     0|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count of Null values after handling.\n",
    "\n",
    "df_invallid_replaced.select([count(when(col(c).isNull(), c)).alias(c) for c in df_invallid_replaced.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e70bb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee5d1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b0431ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+----------+\n",
      "|  Name|Subject|Mark|Attendance|\n",
      "+------+-------+----+----------+\n",
      "|  John|   Math|  75|   Present|\n",
      "|Martin|Science|NULL|    Absent|\n",
      "| Sarah|History|  85|      NULL|\n",
      "|  Alex|   Math|NULL|   Present|\n",
      "|   Eve|   Math|  90|    Absent|\n",
      "+------+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", 'Math', 75, \"Present\"),\n",
    "        ('Martin', 'Science', None, \"Absent\"),\n",
    "        ('Sarah', 'History', 85, None),\n",
    "        ('Alex', \"Math\", None, \"Present\"),\n",
    "        ('Eve', 'Math', 90, 'Absent')] \n",
    "columns = ['Name', 'Subject', 'Mark', 'Attendance'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5b6b77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+----------+\n",
      "| Name|Subject|Mark|Attendance|\n",
      "+-----+-------+----+----------+\n",
      "| John|   Math|  75|   Present|\n",
      "|Sarah|History|  85|      NULL|\n",
      "|  Eve|   Math|  90|    Absent|\n",
      "+-----+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter using isNotNull\n",
    "df_not_null = df.filter(col('Mark').isNotNull())\n",
    "df_not_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ffd7fd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+----------+\n",
      "|  Name|Subject|Mark|Attendance|\n",
      "+------+-------+----+----------+\n",
      "|  John|   Math|  75|   Present|\n",
      "|Martin|Science|NULL|    Absent|\n",
      "| Sarah|History|  85|      NULL|\n",
      "|  Alex|   Math|NULL|   Present|\n",
      "|   Eve|   Math|  90|    Absent|\n",
      "+------+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter using 'OR'\n",
    "df_not_null_either = df.filter(col('Mark').isNotNull() | col('Attendance').isNotNull())\n",
    "df_not_null_either.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2d1d8e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+----------+\n",
      "| Name|Subject|Mark|Attendance|\n",
      "+-----+-------+----+----------+\n",
      "| John|   Math|  75|   Present|\n",
      "|Sarah|History|  85|      NULL|\n",
      "|  Eve|   Math|  90|    Absent|\n",
      "+-----+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_drp = df.dropna(subset= ['Mark'])\n",
    "df_drp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a578ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae31aed",
   "metadata": {},
   "source": [
    "# 𝐑𝐞𝐩𝐚𝐫𝐭𝐢𝐭𝐢𝐨𝐧 𝐯𝐬. 𝐂𝐨𝐚𝐥𝐞𝐬𝐜𝐞:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "187dc6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  Alice|  1|\n",
      "|    Bob|  2|\n",
      "|Charlie|  3|\n",
      "|  David|  4|\n",
      "|    Eva|  5|\n",
      "|  Frank|  6|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Alice', 1), ('Bob',2), ('Charlie', 3), ('David', 4), ('Eva', 5), ('Frank', 6)] \n",
    "columns = ['name', 'id'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "416f25ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions before repartitioning: 8\n"
     ]
    }
   ],
   "source": [
    "print('Number of partitions before repartitioning:', df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50a9ecd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, id: bigint]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartitioning the dataframe into 4 partitions for parallelism \n",
    "df_repartitioned = df.repartition(4)\n",
    "df_repartitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1de36573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repartitioned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc27eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  David|  4|\n",
      "|    Eva|  5|\n",
      "|  Frank|  6|\n",
      "|Charlie|  3|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a transformation (filtering) to check data\n",
    "df_filtered = df_repartitioned.filter(col('id') > 2)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "31448169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  David|  4|\n",
      "|    Eva|  5|\n",
      "|Charlie|  3|\n",
      "|  Frank|  6|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Coalescing the partitions after filtering (reducing to 2 partitions for optimization)\n",
    "df_coaleasced = df_filtered.coalesce(2)\n",
    "print(df_coaleasced.rdd.getNumPartitions())\n",
    "df_coaleasced.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5f4e4213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   name| department|\n",
      "+-------+-----------+\n",
      "|  Alice|         HR|\n",
      "|    Bob|    Finance|\n",
      "|Charlie|Engineering|\n",
      "|  David|         HR|\n",
      "|    Eva|Engineering|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a join with another Dataframe to demonstrate partitioning in action\n",
    "\n",
    "data2 = [('Alice', 'HR'), (\"Bob\", \"Finance\"), (\"Charlie\",\"Engineering\"), ('David', 'HR'), (\"Eva\", \"Engineering\")]\n",
    "column2 = ['name', 'department']\n",
    "df2 = spark.createDataFrame(data2, column2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b3813dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, department: string]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition the second dataframe by 'name' to align partitioning for a more efficient join\n",
    "\n",
    "df2_partitioned = df2.repartition('name')\n",
    "df2_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57def181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------+\n",
      "|   name| id| department|\n",
      "+-------+---+-----------+\n",
      "|  Alice|  1|         HR|\n",
      "|    Bob|  2|    Finance|\n",
      "|Charlie|  3|Engineering|\n",
      "|  David|  4|         HR|\n",
      "|    Eva|  5|Engineering|\n",
      "+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined = df_repartitioned.join(df2_partitioned, on='name', how='inner')\n",
    "df_joined.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f6205",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb38f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "|  5|    Eve| 22|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Alice', 30), (2, 'Bob', 25), (3, 'Charlie', 35), (4, 'David', 40), (5, 'Eve', 22)] \n",
    "columns = ['id', 'name', 'age'] \n",
    "df = spark.createDataFrame(data, columns) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1e58d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the checkpoint directory \n",
    "checkpoint_dir = 'file:///temp/spark_checkpoints' # Local file system checkpoint directory\n",
    "spark.sparkContext.setCheckpointDir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99327f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------------+\n",
      "| id|   name|age|age_plus_ten|\n",
      "+---+-------+---+------------+\n",
      "|  1|  Alice| 30|          40|\n",
      "|  3|Charlie| 35|          45|\n",
      "|  4|  David| 40|          50|\n",
      "+---+-------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform transformations \n",
    "df_trns = df.filter(col('age') > 25).withColumn('age_plus_ten', col('age') + 10)\n",
    "df_trns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "af4009b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint the traansformed dataframe\n",
    "df_trns = df_trns.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "641c1bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------------+\n",
      "| id|   name|age|age_plus_ten|\n",
      "+---+-------+---+------------+\n",
      "|  1|  Alice| 30|          40|\n",
      "|  3|Charlie| 35|          45|\n",
      "|  4|  David| 40|          50|\n",
      "+---+-------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5e8c2c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------------+-------------+\n",
      "| id|   name|age|age_plus_ten|age_times_two|\n",
      "+---+-------+---+------------+-------------+\n",
      "|  1|  Alice| 30|          40|           60|\n",
      "|  3|Charlie| 35|          45|           70|\n",
      "|  4|  David| 40|          50|           80|\n",
      "+---+-------+---+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform another transformation on the checkpointed dataframe\n",
    "df_final = df_trns.withColumn('age_times_two', col('age') *2)\n",
    "df_final.show()\n",
    "\n",
    "# Simulating a failure to see checkpoint recovery\n",
    "# Uncommetn the line below to simulate a failure after checkpointing\n",
    "# raise Exception ('Simulating a job failure after checkpointing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b057a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec3a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_data = [(1, 'Alice', 'alice@example.com'),\n",
    "                 (2, 'Bob', 'bob@example.com'),\n",
    "                 (3, 'Charlie', 'charlie@example.com')]\n",
    "customers_columns = ['customer_id', 'name', 'email']\n",
    "\n",
    "orders_data = [(101, 1, 1001, '2023-01-01'),\n",
    "              (102, 2, 1002, '2023-01-05'),\n",
    "              (103, 3, 1001, '2023-01-10')]\n",
    "orders_columns = ['order_id', 'customer_id', 'product_id', 'order_date']\n",
    "\n",
    "products_data = [(1001, 'Laptop', 'Electronics'),\n",
    "                (1002, 'Shoes', 'Apparel')]\n",
    "products_columns = ['product_id', 'product_name', 'category']\n",
    "\n",
    "payments_data = [ (1, 101, 1200.50, 'Completed'),\n",
    "                (2, 102, 50.00, 'Pending'),\n",
    "                (3, 103, 1200.50, 'Completed')]\n",
    "payments_columns = ['payment_id', 'order_id', 'amount', 'payment_status']\n",
    "\n",
    "\n",
    "df_customers = spark.createDataFrame(customers_data, customers_columns)\n",
    "df_orders = spark.createDataFrame(orders_data, orders_columns)\n",
    "df_products = spark.createDataFrame(products_data, products_columns)\n",
    "df_payments = spark.createDataFrame(payments_data, payments_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437935a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_orders.join(df_customers, on='customer_id', how='inner')\n",
    "df_joined = df_joined.join(df_products, on='product_id', how='inner')\n",
    "df_joined = df_joined.join(df_payments, on = 'order_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a94988d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------+----------+------------+-----------+------+--------------+\n",
      "|   name|              email|order_id|order_date|product_name|   category|amount|payment_status|\n",
      "+-------+-------------------+--------+----------+------------+-----------+------+--------------+\n",
      "|  Alice|  alice@example.com|     101|2023-01-01|      Laptop|Electronics|1200.5|     Completed|\n",
      "|    Bob|    bob@example.com|     102|2023-01-05|       Shoes|    Apparel|  50.0|       Pending|\n",
      "|Charlie|charlie@example.com|     103|2023-01-10|      Laptop|Electronics|1200.5|     Completed|\n",
      "+-------+-------------------+--------+----------+------------+-----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.select(\n",
    "    'name', 'email', 'order_id', 'order_date', 'product_name', 'category', 'amount', 'payment_status').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c62b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
