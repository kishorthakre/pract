{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4385745",
   "metadata": {},
   "source": [
    "https://pravash-techie.medium.com/pyspark-interview-questions-coding-part-1-b4b7cea4d2f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1f750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b9d068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Med</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2a930ce6d50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "spark = SparkSession.builder.appName('Med').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d5d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587baf2",
   "metadata": {},
   "source": [
    "### Q1. ClickStream\n",
    "Given a clickstream of user activity data , find the relevant user session for each click event."
   ]
  },
  {
   "cell_type": "raw",
   "id": "99a40aab",
   "metadata": {},
   "source": [
    "click_time | user_id\n",
    "2018–01–01 11:00:00 | u1\n",
    "2018–01–01 12:00:00 | u1\n",
    "2018–01–01 13:00:00 | u1\n",
    "2018–01–01 13:00:00 | u1\n",
    "2018–01–01 14:00:00 | u1\n",
    "2018–01–01 15:00:00 | u1\n",
    "2018–01–01 11:00:00 | u2\n",
    "2018–01–02 11:00:00 | u2\n",
    "\n",
    "session definition:\n",
    "1. session expires after inactivity of 30mins, because of inactivity no clickstream will be generated\n",
    "2. session remain active for total of 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971f771d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- click_time: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n",
      "+-------------------+-------+\n",
      "|         click_time|user_id|\n",
      "+-------------------+-------+\n",
      "|2018-01-01 11:00:00|     u1|\n",
      "|2018-01-01 12:00:00|     u1|\n",
      "|2018-01-01 13:00:00|     u1|\n",
      "|2018-01-01 13:00:00|     u1|\n",
      "|2018-01-01 14:00:00|     u1|\n",
      "|2018-01-01 15:00:00|     u1|\n",
      "|2018-01-01 11:00:00|     u2|\n",
      "|2018-01-02 11:00:00|     u2|\n",
      "+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = \"click_time STRING, user_id STRING\" \n",
    "\n",
    "data = [\n",
    "    (\"2018-01-01 11:00:00\", \"u1\"),\n",
    "    (\"2018-01-01 12:00:00\", \"u1\"),\n",
    "    (\"2018-01-01 13:00:00\", \"u1\"),\n",
    "    (\"2018-01-01 13:00:00\", \"u1\"),\n",
    "    (\"2018-01-01 14:00:00\", \"u1\"),\n",
    "    (\"2018-01-01 15:00:00\", \"u1\"),\n",
    "    (\"2018-01-01 11:00:00\", \"u2\"),\n",
    "    (\"2018-01-02 11:00:00\", \"u2\")\n",
    "]\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cba20af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------------+\n",
      "|         click_time|user_id|click_timestamp|\n",
      "+-------------------+-------+---------------+\n",
      "|2018-01-01 11:00:00|     u1|     1514784600|\n",
      "|2018-01-01 12:00:00|     u1|     1514788200|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|\n",
      "|2018-01-01 14:00:00|     u1|     1514795400|\n",
      "|2018-01-01 15:00:00|     u1|     1514799000|\n",
      "|2018-01-01 11:00:00|     u2|     1514784600|\n",
      "|2018-01-02 11:00:00|     u2|     1514871000|\n",
      "+-------------------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert click_time to Unix timestamp for easier calculations \n",
    "\n",
    "df = df.withColumn(\"click_timestamp\", unix_timestamp(\"click_time\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd27954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------------+--------------------+\n",
      "|         click_time|user_id|click_timestamp|prev_click_timestamp|\n",
      "+-------------------+-------+---------------+--------------------+\n",
      "|2018-01-01 11:00:00|     u1|     1514784600|                NULL|\n",
      "|2018-01-01 12:00:00|     u1|     1514788200|          1514784600|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|          1514788200|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|          1514791800|\n",
      "|2018-01-01 14:00:00|     u1|     1514795400|          1514791800|\n",
      "|2018-01-01 15:00:00|     u1|     1514799000|          1514795400|\n",
      "|2018-01-01 11:00:00|     u2|     1514784600|                NULL|\n",
      "|2018-01-02 11:00:00|     u2|     1514871000|          1514784600|\n",
      "+-------------------+-------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy('user_id').orderBy('click_timestamp')\n",
    "\n",
    "# Getting the previous row value using lag\n",
    "\n",
    "df = df.withColumn(\"prev_click_timestamp\", lag('click_timestamp', 1).over(window))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39996038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------------+--------------------+--------------+\n",
      "|         click_time|user_id|click_timestamp|prev_click_timestamp|timestamp_diff|\n",
      "+-------------------+-------+---------------+--------------------+--------------+\n",
      "|2018-01-01 11:00:00|     u1|     1514784600|                NULL|          NULL|\n",
      "|2018-01-01 12:00:00|     u1|     1514788200|          1514784600|          60.0|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|          1514788200|          60.0|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|          1514791800|           0.0|\n",
      "|2018-01-01 14:00:00|     u1|     1514795400|          1514791800|          60.0|\n",
      "|2018-01-01 15:00:00|     u1|     1514799000|          1514795400|          60.0|\n",
      "|2018-01-01 11:00:00|     u2|     1514784600|                NULL|          NULL|\n",
      "|2018-01-02 11:00:00|     u2|     1514871000|          1514784600|        1440.0|\n",
      "+-------------------+-------+---------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Difference between click time and dividing that with 60\n",
    "\n",
    "df = df.withColumn(\"timestamp_diff\", (col('click_timestamp') - col('prev_click_timestamp')) / 60)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f66e3f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------------+--------------------+--------------+\n",
      "|         click_time|user_id|click_timestamp|prev_click_timestamp|timestamp_diff|\n",
      "+-------------------+-------+---------------+--------------------+--------------+\n",
      "|2018-01-01 11:00:00|     u1|     1514784600|                NULL|           0.0|\n",
      "|2018-01-01 12:00:00|     u1|     1514788200|          1514784600|          60.0|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|          1514788200|          60.0|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|          1514791800|           0.0|\n",
      "|2018-01-01 14:00:00|     u1|     1514795400|          1514791800|          60.0|\n",
      "|2018-01-01 15:00:00|     u1|     1514799000|          1514795400|          60.0|\n",
      "|2018-01-01 11:00:00|     u2|     1514784600|                NULL|           0.0|\n",
      "|2018-01-02 11:00:00|     u2|     1514871000|          1514784600|        1440.0|\n",
      "+-------------------+-------+---------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Updating null with 0 \n",
    "#df.na.fill(value = 0).show()\n",
    "\n",
    "df = df.withColumn(\"timestamp_diff\", when(col('timestamp_diff').isNull(), 0).otherwise(col('timestamp_diff')))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6acc6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------------+--------------------+--------------+-----------+\n",
      "|         click_time|user_id|click_timestamp|prev_click_timestamp|timestamp_diff|session_new|\n",
      "+-------------------+-------+---------------+--------------------+--------------+-----------+\n",
      "|2018-01-01 11:00:00|     u1|     1514784600|                NULL|          NULL|          0|\n",
      "|2018-01-01 12:00:00|     u1|     1514788200|          1514784600|          60.0|          1|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|          1514788200|          60.0|          1|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|          1514791800|           0.0|          0|\n",
      "|2018-01-01 14:00:00|     u1|     1514795400|          1514791800|          60.0|          1|\n",
      "|2018-01-01 15:00:00|     u1|     1514799000|          1514795400|          60.0|          1|\n",
      "|2018-01-01 11:00:00|     u2|     1514784600|                NULL|          NULL|          0|\n",
      "|2018-01-02 11:00:00|     u2|     1514871000|          1514784600|        1440.0|          1|\n",
      "+-------------------+-------+---------------+--------------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for new session \n",
    "\n",
    "df = df.withColumn(\"session_new\", when(col(\"timestamp_diff\") >30, 1).otherwise(0))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "888a10b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------------+--------------------+--------------+-----------+----------------+\n",
      "|         click_time|user_id|click_timestamp|prev_click_timestamp|timestamp_diff|session_new|session_new_name|\n",
      "+-------------------+-------+---------------+--------------------+--------------+-----------+----------------+\n",
      "|2018-01-01 11:00:00|     u1|     1514784600|                NULL|          NULL|          0|          u1--S0|\n",
      "|2018-01-01 12:00:00|     u1|     1514788200|          1514784600|          60.0|          1|          u1--S1|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|          1514788200|          60.0|          1|          u1--S2|\n",
      "|2018-01-01 13:00:00|     u1|     1514791800|          1514791800|           0.0|          0|          u1--S2|\n",
      "|2018-01-01 14:00:00|     u1|     1514795400|          1514791800|          60.0|          1|          u1--S3|\n",
      "|2018-01-01 15:00:00|     u1|     1514799000|          1514795400|          60.0|          1|          u1--S4|\n",
      "|2018-01-01 11:00:00|     u2|     1514784600|                NULL|          NULL|          0|          u2--S0|\n",
      "|2018-01-02 11:00:00|     u2|     1514871000|          1514784600|        1440.0|          1|          u2--S1|\n",
      "+-------------------+-------+---------------+--------------------+--------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New session names \n",
    "\n",
    "df.withColumn(\"session_new_name\", concat(col('user_id'), lit(\"--S\"), sum(col('session_new')).over(window))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd52455",
   "metadata": {},
   "source": [
    "### Q2. Max Salary\n",
    "Ask is to find the job titles of the highest-paid employees. output should include the highest-paid title or multiple titles with the same salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8756839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+------------+-----------+\n",
      "|worker_id|first_name|last_name|salary|joining_date| department|\n",
      "+---------+----------+---------+------+------------+-----------+\n",
      "|        1|      John|      Doe| 10000|  2023-01-01|Engineering|\n",
      "|        2|      Jane|    Smith| 12000|  2022-12-01|  Marketing|\n",
      "|        3|     Alice|  Johnson| 12000|  2022-11-01|Engineering|\n",
      "+---------+----------+---------+------+------------+-----------+\n",
      "\n",
      "+-------------+------------+-------------+\n",
      "|worker_ref_id|worker_title|affected_from|\n",
      "+-------------+------------+-------------+\n",
      "|            1|    Engineer|   2022-01-01|\n",
      "|            2|     Manager|   2022-01-01|\n",
      "|            3|    Engineer|   2022-01-01|\n",
      "+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worker_data = [(1, 'John', 'Doe', 10000, '2023-01-01', 'Engineering'),\n",
    "        (2, 'Jane', 'Smith', 12000, '2022-12-01', 'Marketing'),\n",
    "        (3, 'Alice', 'Johnson', 12000, '2022-11-01', 'Engineering')]\n",
    "columns = ['worker_id', 'first_name', 'last_name', 'salary', 'joining_date', 'department']\n",
    "worker = spark.createDataFrame(worker_data, columns)\n",
    "worker.show()\n",
    "\n",
    "title_data = [(1, 'Engineer', '2022-01-01'),\n",
    "        (2, 'Manager', '2022-01-01'),\n",
    "        (3, 'Engineer', '2022-01-01')]\n",
    "columns = ['worker_ref_id', 'worker_title', 'affected_from']\n",
    "title = spark.createDataFrame(title_data, columns)\n",
    "title.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5009e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+\n",
      "|worker_id|first_name|last_name|salary|joining_date| department|worker_ref_id|worker_title|affected_from|\n",
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+\n",
      "|        1|      John|      Doe| 10000|  2023-01-01|Engineering|            1|    Engineer|   2022-01-01|\n",
      "|        2|      Jane|    Smith| 12000|  2022-12-01|  Marketing|            2|     Manager|   2022-01-01|\n",
      "|        3|     Alice|  Johnson| 12000|  2022-11-01|Engineering|            3|    Engineer|   2022-01-01|\n",
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = worker.join(title, worker.worker_id == title.worker_ref_id)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45a75f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+-----------+\n",
      "|worker_id|first_name|last_name|salary|joining_date| department|worker_ref_id|worker_title|affected_from|salary_rank|\n",
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+-----------+\n",
      "|        2|      Jane|    Smith| 12000|  2022-12-01|  Marketing|            2|     Manager|   2022-01-01|          1|\n",
      "|        3|     Alice|  Johnson| 12000|  2022-11-01|Engineering|            3|    Engineer|   2022-01-01|          1|\n",
      "|        1|      John|      Doe| 10000|  2023-01-01|Engineering|            1|    Engineer|   2022-01-01|          3|\n",
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rank_df = df.withColumn('salary_rank', rank().over(Window.orderBy(df.salary.desc())))\n",
    "rank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c959430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+-----------+\n",
      "|worker_id|first_name|last_name|salary|joining_date| department|worker_ref_id|worker_title|affected_from|salary_rank|\n",
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+-----------+\n",
      "|        2|      Jane|    Smith| 12000|  2022-12-01|  Marketing|            2|     Manager|   2022-01-01|          1|\n",
      "|        3|     Alice|  Johnson| 12000|  2022-11-01|Engineering|            3|    Engineer|   2022-01-01|          1|\n",
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highest_paid_df = rank_df.filter(rank_df['salary_rank'] == 1)\n",
    "highest_paid_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54ca1908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+---------------+------+\n",
      "|worker_id|first_name|last_name|best_paid_title|salary|\n",
      "+---------+----------+---------+---------------+------+\n",
      "|        2|      Jane|    Smith|        Manager| 12000|\n",
      "|        3|     Alice|  Johnson|       Engineer| 12000|\n",
      "+---------+----------+---------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = highest_paid_df.select('worker_id', 'first_name', 'last_name', 'worker_title', 'salary')\\\n",
    "                           .withColumnRenamed('worker_title', 'best_paid_title' )\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ff2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4471c618",
   "metadata": {},
   "source": [
    "### Q3. Highest and Lowest Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6d96440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+------------+-----------+\n",
      "|worker_id|first_name|last_name|salary|joining_date| department|\n",
      "+---------+----------+---------+------+------------+-----------+\n",
      "|        1|      John|      Doe|  5000|  2023-01-01|Engineering|\n",
      "|        2|      Jane|    Smith|  6000|  2023-01-15|  Marketing|\n",
      "|        3|     Alice|  Johnson|  4500|  2023-02-05|Engineering|\n",
      "+---------+----------+---------+------+------------+-----------+\n",
      "\n",
      "+-------------+------------+-------------+\n",
      "|worker_ref_id|worker_title|affected_from|\n",
      "+-------------+------------+-------------+\n",
      "|            1|    Engineer|   2022-01-01|\n",
      "|            2|     Manager|   2022-01-01|\n",
      "|            3|    Engineer|   2022-01-01|\n",
      "+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worker_data = [\n",
    "    (1, 'John', 'Doe', 5000, '2023-01-01', 'Engineering'),\n",
    "    (2, 'Jane', 'Smith', 6000, '2023-01-15', 'Marketing'),\n",
    "    (3, 'Alice', 'Johnson', 4500, '2023-02-05', 'Engineering')\n",
    "]\n",
    "title_data = [\n",
    "    (1, 'Engineer', '2022-01-01'),\n",
    "    (2, 'Manager', '2022-01-01'),\n",
    "    (3, 'Engineer', '2022-01-01')\n",
    "]\n",
    "worker_columns = ['worker_id', 'first_name', 'last_name', 'salary', 'joining_date', 'department']\n",
    "title_columns = ['worker_ref_id', 'worker_title', 'affected_from']\n",
    "worker_df = spark.createDataFrame(worker_data, worker_columns)\n",
    "title_df = spark.createDataFrame(title_data, title_columns)\n",
    "worker_df.show()\n",
    "title_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3e44679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+\n",
      "|worker_id|first_name|last_name|salary|joining_date| department|worker_ref_id|worker_title|affected_from|\n",
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+\n",
      "|        1|      John|      Doe| 10000|  2023-01-01|Engineering|            1|    Engineer|   2022-01-01|\n",
      "|        2|      Jane|    Smith| 12000|  2022-12-01|  Marketing|            2|     Manager|   2022-01-01|\n",
      "|        3|     Alice|  Johnson| 12000|  2022-11-01|Engineering|            3|    Engineer|   2022-01-01|\n",
      "+---------+----------+---------+------+------------+-----------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = worker.join(title, worker.worker_id == title.worker_ref_id, 'inner')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "754a40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+-----------+----------+----------+\n",
      "|worker_id|first_name|last_name|salary| department|max_salary|min_salary|\n",
      "+---------+----------+---------+------+-----------+----------+----------+\n",
      "|        1|      John|      Doe| 10000|Engineering|     10000|     10000|\n",
      "|        2|      Jane|    Smith| 12000|  Marketing|     12000|     12000|\n",
      "|        3|     Alice|  Johnson| 12000|Engineering|     12000|     12000|\n",
      "+---------+----------+---------+------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df.groupBy('worker_id','first_name','last_name',\"salary\", \"department\")\\\n",
    "              .agg(max('salary').alias('max_salary'),min('salary').alias('min_salary'))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afe7cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+-----------+----------+----------+--------------+\n",
      "|worker_id|first_name|last_name|salary| department|max_salary|min_salary|   salary_type|\n",
      "+---------+----------+---------+------+-----------+----------+----------+--------------+\n",
      "|        1|      John|      Doe| 10000|Engineering|     10000|     10000|Highest Salary|\n",
      "|        2|      Jane|    Smith| 12000|  Marketing|     12000|     12000|Highest Salary|\n",
      "|        3|     Alice|  Johnson| 12000|Engineering|     12000|     12000|Highest Salary|\n",
      "+---------+----------+---------+------+-----------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = result_df.withColumn('salary_type', \n",
    "                    when(result_df['salary'] == result_df['max_salary'], 'Highest Salary')\n",
    "                    .when(result_df['salary'] == result_df['min_salary'], 'Lowest Salary'))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0290ec",
   "metadata": {},
   "source": [
    "### Q4. UNPivot the Column into Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d346233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+------+------+\n",
      "|StudentID|StudentName|AScore|BScore|CScore|\n",
      "+---------+-----------+------+------+------+\n",
      "|      123|          A|    30|    31|    32|\n",
      "|      124|          B|    40|    41|    42|\n",
      "|      125|          B|    50|    51|    52|\n",
      "+---------+-----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (123, \"A\", 30, 31, 32),\n",
    "    (124, \"B\", 40, 41, 42),\n",
    "    (125, \"B\", 50, 51, 52)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"StudentID\", \"StudentName\", \"AScore\", \"BScore\", \"CScore\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4c1350f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------+-----+\n",
      "|StudentID|StudentName|Subject|Score|\n",
      "+---------+-----------+-------+-----+\n",
      "|      123|          A| AScore|   30|\n",
      "|      123|          A| BScore|   31|\n",
      "|      123|          A| CScore|   32|\n",
      "|      124|          B| AScore|   40|\n",
      "|      124|          B| BScore|   41|\n",
      "|      124|          B| CScore|   42|\n",
      "|      125|          B| AScore|   50|\n",
      "|      125|          B| BScore|   51|\n",
      "|      125|          B| CScore|   52|\n",
      "+---------+-----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpvt_exp = \"stack(3, 'AScore',AScore,'BScore',BScore, 'CScore', CScore) as (Subject, Score)\"\n",
    "unpvt_df = df.select('StudentID', 'StudentName', expr(unpvt_exp))\n",
    "unpvt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75e2b814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------+-----+\n",
      "|StudentID|StudentName|Subject|Score|\n",
      "+---------+-----------+-------+-----+\n",
      "|      123|          A| AScore|   30|\n",
      "|      123|          A| BScore|   31|\n",
      "|      123|          A| CScore|   32|\n",
      "|      124|          B| AScore|   40|\n",
      "|      124|          B| BScore|   41|\n",
      "|      124|          B| CScore|   42|\n",
      "|      125|          B| AScore|   50|\n",
      "|      125|          B| BScore|   51|\n",
      "|      125|          B| CScore|   52|\n",
      "+---------+-----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR\n",
    "pivot_df = df.selectExpr(\n",
    "    \"StudentID\",\n",
    "    \"StudentName\",\n",
    "    \"stack(3, 'AScore', AScore, 'BScore', BScore, 'CScore', CScore) as (Subject, Score)\"\n",
    ")\n",
    "\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4c36e",
   "metadata": {},
   "source": [
    "### Q5. Repeat IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0b95f52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,),(2,),(3,),], ['id'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da6f004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  1|\n",
      "|  2|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "odf = df.selectExpr('explode(sequence(1, id)) as id')\n",
    "odf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6824ba",
   "metadata": {},
   "source": [
    "### Q6. Group Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a569c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+\n",
      "| col1|col2|col3|\n",
      "+-----+----+----+\n",
      "|alpha|  aa|   1|\n",
      "|alpha|  aa|   2|\n",
      "| beta|  bb|   3|\n",
      "| beta|  bb|   5|\n",
      "| beta|  bb|   4|\n",
      "+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"alpha\", \"aa\", 1),\n",
    "        (\"alpha\", \"aa\", 2),\n",
    "        (\"beta\", \"bb\", 3),\n",
    "        (\"beta\", \"bb\", 5),\n",
    "        (\"beta\", \"bb\", 4)]\n",
    "schema = [\"col1\", \"col2\", \"col3\"]\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0d282f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "| col1|col2|col3_list|\n",
      "+-----+----+---------+\n",
      "|alpha|  aa|   [1, 2]|\n",
      "| beta|  bb|[3, 5, 4]|\n",
      "+-----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_grp = df.groupBy('col1', 'col2').agg(collect_list('col3').alias('col3_list'))\n",
    "df_grp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a7209",
   "metadata": {},
   "source": [
    "### Q8. Employee Total Hours Inside\n",
    "From the below data, find the total hours employee was inside office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "318254a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----+\n",
      "|emp_id|         punch_time|flag|\n",
      "+------+-------------------+----+\n",
      "| 11114|2024-06-01 08:30:00|   I|\n",
      "| 11114|2024-06-01 10:30:00|   O|\n",
      "| 11114|2024-06-01 11:30:00|   I|\n",
      "| 11114|2024-06-01 15:30:00|   O|\n",
      "| 11115|2024-06-01 09:30:00|   I|\n",
      "| 11115|2024-06-01 17:30:00|   O|\n",
      "+------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "data = [\n",
    "    (11114, datetime.datetime.strptime('2024-06-01 08:30:00.00', \"%Y-%m-%d %H:%M:%S.%f\"), \"I\"),\n",
    "    (11114, datetime.datetime.strptime('2024-06-01 10:30:00.00', \"%Y-%m-%d %H:%M:%S.%f\"), 'O'),\n",
    "    (11114, datetime.datetime.strptime('2024-06-01 11:30:00.00', \"%Y-%m-%d %H:%M:%S.%f\"), 'I'),\n",
    "    (11114, datetime.datetime.strptime('2024-06-01 15:30:00.00', \"%Y-%m-%d %H:%M:%S.%f\"), 'O'),\n",
    "    (11115, datetime.datetime.strptime('2024-06-01 09:30:00.00', \"%Y-%m-%d %H:%M:%S.%f\"), 'I'),\n",
    "    (11115, datetime.datetime.strptime('2024-06-01 17:30:00.00', \"%Y-%m-%d %H:%M:%S.%f\"), 'O')\n",
    "]\n",
    "schema = StructType([\n",
    "                    StructField('emp_id', LongType(), True),\n",
    "                    StructField('punch_time', TimestampType(), True),\n",
    "                    StructField('flag', StringType(), True)\n",
    "])\n",
    "df =  spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9a0f628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----+-------------------+\n",
      "|emp_id|         punch_time|flag|          prev_time|\n",
      "+------+-------------------+----+-------------------+\n",
      "| 11114|2024-06-01 08:30:00|   I|               NULL|\n",
      "| 11114|2024-06-01 10:30:00|   O|2024-06-01 08:30:00|\n",
      "| 11114|2024-06-01 11:30:00|   I|2024-06-01 10:30:00|\n",
      "| 11114|2024-06-01 15:30:00|   O|2024-06-01 11:30:00|\n",
      "| 11115|2024-06-01 09:30:00|   I|               NULL|\n",
      "| 11115|2024-06-01 17:30:00|   O|2024-06-01 09:30:00|\n",
      "+------+-------------------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy('emp_id').orderBy(col('punch_time'))\n",
    "df = df.withColumn('prev_time', lag(col('punch_time')).over(window))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca32de39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----+-------------------+---------+\n",
      "|emp_id|         punch_time|flag|          prev_time|time_diff|\n",
      "+------+-------------------+----+-------------------+---------+\n",
      "| 11114|2024-06-01 08:30:00|   I|               NULL|     NULL|\n",
      "| 11114|2024-06-01 10:30:00|   O|2024-06-01 08:30:00|      2.0|\n",
      "| 11114|2024-06-01 11:30:00|   I|2024-06-01 10:30:00|      1.0|\n",
      "| 11114|2024-06-01 15:30:00|   O|2024-06-01 11:30:00|      4.0|\n",
      "| 11115|2024-06-01 09:30:00|   I|               NULL|     NULL|\n",
      "| 11115|2024-06-01 17:30:00|   O|2024-06-01 09:30:00|      8.0|\n",
      "+------+-------------------+----+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('time_diff', (col('punch_time').cast('long') - col('prev_time').cast('long'))/3600)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2f2dc935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|emp_id|total_time|\n",
      "+------+----------+\n",
      "| 11114|       6.0|\n",
      "| 11115|       8.0|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.groupBy('emp_id').agg(sum(when(col('flag') == 'O', col('time_diff')).otherwise(0)).alias('total_time'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00baf1bc",
   "metadata": {},
   "source": [
    "### Q9. Employee with Manager\n",
    "From the given data set, Fetch the manager and their employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e48d91ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+\n",
      "|employee_id|first_name|manager_id|\n",
      "+-----------+----------+----------+\n",
      "|       4529|     Nancy|      4125|\n",
      "|       4238|      John|      4329|\n",
      "|       4329|   Martina|      4125|\n",
      "|       4009|     Klaus|      4329|\n",
      "|       4125|   Mafalda|      NULL|\n",
      "|       4500|     Jakub|      4529|\n",
      "|       4118|     Moira|      4952|\n",
      "|       4012|       Jon|      4952|\n",
      "|       4952|    Sandra|      4529|\n",
      "|       4444|    Seamus|      4329|\n",
      "+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('4529', 'Nancy', '4125'),\n",
    "('4238','John', '4329'),\n",
    "('4329', 'Martina', '4125'),\n",
    "('4009', 'Klaus', '4329'),\n",
    "('4125', 'Mafalda', 'NULL'),\n",
    "('4500', 'Jakub', '4529'),\n",
    "('4118', 'Moira', '4952'),\n",
    "('4012', 'Jon', '4952'),\n",
    "('4952', 'Sandra', '4529'),\n",
    "('4444', 'Seamus', '4329')]\n",
    "schema = ['employee_id', 'first_name', 'manager_id']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "91a4c3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+-----------+----------+----------+\n",
      "|employee_id|first_name|manager_id|employee_id|first_name|manager_id|\n",
      "+-----------+----------+----------+-----------+----------+----------+\n",
      "|       4529|     Nancy|      4125|       4125|   Mafalda|      NULL|\n",
      "|       4329|   Martina|      4125|       4125|   Mafalda|      NULL|\n",
      "|       4238|      John|      4329|       4329|   Martina|      4125|\n",
      "|       4009|     Klaus|      4329|       4329|   Martina|      4125|\n",
      "|       4444|    Seamus|      4329|       4329|   Martina|      4125|\n",
      "|       4500|     Jakub|      4529|       4529|     Nancy|      4125|\n",
      "|       4952|    Sandra|      4529|       4529|     Nancy|      4125|\n",
      "|       4118|     Moira|      4952|       4952|    Sandra|      4529|\n",
      "|       4012|       Jon|      4952|       4952|    Sandra|      4529|\n",
      "+-----------+----------+----------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.alias('e').join(df.alias('m'), col('e.manager_id')==col('m.employee_id'),'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c0e4bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+------------+\n",
      "|employee_id|first_name|manager_id|manager_name|\n",
      "+-----------+----------+----------+------------+\n",
      "|       4529|     Nancy|      4125|     Mafalda|\n",
      "|       4329|   Martina|      4125|     Mafalda|\n",
      "|       4238|      John|      4329|     Martina|\n",
      "|       4009|     Klaus|      4329|     Martina|\n",
      "|       4444|    Seamus|      4329|     Martina|\n",
      "|       4500|     Jakub|      4529|       Nancy|\n",
      "|       4952|    Sandra|      4529|       Nancy|\n",
      "|       4118|     Moira|      4952|      Sandra|\n",
      "|       4012|       Jon|      4952|      Sandra|\n",
      "+-----------+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdf = df.alias('e').join(df.alias('m'), col('e.manager_id')==col('m.employee_id'),'inner')\\\n",
    "       .select(col('e.employee_id'), col('e.first_name'), col('e.manager_id'),col('m.first_name').alias('manager_name'))\n",
    "rdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fdac75ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+\n",
      "|manager_id|manager_name|count|\n",
      "+----------+------------+-----+\n",
      "|      4125|     Mafalda|    2|\n",
      "|      4329|     Martina|    3|\n",
      "|      4529|       Nancy|    2|\n",
      "|      4952|      Sandra|    2|\n",
      "+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdf.groupBy('manager_id', 'manager_name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b3ef9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db441e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
