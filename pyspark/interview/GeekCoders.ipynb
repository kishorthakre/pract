{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e915d57",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=Rp5LWT4or-o&list=PLxy0DxWEupiODTF_xM5Lw1ghc0XtLCUhC&index=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d8b9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664b6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20b9b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>GC</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x19f25840050>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('GC').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd019809",
   "metadata": {},
   "source": [
    "### 13 Write a Pyspark query to report the movies with an odd-numbered ID and a description that is not 'boring'. Return the result table in descending order by rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c7a48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "|  2|   Science|    fiction|   8.5|\n",
      "|  3|     irish|     boring|   6.2|\n",
      "|  4|  Ice song|    Fantacy|   8.6|\n",
      "|  5|House card|Interesting|   9.1|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1, 'War','great 3D',8.9) \n",
    ",(2, 'Science','fiction',8.5) \n",
    ",(3, 'irish','boring',6.2)\n",
    ",(4, 'Ice song','Fantacy',8.6)  \n",
    ",(5, 'House card','Interesting',9.1)]   \n",
    "schema=\"ID int,movie string,description string,rating double\"\n",
    "df=spark.createDataFrame(data,schema) \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45502368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((col('ID') % 2 != 0 ) & (trim(lower(col('description'))) != 'boring')).orderBy(col('rating').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f123f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR \n",
    "df.select('*').filter((col('ID')%2!=0) & (trim(lower(col('description'))) != 'boring')).orderBy(col('rating').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae162506",
   "metadata": {},
   "source": [
    "### 14 Collect_list and Aggregation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25babeb1",
   "metadata": {},
   "source": [
    "john    |  [{tomato, 5}, {𝚋𝚊𝚗𝚊𝚗𝚊, 2}]\n",
    "𝚋𝚒𝚕𝚕    |  [{𝚊𝚙𝚙𝚕𝚎, 4}, {𝚝𝚊𝚌𝚘, 2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82cad3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+\n",
      "|name|  item|weight|\n",
      "+----+------+------+\n",
      "|john|tomato|     2|\n",
      "|bill| apple|     2|\n",
      "|john|banana|     2|\n",
      "|john|tomato|     3|\n",
      "|bill|  taco|     2|\n",
      "|bill| apple|     2|\n",
      "+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"john\", \"tomato\", 2),\n",
    "    (\"bill\", \"apple\", 2),\n",
    "    (\"john\", \"banana\", 2),\n",
    "    (\"john\", \"tomato\", 3),\n",
    "    (\"bill\", \"taco\", 2),\n",
    "    (\"bill\", \"apple\", 2),\n",
    "]\n",
    "schema = \"name string,item string,weight int\"\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f8b615b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----------+\n",
      "|name|  item|sum_weight|\n",
      "+----+------+----------+\n",
      "|john|tomato|         5|\n",
      "|bill| apple|         4|\n",
      "|john|banana|         2|\n",
      "|bill|  taco|         2|\n",
      "+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final =df.groupBy('name', 'item').agg(sum(col('weight')).alias('sum_weight'))\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a0f6bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------------+\n",
      "|name|items                     |\n",
      "+----+--------------------------+\n",
      "|bill|[{apple, 4}, {taco, 2}]   |\n",
      "|john|[{tomato, 5}, {banana, 2}]|\n",
      "+----+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fi = df_final.groupBy('name').agg(collect_list(struct('item', 'sum_weight')).alias('items')).orderBy('name')\n",
    "df_fi.show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252e9ca",
   "metadata": {},
   "source": [
    "### 15 find all duplicate emails\n",
    "Write a pyspark dataframe query to find all duplicate emails in a table named Person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e86ad4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| ID|        email|\n",
      "+---+-------------+\n",
      "|  1|abc@gmail.com|\n",
      "|  2|bcd@gmail.com|\n",
      "|  3|abc@gmail.com|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"abc@gmail.com\"), (2, \"bcd@gmail.com\"), (3, \"abc@gmail.com\")]\n",
    "schema = \"ID int,email string\"\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd702b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        email|\n",
      "+-------------+\n",
      "|abc@gmail.com|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('email').count().filter(col('count')>1).select('email').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e969cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        email|\n",
      "+-------------+\n",
      "|abc@gmail.com|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('test')\n",
    "spark.sql(\"\"\"select email from test group by email having count(email)>1\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d71693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9484f1e2",
   "metadata": {},
   "source": [
    "### 18 Write a pyspark query for a report that provides the customer ids from the customer table that bought all the products in the product table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb38aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|product_key|\n",
      "+-----------+-----------+\n",
      "|          1|          5|\n",
      "|          2|          6|\n",
      "|          3|          5|\n",
      "|          3|          6|\n",
      "|          1|          6|\n",
      "+-----------+-----------+\n",
      "\n",
      "+-----------+\n",
      "|product_key|\n",
      "+-----------+\n",
      "|          5|\n",
      "|          6|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,5),(2,6),(3,5),(3,6),(1,6)]\n",
    "schema=\"customer_id int,product_key int\"\n",
    "customer_df=spark.createDataFrame(data,schema)\n",
    "\n",
    "data=[(5,),(6,)]\n",
    "schema=\"product_key int\"\n",
    "product_df=spark.createDataFrame(data,schema)\n",
    "\n",
    "customer_df.show()\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "815cb6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|cnt_products|\n",
      "+------------+\n",
      "|           2|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take unique key from product table \n",
    "df_product = product_df.agg(countDistinct(col('product_key')).alias('cnt_products'))\n",
    "df_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f192fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|cnt_products|\n",
      "+-----------+------------+\n",
      "|          1|           2|\n",
      "|          3|           2|\n",
      "|          2|           1|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer = customer_df.groupBy(col('customer_id')).agg(countDistinct(col('product_key')).alias('cnt_products'))\n",
    "df_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb43ebe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_product.join(df_customer, df_product.cnt_products == df_customer.cnt_products, 'inner').select('customer_id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a244d",
   "metadata": {},
   "source": [
    "### 19 . Get the employees dept id with maximun and minimum salary in each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "379adab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+\n",
      "| emp_name|dept_id|salary|\n",
      "+---------+-------+------+\n",
      "|   Genece|      2| 75000|\n",
      "|   Jaimin|      2| 80000|\n",
      "|   Pankaj|      2| 80000|\n",
      "| Tarvares|      2| 70000|\n",
      "| Marlania|      4| 70000|\n",
      "|   Briana|      4| 85000|\n",
      "|  Kimberi|      4| 55000|\n",
      "|Gabriella|      4| 55000|\n",
      "|   Lakken|      5| 60000|\n",
      "| Latoynia|      5| 65000|\n",
      "+---------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[('Genece' , 2 , 75000),\n",
    "('Jaimin' , 2 , 80000 ),\n",
    "('Pankaj' , 2 , 80000 ),\n",
    "('Tarvares' , 2 , 70000),\n",
    "('Marlania' , 4 , 70000),\n",
    "('Briana' , 4 , 85000),\n",
    "('Kimberi' , 4 , 55000),\n",
    "('Gabriella' , 4 , 55000),  \n",
    "('Lakken', 5, 60000),\n",
    "('Latoynia' , 5 , 65000) ]\n",
    "schema=\"emp_name string,dept_id int,salary int\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d5b403d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+\n",
      "|dept_id_1|max_salary|min_salary|\n",
      "+---------+----------+----------+\n",
      "|        2|     80000|     70000|\n",
      "|        4|     85000|     55000|\n",
      "|        5|     65000|     60000|\n",
      "+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = df.groupBy('dept_id').agg(max('salary').alias('max_salary'), min('salary').alias('min_salary')).withColumnRenamed('dept_id', 'dept_id_1')\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5bf1c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+---------+----------+----------+\n",
      "| emp_name|dept_id|salary|dept_id_1|max_salary|min_salary|\n",
      "+---------+-------+------+---------+----------+----------+\n",
      "|   Genece|      2| 75000|        2|     80000|     70000|\n",
      "|   Jaimin|      2| 80000|        2|     80000|     70000|\n",
      "|   Pankaj|      2| 80000|        2|     80000|     70000|\n",
      "| Marlania|      4| 70000|        4|     85000|     55000|\n",
      "| Tarvares|      2| 70000|        2|     80000|     70000|\n",
      "|   Briana|      4| 85000|        4|     85000|     55000|\n",
      "|  Kimberi|      4| 55000|        4|     85000|     55000|\n",
      "|Gabriella|      4| 55000|        4|     85000|     55000|\n",
      "|   Lakken|      5| 60000|        5|     65000|     60000|\n",
      "| Latoynia|      5| 65000|        5|     65000|     60000|\n",
      "+---------+-------+------+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = df.join(df_1, df.dept_id == df_1.dept_id_1, 'inner')\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6eb2a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+---------+----------+----------+\n",
      "| emp_name|dept_id|salary|dept_id_1|max_salary|min_salary|\n",
      "+---------+-------+------+---------+----------+----------+\n",
      "|   Jaimin|      2| 80000|        2|     80000|     70000|\n",
      "|   Pankaj|      2| 80000|        2|     80000|     70000|\n",
      "| Tarvares|      2| 70000|        2|     80000|     70000|\n",
      "|   Briana|      4| 85000|        4|     85000|     55000|\n",
      "|  Kimberi|      4| 55000|        4|     85000|     55000|\n",
      "|Gabriella|      4| 55000|        4|     85000|     55000|\n",
      "|   Lakken|      5| 60000|        5|     65000|     60000|\n",
      "| Latoynia|      5| 65000|        5|     65000|     60000|\n",
      "+---------+-------+------+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3 = df_2.filter((col('max_salary') == col('salary')) | (col('min_salary') == col('salary')))\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f60c858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+\n",
      "|dept_id|salary|           emp_names|\n",
      "+-------+------+--------------------+\n",
      "|      2| 80000|    [Jaimin, Pankaj]|\n",
      "|      2| 70000|          [Tarvares]|\n",
      "|      4| 85000|            [Briana]|\n",
      "|      4| 55000|[Kimberi, Gabriella]|\n",
      "|      5| 60000|            [Lakken]|\n",
      "|      5| 65000|          [Latoynia]|\n",
      "+-------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f = df_3.groupBy('dept_id', 'salary').agg(collect_list('emp_name').alias('emp_names'))\n",
    "df_f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82970a04",
   "metadata": {},
   "source": [
    "### 20 . Cache and Persist DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa0ac962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|Firstname|Lastname| City|\n",
      "+---------+--------+-----+\n",
      "|      Raj|     Doe| NULL|\n",
      "|     NULL|  Samuel|VIZAG|\n",
      "|    David|   Smith| NULL|\n",
      "|   Samson|    NULL|  HYD|\n",
      "|     Immi|   Steve|  BNG|\n",
      "|     NULL|    NULL| NULL|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Raj\",\"Doe\",None),\n",
    " (None,\"Samuel\",\"VIZAG\"),\n",
    " (\"David\",\"Smith\", None),\n",
    " (\"Samson\",None, \"HYD\"),\n",
    " (\"Immi\", \"Steve\", \"BNG\"),\n",
    " (None, None, None)]\n",
    "\n",
    "columns = [\"Firstname\", \"Lastname\", \"City\"]\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4c9f05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firstname 33.33333333333333\n",
      "Lastname 33.33333333333333\n",
      "City 50.0\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    total_count = df.select(col(i)).count()\n",
    "    null_values = df.filter(col(i).isNull()).count()\n",
    "    percentage = (null_values/total_count)*100\n",
    "    print(i, percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120d296",
   "metadata": {},
   "source": [
    "### 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb56da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "| name|            Hobbies|\n",
      "+-----+-------------------+\n",
      "|Alice|[Badminton, Tennis]|\n",
      "|  Bob|  [Tennis, Cricket]|\n",
      "|Julie| [Cricket, Carroms]|\n",
      "+-----+-------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- Hobbies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Alice' ,['Badminton', 'Tennis']),\n",
    "        ('Bob', ['Tennis', 'Cricket']),\n",
    "        ('Julie', ['Cricket','Carroms'])]\n",
    "schema = ['name', 'Hobbies']\n",
    "df = spark.createDataFrame(data=data, schema= schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a201231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| name|      col|\n",
      "+-----+---------+\n",
      "|Alice|Badminton|\n",
      "|Alice|   Tennis|\n",
      "|  Bob|   Tennis|\n",
      "|  Bob|  Cricket|\n",
      "|Julie|  Cricket|\n",
      "|Julie|  Carroms|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('name'), explode(col('Hobbies'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a75b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|City1|City2|City3|\n",
      "+-----+-----+-----+\n",
      "|  Goa|     |   AP|\n",
      "|     |   AP| NULL|\n",
      "| NULL|     |  Blr|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Goa','','AP'), ('','AP',None),(None, '','Blr')]\n",
    "schema = 'City1 string, City2 string, City3 string'\n",
    "df2 = spark.createDataFrame(data, schema)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99949baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|City1|City2|City3|\n",
      "+-----+-----+-----+\n",
      "|  Goa| NULL|   AP|\n",
      "| NULL|   AP| NULL|\n",
      "| NULL| NULL|  Blr|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = df2.withColumn('City1', when(col('City1')=='',None).otherwise(col('City1')))\\\n",
    "        .withColumn('City2', when(col('City2')=='',None).otherwise(col('City2')))\\\n",
    "        .withColumn('City3', when(col('City3')=='',None).otherwise(col('City3')))\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12012b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Result|\n",
      "+------+\n",
      "|   Goa|\n",
      "|    AP|\n",
      "|   Blr|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff= df_2.withColumn('Result', coalesce(col('City1'), col('City2'), col('City3'))).select('Result')\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e73ad44",
   "metadata": {},
   "source": [
    "### 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85a8a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------+\n",
      "|name      |address                                      |\n",
      "+----------+---------------------------------------------+\n",
      "|John Doe  |{\"street\": \"123 Main St\", \"city\": \"Anytown\"} |\n",
      "|Jane Smith|{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}|\n",
      "+----------+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[('John Doe','{\"street\": \"123 Main St\", \"city\": \"Anytown\"}'),('Jane Smith','{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}')]\n",
    "df=spark.createDataFrame(data,schema=\"name string,address string\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec529d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------+-----------------------+\n",
      "|name      |address                                      |parsed_json            |\n",
      "+----------+---------------------------------------------+-----------------------+\n",
      "|John Doe  |{\"street\": \"123 Main St\", \"city\": \"Anytown\"} |{123 Main St, Anytown} |\n",
      "|Jane Smith|{\"street\": \"456 Elm St\", \"city\": \"Othertown\"}|{456 Elm St, Othertown}|\n",
      "+----------+---------------------------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('parsed_json', from_json(col('address'),'street string, city string'))\n",
    "df1.show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d488928",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.withColumn() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mwithColumn(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m), col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_json\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstreet\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreet\u001b[39m\u001b[38;5;124m'\u001b[39m),  col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_json\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcity\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      2\u001b[0m df2\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mTypeError\u001b[0m: DataFrame.withColumn() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(col('name'), col('parsed_json').street.alias('street'),  col('parsed_json').city.alias('city'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b2aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99bcace1",
   "metadata": {},
   "source": [
    "### 27"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14c9295f",
   "metadata": {},
   "source": [
    "Python globals() function is a build-in function in Python that returns the dictionary of the current global symbol table. \n",
    "\n",
    "Symbol table: A symbol table is a data structure that contains all necessary information about the program. These include variable names, methods, classes, etc. The global symbol table stores all information related to the program's global scope and is accessed in Python using the globals method."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d90b88da",
   "metadata": {},
   "source": [
    "1. get all the dataframe asscoiated to this notebook / spark session\n",
    "2. display all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9d0c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| Id|Name|\n",
      "+---+----+\n",
      "|  1|   A|\n",
      "|  2|   B|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=[(1,'A'), (2,'B')], schema=\"Id int, Name string\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a1c4b98",
   "metadata": {},
   "source": [
    "for i, j in globals().items():\n",
    "    if isinstance(j, DataFrame):\n",
    "        print(j.show())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43ad0deb",
   "metadata": {},
   "source": [
    "#print(globals().items())\n",
    "for i, j in globals().items():\n",
    "    if (type(j)==DataFrame):\n",
    "        print(i)\n",
    "        print(j.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49594d8",
   "metadata": {},
   "source": [
    "### 28 Find missing Numbers in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "971fd587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| Id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [ (1, ), (2,), (3,), (6,), (7,), (8,)]\n",
    "schema=\"Id int\"\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c4f5539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list = df.select(min(col('Id')), max(col('Id')))\n",
    "df_new = spark.range(df_list.first()[0], df_list.first()[1]+1)\n",
    "df_new.subtract(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe8c04",
   "metadata": {},
   "source": [
    "### 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7195b493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| Name| Sub|Marks|\n",
      "+-----+----+-----+\n",
      "|Rudra|math|   79|\n",
      "|Rudra| eng|   60|\n",
      "|Shivu|math|   68|\n",
      "|Shivu| eng|   59|\n",
      "|  Anu|math|   65|\n",
      "|  Anu| eng|   80|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[\n",
    "('Rudra','math',79),\n",
    "('Rudra','eng',60),\n",
    "('Shivu','math', 68),\n",
    "('Shivu','eng', 59),\n",
    "('Anu','math', 65),\n",
    "('Anu','eng',80)\n",
    "]\n",
    "schema=\"Name string,Sub string,Marks int\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce42dfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "| Name|eng|math|\n",
      "+-----+---+----+\n",
      "|Shivu| 59|  68|\n",
      "|Rudra| 60|  79|\n",
      "|  Anu| 80|  65|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('Name').pivot('Sub').sum('Marks')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15ab73ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| Name|Marks_New|\n",
      "+-----+---------+\n",
      "|Rudra| [79, 60]|\n",
      "|Shivu| [68, 59]|\n",
      "|  Anu| [65, 80]|\n",
      "+-----+---------+\n",
      "\n",
      "+-----+----+---+\n",
      "| Name|math|eng|\n",
      "+-----+----+---+\n",
      "|Rudra|  79| 60|\n",
      "|Shivu|  68| 59|\n",
      "|  Anu|  65| 80|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR \n",
    "df2 = df.groupBy(col('Name')).agg(collect_list(col(\"Marks\")).alias(\"Marks_New\"))\n",
    "df2.show()\n",
    "df3 = df2.select(col('Name'), col('Marks_New')[0].alias('math'),col('Marks_New')[1].alias('eng'))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e25494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357f669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffe9cc70",
   "metadata": {},
   "source": [
    "### 43 Write a Pyspark code to get all the customer ID along with their final balance amount after calculating their transactions based on transaction type.\n",
    "If any customer has not any transactions then his final balance should remain same as current amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "375dd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------------------+\n",
      "|customer_id|transaction_type|transaction_amount|\n",
      "+-----------+----------------+------------------+\n",
      "|          1|          credit|              30.0|\n",
      "|          1|           debit|              90.0|\n",
      "|          2|          credit|              50.0|\n",
      "|          3|           debit|              57.0|\n",
      "|          2|           debit|              90.0|\n",
      "+-----------+----------------+------------------+\n",
      "\n",
      "+-----------+--------------+\n",
      "|customer_id|current_amount|\n",
      "+-----------+--------------+\n",
      "|          1|        1000.0|\n",
      "|          2|        2000.0|\n",
      "|          3|        3000.0|\n",
      "|          4|        4000.0|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_data = [\n",
    "    (1, \"credit\", 30.0),\n",
    "    (1, \"debit\", 90.0),\n",
    "    (2, \"credit\", 50.0),\n",
    "    (3, \"debit\", 57.0),\n",
    "    (2, \"debit\", 90.0)]\n",
    "transaction_schema = StructType([\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField(\"transaction_type\", StringType(), True),\n",
    "    StructField(\"transaction_amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "trans_df = spark.createDataFrame(transactions_data, schema=transaction_schema)\n",
    "trans_df.show()\n",
    "\n",
    "amounts_data = [\n",
    "    (1, 1000.0),\n",
    "    (2, 2000.0),\n",
    "    (3, 3000.0),\n",
    "    (4, 4000.0)]\n",
    "amount_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"current_amount\", FloatType(), True)])\n",
    "\n",
    "amt_df = spark.createDataFrame(amounts_data, schema=amount_schema)\n",
    "amt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae0b5578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+--------------------+\n",
      "|customer_id|total_credit|total_debit|total_remaining_amnt|\n",
      "+-----------+------------+-----------+--------------------+\n",
      "|          1|        30.0|       90.0|                60.0|\n",
      "|          2|        50.0|       90.0|                40.0|\n",
      "|          3|         0.0|       57.0|                57.0|\n",
      "+-----------+------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = trans_df.groupBy('customer_id').agg(sum(when(col('transaction_type') == 'credit', col('transaction_amount'))\\\n",
    "                                               .otherwise(0)).alias('total_credit'),\\\n",
    "                                           sum(when(col('transaction_type') == 'debit', col('transaction_amount'))\\\n",
    "                                               .otherwise(0)).alias('total_debit'))\\\n",
    "                                      .withColumn('total_remaining_amnt', col('total_debit')-col('total_credit'))\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20403e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|current_amount|\n",
      "+-----------+--------------+\n",
      "|          1|         940.0|\n",
      "|          2|        1960.0|\n",
      "|          3|        2943.0|\n",
      "|          4|        4000.0|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = amt_df.join(df_1, amt_df.customer_id == df_1.customer_id, how='left')\\\n",
    "                          .withColumn('total_remaining_amnt', coalesce(col('total_remaining_amnt'), lit(0)))\\\n",
    "                          .withColumn('current_amount', col('current_amount') - col('total_remaining_amnt'))\\\n",
    "                          .drop(df_1.customer_id)\\\n",
    "                          .select('customer_id', 'current_amount')\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4792cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcbd90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
