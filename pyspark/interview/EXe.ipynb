{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f048250a",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/pyspark/pyspark-exercises-101-pyspark-exercises-for-data-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ade8e",
   "metadata": {},
   "source": [
    "### 1. How to import PySpark and check the version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4ef35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680d022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08c321be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Ex').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac06275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc56f9",
   "metadata": {},
   "source": [
    "### 57. How to View PySpark Cluster Details?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc5b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://LAPTOP-3V2ROQ70:4040\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878bbd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81da5702",
   "metadata": {},
   "source": [
    "### Create empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdda82d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty RDD by using emptyRDD() \n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "print(emptyRDD.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73947b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038d6bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates Empty RDD using parallelize\n",
    "rdd22 = spark.sparkContext.parallelize([])\n",
    "rdd22.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b57ed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Empty DataFrame without Schema (no columns) \n",
    "\n",
    "df = spark.createDataFrame([], StructType([]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fba987ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Empty DataFrame with Schema (StructType)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType \n",
    "schema = StructType([\n",
    "    StructField('firstname', StringType(), True),\n",
    "    StructField('middlename', StringType(), True),\n",
    "    StructField('lastname', StringType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(emptyRDD, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "789c0af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Convert Empty RDD to DataFrame\n",
    "\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7513283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Create Empty DataFrame with Schema. \n",
    "df2 = spark.createDataFrame([], schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713a1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b0cdb27",
   "metadata": {},
   "source": [
    "### 2. How to convert the index of a PySpark DataFrame into a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92826062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|  Alice|    1|\n",
      "|    Bob|    2|\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)],\n",
    "                            [\"Name\", \"Value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12de124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5086eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|   Name|Value|index|\n",
      "+-------+-----+-----+\n",
      "|  Alice|    1|    0|\n",
      "|    Bob|    2|    1|\n",
      "|Charlie|    3|    2|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "df.withColumn('index', row_number().over(window) - 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52dc8c",
   "metadata": {},
   "source": [
    "### 3. How to combine many lists to form a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f402e402",
   "metadata": {},
   "source": [
    "Create a PySpark DataFrame from list1 and list2\n",
    "\n",
    "Hint: For Creating DataFrame from multiple lists, first create an RDD (Resilient Distributed Dataset) from those lists and then convert the RDD to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb704a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(list(zip(list1, list2)))\n",
    "df = rdd.toDF(['col1', 'col2'])\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "823ffb0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "|   c|   3|\n",
      "|   d|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(list(zip(list1, list2)))\n",
    "df = rdd.toDF(['col1','col2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f873db4",
   "metadata": {},
   "source": [
    "### 4. How to get the items of list A not present in list B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "110cb00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd1 = sc.parallelize(list_A)\n",
    "rdd2 = sc.parallelize(list_B)\n",
    "\n",
    "rdd = rdd1.subtract(rdd2)\n",
    "\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd79973",
   "metadata": {},
   "source": [
    "### 5. How to get the items not common to both list A and list B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb10ebbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "\n",
    "sc = spark.sparkContext\n",
    "rdd1 = sc.parallelize(list_A)\n",
    "rdd2 = sc.parallelize(list_B)\n",
    "\n",
    "r1 = rdd1.subtract(rdd2)\n",
    "r2 = rdd2.subtract(rdd1)\n",
    "\n",
    "r = r1.union(r2)\n",
    "rdd = r.collect()\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75dc62e",
   "metadata": {},
   "source": [
    "### 6. How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e2eca08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, ['Name', 'Age'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5df62330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0, 20.0, 30.0, 50.0, 86.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles = df.approxQuantile('Age', [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d28038e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  10.0\n",
      "25th percentile:  20.0\n",
      "Median:  30.0\n",
      "75th percentile:  50.0\n",
      "Max:  86.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \", quantiles[0])\n",
    "print(\"25th percentile: \", quantiles[1])\n",
    "print(\"Median: \", quantiles[2])\n",
    "print(\"75th percentile: \", quantiles[3])\n",
    "print(\"Max: \", quantiles[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b9387",
   "metadata": {},
   "source": [
    "### 7. How to get frequency counts of unique items of a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce111b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "687c059c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|   Doctor|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(name='John', job='Engineer'),\n",
    "    Row(name='John', job='Engineer'),\n",
    "    Row(name='Mary', job='Scientist'),\n",
    "    Row(name='Bob', job='Engineer'),\n",
    "    Row(name='Bob', job='Engineer'),\n",
    "    Row(name='Bob', job='Scientist'),\n",
    "    Row(name='Sam', job='Doctor')\n",
    "]\n",
    "df= spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21e8b08b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "| Engineer|    4|\n",
      "|Scientist|    2|\n",
      "|   Doctor|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('job').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0032883",
   "metadata": {},
   "source": [
    "### 8. How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e24a9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06520df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Engineer', 'Scientist']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = df.groupBy('job').count().orderBy('count', ascending= False).limit(2)\n",
    "t = t1.select('job').rdd.flatMap(lambda x:x).collect()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "040a816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|    Other|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('job', when(col('job').isin(t), col('job')).otherwise('Other'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b575b8",
   "metadata": {},
   "source": [
    "### 9. How to Drop rows with NA values specific to a particular column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3716d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df06fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B|    3| 456|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=['Value']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac8061",
   "metadata": {},
   "source": [
    "### 10. How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12177f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4af02ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|new_col1|new_col2|new_col3|\n",
      "+--------+--------+--------+\n",
      "|       1|       2|       3|\n",
      "|       4|       5|       6|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.toDF(*new_names).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb71e862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|new_col1|new_col2|new_col3|\n",
      "+--------+--------+--------+\n",
      "|       1|       2|       3|\n",
      "|       4|       5|       6|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for old_names, new_names in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old_names, new_names)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e544d1",
   "metadata": {},
   "source": [
    "### 11. print random number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f50e8934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8135d9bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              values|\n",
      "+--------------------+\n",
      "|  0.6215324217275521|\n",
      "|  0.9620160229965483|\n",
      "|  0.5762688869088491|\n",
      "|2.594845203433005E-4|\n",
      "|  0.5690733699843449|\n",
      "|   0.531704931092798|\n",
      "|  0.2281446039677244|\n",
      "|  0.6206951757453757|\n",
      "|  0.8652795522695643|\n",
      "| 0.12055456066418213|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nitem=10\n",
    "df = spark.range(nitem).select(rand().alias('values'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79da476",
   "metadata": {},
   "source": [
    "### 13. How to find the numbers that are multiples of 3 from a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "329fb603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      "\n",
      "+---+------+\n",
      "| id|random|\n",
      "+---+------+\n",
      "|  0|     7|\n",
      "|  1|     9|\n",
      "|  2|     8|\n",
      "|  3|     8|\n",
      "|  4|     3|\n",
      "|  5|     1|\n",
      "|  6|     7|\n",
      "|  7|     4|\n",
      "|  8|     5|\n",
      "|  9|     1|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- random: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "df.printSchema()\n",
    "df = df.withColumn('random', ((rand(42)*10)+1).cast('int'))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea41d7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+\n",
      "| id|random|is_multiple_of_3|\n",
      "+---+------+----------------+\n",
      "|  0|     7|              No|\n",
      "|  1|     9|             yes|\n",
      "|  2|     8|              No|\n",
      "|  3|     8|              No|\n",
      "|  4|     3|             yes|\n",
      "|  5|     1|              No|\n",
      "|  6|     7|              No|\n",
      "|  7|     4|              No|\n",
      "|  8|     5|              No|\n",
      "|  9|     1|              No|\n",
      "+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('is_multiple_of_3', when(col('random') %3 == 0, \"yes\").otherwise('No')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73fcbc",
   "metadata": {},
   "source": [
    "### 14. How to extract items at given positions from a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc5d5e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| id|random|index|\n",
      "+---+------+-----+\n",
      "|  0|     7|    0|\n",
      "|  4|     3|    4|\n",
      "|  5|     1|    5|\n",
      "|  8|     5|    8|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos = [0, 4, 8, 5] \n",
    "\n",
    "df = spark.range(10)\n",
    "df = df.withColumn('random', ((rand(42)*10)+1).cast('int'))\n",
    "\n",
    "# add index\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "df = df.withColumn('index', row_number().over(w)-1)\n",
    "\n",
    "# Filter the DF based on the specified positions\n",
    "df_f = df.filter(col('index').isin(pos))\n",
    "df_f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bbab5",
   "metadata": {},
   "source": [
    "### 15. How to stack two DataFrames vertically ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14882985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "+------+-----+-----+\n",
      "\n",
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_3|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_A.show()\n",
    "\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "df_B.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bf829f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_A.union(df_B).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341323e1",
   "metadata": {},
   "source": [
    "### 17. How to convert the first character of each element in a series to uppercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05806bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| john|\n",
      "|alice|\n",
      "|  bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcd7d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| JOHN|\n",
      "|ALICE|\n",
      "|  BOB|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| John|\n",
      "|Alice|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('name', upper(col('name'))).show()\n",
    "\n",
    "# Convert the first character to uppercase\n",
    "df.withColumn('name', initcap(col('name'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf967d",
   "metadata": {},
   "source": [
    "### 18. How to compute summary statistics for all columns in a dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d7065bd",
   "metadata": {},
   "source": [
    ".describe() function takes cols:String*(columns in df) as optional args. (create new dataframe)\n",
    "\n",
    ".summary() function takes statistics:String*(count,mean,stddev..etc) as optional args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a02a1d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  James| 34| 55000|\n",
      "|Michael| 30| 70000|\n",
      "| Robert| 37| 60000|\n",
      "|  Maria| 29| 80000|\n",
      "|    Jen| 32| 65000|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bf8c60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+-----------------+\n",
      "|summary|  name|              age|           salary|\n",
      "+-------+------+-----------------+-----------------+\n",
      "|  count|     5|                5|                5|\n",
      "|   mean|  NULL|             32.4|          66000.0|\n",
      "| stddev|  NULL|3.209361307176242|9617.692030835675|\n",
      "|    min| James|               29|            55000|\n",
      "|    max|Robert|               37|            80000|\n",
      "+-------+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6fbc48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+-----------------+\n",
      "|summary|  name|              age|           salary|\n",
      "+-------+------+-----------------+-----------------+\n",
      "|  count|     5|                5|                5|\n",
      "|   mean|  NULL|             32.4|          66000.0|\n",
      "| stddev|  NULL|3.209361307176242|9617.692030835675|\n",
      "|    min| James|               29|            55000|\n",
      "|    25%|  NULL|               30|            60000|\n",
      "|    50%|  NULL|               32|            65000|\n",
      "|    75%|  NULL|               34|            70000|\n",
      "|    max|Robert|               37|            80000|\n",
      "+-------+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee9e32",
   "metadata": {},
   "source": [
    "### 19. How to calculate the number of characters in each word in a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50dc8336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| john|\n",
      "|alice|\n",
      "|  bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7140dc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| name|word_length|\n",
      "+-----+-----------+\n",
      "| john|          4|\n",
      "|alice|          5|\n",
      "|  bob|          3|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('word_length', length(df.name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44583743",
   "metadata": {},
   "source": [
    "### 20 How to compute difference of differences between consecutive numbers of a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e59267db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  James| 34| 55000|\n",
      "|Michael| 30| 70000|\n",
      "| Robert| 37| 60000|\n",
      "|  Maria| 29| 80000|\n",
      "|    Jen| 32| 65000|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfd21a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+-----------+----------+\n",
      "|   name|age|salary|         id|prev_value|\n",
      "+-------+---+------+-----------+----------+\n",
      "|  James| 34| 55000| 8589934592|      NULL|\n",
      "|Michael| 30| 70000|25769803776|     55000|\n",
      "| Robert| 37| 60000|34359738368|     70000|\n",
      "|  Maria| 29| 80000|51539607552|     60000|\n",
      "|    Jen| 32| 65000|60129542144|     80000|\n",
      "+-------+---+------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "window = Window.orderBy('id')\n",
    "\n",
    "# Generate the lag of the variable \n",
    "df = df.withColumn('prev_value', lag(df.salary).over(window))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76df4115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+------+\n",
      "|   name|age|salary|prev_value|  diff|\n",
      "+-------+---+------+----------+------+\n",
      "|  James| 34| 55000|      NULL|     0|\n",
      "|Michael| 30| 70000|     55000| 15000|\n",
      "| Robert| 37| 60000|     70000|-10000|\n",
      "|  Maria| 29| 80000|     60000| 20000|\n",
      "|    Jen| 32| 65000|     80000|-15000|\n",
      "+-------+---+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the difference with lag \n",
    "df.withColumn('diff', when(isnull(df.salary - df.prev_value), 0).otherwise(df.salary-df.prev_value)).drop('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f00fc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|(salary - prev_value)|\n",
      "+---------------------+\n",
      "|                 NULL|\n",
      "|                15000|\n",
      "|               -10000|\n",
      "|                20000|\n",
      "|               -15000|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.salary - df.prev_value).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0957fb3",
   "metadata": {},
   "source": [
    "### 21. How to get the \n",
    "### day of month,  week number, day of year and  day of week from a date strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "676595bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|date_str_1| date_str_2|\n",
      "+----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|\n",
      "|2023-12-31|01 Jan 2010|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01fa5dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+----------+\n",
      "|date_str_1| date_str_2|    date_1|    date_2|\n",
      "+----------+-----------+----------+----------+\n",
      "|2023-05-18|01 Jan 2010|2023-05-18|2010-01-01|\n",
      "|2023-12-31|01 Jan 2010|2023-12-31|2010-01-01|\n",
      "+----------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert date string to date format\n",
    "df = df.withColumn('date_1', to_date(df.date_str_1, 'yyyy-MM-dd'))\n",
    "df = df.withColumn('date_2', to_date(df.date_str_2, 'dd MMM yyyy'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95f407e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "|date_str_1| date_str_2|    date_1|    date_2|day_of_month|week_number|day_of_year|day_of_week|\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|2023-05-18|2010-01-01|          18|         20|        138|          5|\n",
      "|2023-12-31|01 Jan 2010|2023-12-31|2010-01-01|          31|         52|        365|          1|\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"day_of_month\", dayofmonth(df.date_1))\\\n",
    ".withColumn(\"week_number\", weekofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_year\", dayofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_week\", dayofweek(df.date_1))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d558b7d",
   "metadata": {},
   "source": [
    "### 22. How to convert year-month string to dates corresponding to the 4th day of the month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cfe8eccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|MonthYear|\n",
      "+---------+\n",
      "| Jan 2010|\n",
      "| Feb 2011|\n",
      "| Mar 2012|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02d77f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|MonthYear|      Date|\n",
      "+---------+----------+\n",
      "| Jan 2010|2010-01-01|\n",
      "| Feb 2011|2011-02-01|\n",
      "| Mar 2012|2012-03-01|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Date', to_date(df.MonthYear, 'MMM yyyy')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f428d9",
   "metadata": {},
   "source": [
    "### 23 How to filter words that contain atleast 2 vowels from a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b7bd16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "| Apple|\n",
      "|Orange|\n",
      "|  Plan|\n",
      "|Python|\n",
      "| Money|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97e02796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "| Apple|\n",
      "|Orange|\n",
      "| Money|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((length(col('Word')) - length(translate(col('Word'), 'AEIOUaeiou', ''))) >= 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae251ed4",
   "metadata": {},
   "source": [
    "### 30. How to get the nrows, ncolumns, datatype of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51205939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n",
      "[('Word', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(len(df.columns))\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef93ab",
   "metadata": {},
   "source": [
    "### 32. How to check if a dataframe has any missing values and count of missing values in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "850c457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d58811e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df.id.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcb9b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   0|    2|  2|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33d04ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 0, 'Value': 2, 'id': 2}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.collect()[0].asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89611ee0",
   "metadata": {},
   "source": [
    "### 33 How to replace missing values of multiple numeric columns with the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b83d9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1|NULL|\n",
      "|   B|NULL| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6|NULL|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, 123 ),\n",
    "(\"B\", 3, 456),\n",
    "(\"D\", 6, None),\n",
    "], [\"Name\", \"var1\", \"var2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d85714c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "504766a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1| 289|\n",
      "|   B|   3| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6| 289|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_names = [\"var1\", \"var2\"]\n",
    "\n",
    "imputer = Imputer(inputCols= column_names, outputCols= column_names, strategy=\"mean\")\n",
    "model = imputer.fit(df)\n",
    "\n",
    "imputed_df = model.transform(df)\n",
    "imputed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb1c58ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|Name|var1|var2|\n",
      "+----+----+----+\n",
      "|   A|   1|NULL|\n",
      "|   B|   0| 123|\n",
      "|   B|   3| 456|\n",
      "|   D|   6|NULL|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(value=0, subset=['var1']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8db226",
   "metadata": {},
   "source": [
    "### 34. How to change the order of columns of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76d1bee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|First_Name|Last_Name|Age|\n",
      "+----------+---------+---+\n",
      "|      John|      Doe| 30|\n",
      "|      Jane|      Doe| 25|\n",
      "|     Alice|    Smith| 22|\n",
      "+----------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Doe\", 25), (\"Alice\", \"Smith\", 22)]\n",
    "df = spark.createDataFrame(data, [\"First_Name\", \"Last_Name\", \"Age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5cd68dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n",
      "|Age|First_Name|Last_Name|\n",
      "+---+----------+---------+\n",
      "| 30|      John|      Doe|\n",
      "| 25|      Jane|      Doe|\n",
      "| 22|     Alice|    Smith|\n",
      "+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_order = [\"Age\", \"First_Name\", \"Last_Name\"]\n",
    "df.select(*new_order).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a73d0",
   "metadata": {},
   "source": [
    "### 44. How to create lags and leads of a column by group in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2a5c3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|      Date| Store|Sales|\n",
      "+----------+------+-----+\n",
      "|2023-01-01|Store1|  100|\n",
      "|2023-01-02|Store1|  150|\n",
      "|2023-01-03|Store1|  200|\n",
      "|2023-01-04|Store1|  250|\n",
      "|2023-01-05|Store1|  300|\n",
      "|2023-01-01|Store2|   50|\n",
      "|2023-01-02|Store2|   60|\n",
      "|2023-01-03|Store2|   80|\n",
      "|2023-01-04|Store2|   90|\n",
      "|2023-01-05|Store2|  120|\n",
      "+----------+------+-----+\n",
      "\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store: string (nullable = true)\n",
      " |-- Sales: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"2023-01-01\", \"Store1\", 100),\n",
    "(\"2023-01-02\", \"Store1\", 150),\n",
    "(\"2023-01-03\", \"Store1\", 200),\n",
    "(\"2023-01-04\", \"Store1\", 250),\n",
    "(\"2023-01-05\", \"Store1\", 300),\n",
    "(\"2023-01-01\", \"Store2\", 50),\n",
    "(\"2023-01-02\", \"Store2\", 60),\n",
    "(\"2023-01-03\", \"Store2\", 80),\n",
    "(\"2023-01-04\", \"Store2\", 90),\n",
    "(\"2023-01-05\", \"Store2\", 120)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Date\", \"Store\", \"Sales\"])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31aa13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date from string to date type \n",
    "df = df.withColumn('Date', to_date(df.Date, 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c5fb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Window partitioned by store, ordered by Date \n",
    "window = Window.partitionBy('Store').orderBy('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc9637d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+---------+\n",
      "|      Date| Store|Sales|Lag_Sales|\n",
      "+----------+------+-----+---------+\n",
      "|2023-01-01|Store1|  100|     NULL|\n",
      "|2023-01-02|Store1|  150|      100|\n",
      "|2023-01-03|Store1|  200|      150|\n",
      "|2023-01-04|Store1|  250|      200|\n",
      "|2023-01-05|Store1|  300|      250|\n",
      "|2023-01-01|Store2|   50|     NULL|\n",
      "|2023-01-02|Store2|   60|       50|\n",
      "|2023-01-03|Store2|   80|       60|\n",
      "|2023-01-04|Store2|   90|       80|\n",
      "|2023-01-05|Store2|  120|       90|\n",
      "+----------+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create lag variables\n",
    "df = df.withColumn('Lag_Sales', lag(df.Sales).over(window))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35c43847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+---------+----------+\n",
      "|      Date| Store|Sales|Lag_Sales|Lead_Sales|\n",
      "+----------+------+-----+---------+----------+\n",
      "|2023-01-01|Store1|  100|     NULL|       150|\n",
      "|2023-01-02|Store1|  150|      100|       200|\n",
      "|2023-01-03|Store1|  200|      150|       250|\n",
      "|2023-01-04|Store1|  250|      200|       300|\n",
      "|2023-01-05|Store1|  300|      250|      NULL|\n",
      "|2023-01-01|Store2|   50|     NULL|        60|\n",
      "|2023-01-02|Store2|   60|       50|        80|\n",
      "|2023-01-03|Store2|   80|       60|        90|\n",
      "|2023-01-04|Store2|   90|       80|       120|\n",
      "|2023-01-05|Store2|  120|       90|      NULL|\n",
      "+----------+------+-----+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create lead variables \n",
    "df = df.withColumn('Lead_Sales', lead(df.Sales).over(window))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b78d0",
   "metadata": {},
   "source": [
    "### 49. How to Pivot the dataframe (converting rows into columns) ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a40ae437",
   "metadata": {},
   "source": [
    "pivot() function is used to rotate/transpose the data from one column into multiple Dataframe columns and back using unpivot(). Pivot() It is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "074e8c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+\n",
      "|year|quarter|region|revenue|\n",
      "+----+-------+------+-------+\n",
      "|2021|      1|    US|   5000|\n",
      "|2021|      1|    EU|   4000|\n",
      "|2021|      2|    US|   5500|\n",
      "|2021|      2|    EU|   4500|\n",
      "|2021|      3|    US|   6000|\n",
      "|2021|      3|    EU|   5000|\n",
      "|2021|      4|    US|   7000|\n",
      "|2021|      4|    EU|   6000|\n",
      "+----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24ea4867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+----+\n",
      "|year|quarter|  EU|  US|\n",
      "+----+-------+----+----+\n",
      "|2021|      2|4500|5500|\n",
      "|2021|      1|4000|5000|\n",
      "|2021|      3|5000|6000|\n",
      "|2021|      4|6000|7000|\n",
      "+----+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df = df.groupBy('year', 'quarter').pivot('region').sum('revenue')\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd44fd",
   "metadata": {},
   "source": [
    "### 50. How to UnPivot the dataframe (converting columns into rows) ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "267335b1",
   "metadata": {},
   "source": [
    "UnPivot EU, US columns and create region, revenue Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1cde74ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+----+\n",
      "|year|quarter|  EU|  US|\n",
      "+----+-------+----+----+\n",
      "|2021|      2|4500|5500|\n",
      "|2021|      1|4000|5000|\n",
      "|2021|      3|5000|6000|\n",
      "|2021|      4|6000|7000|\n",
      "+----+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(2021, 2, 4500, 5500),\n",
    "(2021, 1, 4000, 5000),\n",
    "(2021, 3, 5000, 6000),\n",
    "(2021, 4, 6000, 7000)]\n",
    "\n",
    "columns = [\"year\", \"quarter\", \"EU\", \"US\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "70cab457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+\n",
      "|year|quarter|region|revenue|\n",
      "+----+-------+------+-------+\n",
      "|2021|      2|    EU|   4500|\n",
      "|2021|      2|    US|   5500|\n",
      "|2021|      1|    EU|   4000|\n",
      "|2021|      1|    US|   5000|\n",
      "|2021|      3|    EU|   5000|\n",
      "|2021|      3|    US|   6000|\n",
      "|2021|      4|    EU|   6000|\n",
      "|2021|      4|    US|   7000|\n",
      "+----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivotExpr = \"stack(2, 'EU', EU, 'US', US) as (region, revenue)\"\n",
    "unPivotDF = df.select('year', 'quarter', expr(unpivotExpr)).where(\"revenue is not null\")\n",
    "unPivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d687a",
   "metadata": {},
   "source": [
    "### 51. How to impute missing values with Zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5dc51156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|   1|NULL|\n",
      "|NULL|   2|\n",
      "|   3|   4|\n",
      "|   5|NULL|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, None), (None, 2), (3, 4), (5, None)], [\"a\", \"b\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9ebf26c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  0|\n",
      "|  0|  2|\n",
      "|  3|  4|\n",
      "|  5|  0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76c6fc",
   "metadata": {},
   "source": [
    "### 55. How to convert a column to lower case using UDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de4558ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|        Name|         City|\n",
      "+------------+-------------+\n",
      "|    John Doe|     NEW YORK|\n",
      "|    Jane Doe|  LOS ANGELES|\n",
      "|Mike Johnson|      CHICAGO|\n",
      "|  Sara Smith|SAN FRANCISCO|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('John Doe', 'NEW YORK'),\n",
    "('Jane Doe', 'LOS ANGELES'),\n",
    "('Mike Johnson', 'CHICAGO'),\n",
    "('Sara Smith', 'SAN FRANCISCO')]\n",
    "\n",
    "df = spark.createDataFrame(data, ['Name', 'City'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2858b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(s):\n",
    "    if s is not None:\n",
    "        return s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7eb4236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------+\n",
      "|        Name|         City|   City_lower|\n",
      "+------------+-------------+-------------+\n",
      "|    John Doe|     NEW YORK|     new york|\n",
      "|    Jane Doe|  LOS ANGELES|  los angeles|\n",
      "|Mike Johnson|      CHICAGO|      chicago|\n",
      "|  Sara Smith|SAN FRANCISCO|san francisco|\n",
      "+------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_to_lower = udf(to_lower, StringType())\n",
    "df = df.withColumn('City_lower', udf_to_lower(df.City))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ed5d8",
   "metadata": {},
   "source": [
    "### 59. How to restrict the PySpark to use the number of cores in the system?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9817bcf0",
   "metadata": {},
   "source": [
    "from pyspark import SparkConf, SparkContext \n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.cores', '2')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acfb800",
   "metadata": {},
   "source": [
    "### 60. How to cache PySpark DataFrame or objects and delete cache?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39df3ee2",
   "metadata": {},
   "source": [
    "In PySpark, caching or persisting data is done to speed up data retrieval during iterative and interactive computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2b4c626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, City: string, City_lower: string]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72e65d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------+\n",
      "|        Name|         City|   City_lower|\n",
      "+------------+-------------+-------------+\n",
      "|    John Doe|     NEW YORK|     new york|\n",
      "|    Jane Doe|  LOS ANGELES|  los angeles|\n",
      "|Mike Johnson|      CHICAGO|      chicago|\n",
      "|  Sara Smith|SAN FRANCISCO|san francisco|\n",
      "+------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e39e5",
   "metadata": {},
   "source": [
    "### 61. How to Divide a PySpark DataFrame randomly in a given ratio (0.8, 0.2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7309ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1322de2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------+\n",
      "|        Name|         City|   City_lower|\n",
      "+------------+-------------+-------------+\n",
      "|    Jane Doe|  LOS ANGELES|  los angeles|\n",
      "|Mike Johnson|      CHICAGO|      chicago|\n",
      "|  Sara Smith|SAN FRANCISCO|san francisco|\n",
      "+------------+-------------+-------------+\n",
      "\n",
      "+--------+--------+----------+\n",
      "|    Name|    City|City_lower|\n",
      "+--------+--------+----------+\n",
      "|John Doe|NEW YORK|  new york|\n",
      "+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show()\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a800fea",
   "metadata": {},
   "source": [
    "### 69. How to calculate missing value percentage in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4618569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------+\n",
      "|FirstName|LastName|    City|\n",
      "+---------+--------+--------+\n",
      "|     John|     Doe|    NULL|\n",
      "|     NULL|   Smith|New York|\n",
      "|     Mike|   Smith|    NULL|\n",
      "|     Anna|   Smith|  Boston|\n",
      "|     NULL|    NULL|    NULL|\n",
      "+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", \"Doe\", None),\n",
    "(None, \"Smith\", \"New York\"),\n",
    "(\"Mike\", \"Smith\", None),\n",
    "(\"Anna\", \"Smith\", \"Boston\"),\n",
    "(None, None, None)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"FirstName\", \"LastName\", \"City\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6a9b634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "total_rows = df.count()\n",
    "print(total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "35472564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in FirstName: 40.0%\n",
      "Missing values in LastName: 20.0%\n",
      "Missing values in City: 60.0%\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    null_values = df.filter(df[column].isNull()).count()\n",
    "    missing_percentage = (null_values / total_rows) *100 \n",
    "    print(f\"Missing values in {column}: {missing_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7209b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
