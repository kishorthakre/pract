{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155dd09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531ea1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col, when, avg, asc, desc, coalesce, lit, explode, split, trim\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab9a1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>q2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x269aea71250>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('q2').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a5698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49033d40",
   "metadata": {},
   "source": [
    "###  find records with the second highest 'insert_date' for each 'location'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1892d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This question will tell you how to use transformations in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e610b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------------+\n",
      "| id|location|        insert_date|\n",
      "+---+--------+-------------------+\n",
      "|  1|       A|2023-01-01 10:00:00|\n",
      "|  2|       A|2023-01-02 12:00:00|\n",
      "|  3|       B|2023-01-01 09:00:00|\n",
      "|  4|       B|2023-01-03 11:00:00|\n",
      "|  5|       A|2023-01-03 13:00:00|\n",
      "|  6|       B|2023-01-02 15:00:00|\n",
      "|  7|       C|2023-01-08 18:00:00|\n",
      "|  8|       C|2023-01-07 16:00:00|\n",
      "+---+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"A\", \"2023-01-01 10:00:00\"),\n",
    "         (2, \"A\", \"2023-01-02 12:00:00\"),\n",
    "         (3, \"B\", \"2023-01-01 09:00:00\"),\n",
    "         (4, \"B\", \"2023-01-03 11:00:00\"),\n",
    "         (5, \"A\", \"2023-01-03 13:00:00\"),\n",
    "         (6, \"B\", \"2023-01-02 15:00:00\"),\n",
    "         (7, \"C\", \"2023-01-08 18:00:00\"),\n",
    "         (8, \"C\", \"2023-01-07 16:00:00\")]\n",
    "\n",
    "columns = ['id', 'location', 'insert_date']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f44408e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in spark using transformation and action \n",
    "\n",
    "windowspec = Window.partitionBy('location').orderBy(df['insert_date'].desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b2cae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rnk = df.withColumn('rnk', row_number().over(windowspec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12e5ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------------+\n",
      "| id|location|        insert_date|\n",
      "+---+--------+-------------------+\n",
      "|  2|       A|2023-01-02 12:00:00|\n",
      "|  6|       B|2023-01-02 15:00:00|\n",
      "|  8|       C|2023-01-07 16:00:00|\n",
      "+---+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df_rnk.filter(df_rnk.rnk == 2).select('id', 'location', 'insert_date')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64ade4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "501cd8aa",
   "metadata": {},
   "source": [
    "\n",
    "Write a function named grades_colors to select only the rows \n",
    "where the student's favorite color is green or red and their grade is above 90.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3ea44ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------+-----+\n",
      "|           name|age|favorite_color|grade|\n",
      "+---------------+---+--------------+-----+\n",
      "|       Tim Voss| 19|           red|   91|\n",
      "| Nicole Johnson| 20|        yellow|   95|\n",
      "|  Elsa Williams| 21|         green|   82|\n",
      "|     John james| 20|          blue|   75|\n",
      "|Catherine Jones| 23|         green|   93|\n",
      "+---------------+---+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [(\"Tim Voss\", 19, \"red\", 91),\n",
    "(\"Nicole Johnson\", 20, \"yellow\", 95),\n",
    "(\"Elsa Williams\", 21, \"green\", 82),\n",
    "(\"John james\", 20, \"blue\", 75),\n",
    "(\"Catherine Jones\", 23, \"green\", 93)]\n",
    "columns1 = ['name', 'age', 'favorite_color', 'grade']\n",
    "\n",
    "student_df = spark.createDataFrame(data1, columns1)\n",
    "student_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a74e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------+-----+\n",
      "|           name|age|favorite_color|grade|\n",
      "+---------------+---+--------------+-----+\n",
      "|       Tim Voss| 19|           red|   91|\n",
      "|Catherine Jones| 23|         green|   93|\n",
      "+---------------+---+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def grades_colors(df):\n",
    "    return df.filter((col('favorite_color').isin('red','green')) & (col('grade') > 90))\n",
    "\n",
    "result = grades_colors(student_df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb713dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "9c92258c",
   "metadata": {},
   "source": [
    "# write a Pyspark code snipet to perform the following transformations:\n",
    "\n",
    "# Filter out records where sales are less than 50.\n",
    "# create a new column sales_category which categorizes sales into 'Low' (50-100), \"Medium\" (101-200), and 'High' (>200). \n",
    "\n",
    "# Group the data by sales_category and calculate the average sales for each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2d277da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|   name|sales|\n",
      "+---+-------+-----+\n",
      "|  1|  Alice|   45|\n",
      "|  2|    Bob|  120|\n",
      "|  3|Charlie|   75|\n",
      "|  4|  David|  180|\n",
      "|  5|    Eve|  220|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = [(1, 'Alice', 45),\n",
    "        (2, 'Bob', 120), \n",
    "        (3, 'Charlie', 75),\n",
    "        (4, 'David', 180), \n",
    "        (5, 'Eve', 220)]\n",
    "columns2 = ['id','name', 'sales']\n",
    "df = spark.createDataFrame(data2, columns2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3360d293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|   name|sales|\n",
      "+---+-------+-----+\n",
      "|  2|    Bob|  120|\n",
      "|  3|Charlie|   75|\n",
      "|  4|  David|  180|\n",
      "|  5|    Eve|  220|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out records where sales are less than 50.\n",
    "\n",
    "#df1 = df.filter(col('sales') < 50)\n",
    "df1 = df.filter(col('sales') >= 50)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a63c0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+--------------+\n",
      "| id|   name|sales|sales_category|\n",
      "+---+-------+-----+--------------+\n",
      "|  2|    Bob|  120|        Medium|\n",
      "|  3|Charlie|   75|           Low|\n",
      "|  4|  David|  180|        Medium|\n",
      "|  5|    Eve|  220|          High|\n",
      "+---+-------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new column sales_category which categorizes sales into 'Low' (50-100), \"Medium\" (101-200), and 'High' (>200). \n",
    "\n",
    "df2 = df1.withColumn('sales_category', when(col('sales') <= 100, 'Low')\n",
    "                     .when((col('sales') > 100)&(col('sales')<=200), 'Medium').otherwise('High'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "573192ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|sales_category|average_sales|\n",
      "+--------------+-------------+\n",
      "|        Medium|        150.0|\n",
      "|           Low|         75.0|\n",
      "|          High|        220.0|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the data by sales_category and calculate the average sales for each category. \n",
    "\n",
    "df = df2.groupBy('sales_category').agg(avg('sales').alias('average_sales'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7fd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ede31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ddf7c4eb",
   "metadata": {},
   "source": [
    "Q1) join df and casting_df on movie name.\n",
    "Q2) Find the category of the actor and movie.\n",
    "Q3) Find the what is the avg rating of actor\n",
    "Q4) how to sort in pyspark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "853b1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = [\n",
    "    ('A', 'Good', 9.5, 'horror'),\n",
    "    ('B', 'Good', 8.2, 'Comedy'),\n",
    "    ('C', 'Avg', 9.5, 'Comedy'),\n",
    "    ('D', 'Avg', 9.5, 'horror')\n",
    "]\n",
    "df_columns = [\"title\", \"reviews\", \"ratings\", \"categories\"]\n",
    "df = spark.createDataFrame(df_data, df_columns)\n",
    "\n",
    "casting_df_data = [\n",
    "    ('Actor1', 'A', 9.0),\n",
    "    ('Actor2', 'B', 8.5),\n",
    "    ('Actor3', 'C', 7.5),\n",
    "    ('Actor4', 'D', 8.0)\n",
    "]\n",
    "casting_df_columns = [\"actor_name\", \"movie_name\", \"actor_rating\"]\n",
    "\n",
    "casting_df = spark.createDataFrame(casting_df_data, casting_df_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4967f586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+----------+\n",
      "|title|reviews|ratings|categories|\n",
      "+-----+-------+-------+----------+\n",
      "|    A|   Good|    9.5|    horror|\n",
      "|    B|   Good|    8.2|    Comedy|\n",
      "|    C|    Avg|    9.5|    Comedy|\n",
      "|    D|    Avg|    9.5|    horror|\n",
      "+-----+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aaf34771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+\n",
      "|actor_name|movie_name|actor_rating|\n",
      "+----------+----------+------------+\n",
      "|    Actor1|         A|         9.0|\n",
      "|    Actor2|         B|         8.5|\n",
      "|    Actor3|         C|         7.5|\n",
      "|    Actor4|         D|         8.0|\n",
      "+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casting_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "447e16d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+----------+----------+----------+------------+\n",
      "|title|reviews|ratings|categories|actor_name|movie_name|actor_rating|\n",
      "+-----+-------+-------+----------+----------+----------+------------+\n",
      "|    A|   Good|    9.5|    horror|    Actor1|         A|         9.0|\n",
      "|    B|   Good|    8.2|    Comedy|    Actor2|         B|         8.5|\n",
      "|    C|    Avg|    9.5|    Comedy|    Actor3|         C|         7.5|\n",
      "|    D|    Avg|    9.5|    horror|    Actor4|         D|         8.0|\n",
      "+-----+-------+-------+----------+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1) join df and casting_df on movie name.\n",
    "\n",
    "j_df = df.join(casting_df, df.title == casting_df.movie_name, how = 'full_outer')\n",
    "j_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80a0ccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|actor_name|movie_name|categories|\n",
      "+----------+----------+----------+\n",
      "|    Actor1|         A|    horror|\n",
      "|    Actor2|         B|    Comedy|\n",
      "|    Actor3|         C|    Comedy|\n",
      "|    Actor4|         D|    horror|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q2) Find the category of the actor and movie.\n",
    "\n",
    "actor_movie_catogery = j_df.select('actor_name', 'movie_name', 'categories')\n",
    "actor_movie_catogery.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62aee64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|avg(actor_rating)|\n",
      "+-----------------+\n",
      "|             8.25|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q3) Find the what is the avg rating of actor\n",
    "\n",
    "avg_r = casting_df.agg({'actor_rating':'avg'}).alias('avg_actor_rating')\n",
    "avg_r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9f2e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+----------+\n",
      "|title|reviews|ratings|categories|\n",
      "+-----+-------+-------+----------+\n",
      "|    D|    Avg|    9.5|    horror|\n",
      "|    C|    Avg|    9.5|    Comedy|\n",
      "|    B|   Good|    8.2|    Comedy|\n",
      "|    A|   Good|    9.5|    horror|\n",
      "+-----+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4) how to sort in pyspark?\n",
    "\n",
    "sort_df = df.orderBy(desc('title'))\n",
    "sort_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd0e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2097e638",
   "metadata": {},
   "source": [
    "Write an Pyspark code to find the names of all employees and the name of their immediate boss. \n",
    "If an employee does not have a boss, display \"No Boss\" for them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1cdebea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| ID| Name|Boss|\n",
      "+---+-----+----+\n",
      "|  1|Alice|NULL|\n",
      "|  2|  Bob|   1|\n",
      "|  3|Carol|   2|\n",
      "|  4| Dave|   1|\n",
      "|  5|  Eve|   2|\n",
      "|  6|Frank|   4|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", None),\n",
    "    (2, \"Bob\", 1),\n",
    "    (3, \"Carol\", 2),\n",
    "    (4, \"Dave\", 1),\n",
    "    (5, \"Eve\", 2),\n",
    "    (6, \"Frank\", 4)\n",
    "]\n",
    "schema = [\"ID\", \"Name\", \"Boss\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9256c51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+----+-----+----+\n",
      "| ID| Name|Boss|  ID| Name|Boss|\n",
      "+---+-----+----+----+-----+----+\n",
      "|  1|Alice|NULL|NULL| NULL|NULL|\n",
      "|  2|  Bob|   1|   1|Alice|NULL|\n",
      "|  3|Carol|   2|   2|  Bob|   1|\n",
      "|  4| Dave|   1|   1|Alice|NULL|\n",
      "|  5|  Eve|   2|   2|  Bob|   1|\n",
      "|  6|Frank|   4|   4| Dave|   1|\n",
      "+---+-----+----+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_alias = df.alias(\"e1\")\n",
    "boss_alias = df.alias(\"e2\")\n",
    "\n",
    "df1 = df_alias.join(boss_alias, col(\"e1.Boss\") == col(\"e2.ID\"), 'left')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "75b5f837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|Employee_Name|   Boss|\n",
      "+-------------+-------+\n",
      "|        Alice|No Boss|\n",
      "|          Bob|  Alice|\n",
      "|        Carol|    Bob|\n",
      "|         Dave|  Alice|\n",
      "|          Eve|    Bob|\n",
      "|        Frank|   Dave|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df1.select(col(\"e1.name\").alias(\"Employee_Name\"), coalesce(col('e2.Name'), lit(\"No Boss\")).alias('Boss'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8fe243a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| ID| Name|Boss|\n",
      "+---+-----+----+\n",
      "|  1|Alice|NULL|\n",
      "|  2|  Bob|   1|\n",
      "|  3|Carol|   2|\n",
      "|  4| Dave|   1|\n",
      "|  5|  Eve|   2|\n",
      "|  6|Frank|   4|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372f41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba910821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8e3a9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------+\n",
      "|Empid|  Name|   Locations|\n",
      "+-----+------+------------+\n",
      "|    1|Gaurav|Pune,hyd,Blr|\n",
      "|    2|  Ravi|     hyd,Blr|\n",
      "+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,\"Gaurav\",\"Pune,hyd,Blr\"),(2,\"Ravi\",\"hyd,Blr\")]\n",
    "\n",
    "columns=[\"Empid\",\"Name\",\"Locations\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f0a193c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------+--------+\n",
      "|Empid|  Name|   Locations|Location|\n",
      "+-----+------+------------+--------+\n",
      "|    1|Gaurav|Pune,hyd,Blr|    Pune|\n",
      "|    1|Gaurav|Pune,hyd,Blr|     hyd|\n",
      "|    1|Gaurav|Pune,hyd,Blr|     Blr|\n",
      "|    2|  Ravi|     hyd,Blr|     hyd|\n",
      "|    2|  Ravi|     hyd,Blr|     Blr|\n",
      "+-----+------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"Location\", explode(split(col('Locations'), ',')))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aaab84e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------+--------+\n",
      "|Empid|  Name|   Locations|Location|\n",
      "+-----+------+------------+--------+\n",
      "|    1|Gaurav|Pune,hyd,Blr|    Pune|\n",
      "|    1|Gaurav|Pune,hyd,Blr|     hyd|\n",
      "|    1|Gaurav|Pune,hyd,Blr|     Blr|\n",
      "|    2|  Ravi|     hyd,Blr|     hyd|\n",
      "|    2|  Ravi|     hyd,Blr|     Blr|\n",
      "+-----+------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df1.withColumn(\"Location\", trim(col(\"Location\")))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75de2610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+\n",
      "|Empid|  Name|Location|\n",
      "+-----+------+--------+\n",
      "|    1|Gaurav|    Pune|\n",
      "|    1|Gaurav|     hyd|\n",
      "|    2|  Ravi|     hyd|\n",
      "+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Location not equal to Blr \n",
    "\n",
    "final = result.select(\"Empid\", \"Name\", \"Location\").filter(col(\"Location\") != 'Blr')\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cac231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6d44a311",
   "metadata": {},
   "source": [
    "question: Count the number of movies in each genre? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8b9bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------------+\n",
      "|name                    |genres                          |\n",
      "+------------------------+--------------------------------+\n",
      "|The Shawshank Redemption|[Drama, Crime]                  |\n",
      "|The Godfather           |[Drama, Crime]                  |\n",
      "|Pulp Fiction            |[Drama, Crime, Thriller]        |\n",
      "|The Dark Knight         |[Drama, Crime, Thriller, Action]|\n",
      "+------------------------+--------------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "                            ('The Shawshank Redemption',['Drama', 'Crime']),\n",
    "                            ('The Godfather', ['Drama', 'Crime']),\n",
    "                            ('Pulp Fiction', ['Drama', 'Crime','Thriller']),\n",
    "                            ('The Dark Knight', ['Drama', 'Crime','Thriller','Action']), \n",
    "                           ],\n",
    "                          [\"name\", \"genres\"])\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e637eaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                name|   genre|\n",
      "+--------------------+--------+\n",
      "|The Shawshank Red...|   Drama|\n",
      "|The Shawshank Red...|   Crime|\n",
      "|       The Godfather|   Drama|\n",
      "|       The Godfather|   Crime|\n",
      "|        Pulp Fiction|   Drama|\n",
      "|        Pulp Fiction|   Crime|\n",
      "|        Pulp Fiction|Thriller|\n",
      "|     The Dark Knight|   Drama|\n",
      "|     The Dark Knight|   Crime|\n",
      "|     The Dark Knight|Thriller|\n",
      "|     The Dark Knight|  Action|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select('name', explode(col('genres')).alias('genre'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7e1d3a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   genre|count|\n",
      "+--------+-----+\n",
      "|   Crime|    4|\n",
      "|   Drama|    4|\n",
      "|Thriller|    2|\n",
      "|  Action|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('genre').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08059c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7650ee6c",
   "metadata": {},
   "source": [
    "# How would you calculate the month-wise cumulative revenue using Pyspark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77152078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|revenue|   date|\n",
      "+-------+-------+\n",
      "|   3000| 22-may|\n",
      "|   5000| 23-may|\n",
      "|   5000| 25-may|\n",
      "|  10000|22-june|\n",
      "|   1250|03-july|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [( \n",
    "'3000' ,  '22-may'), \n",
    "('5000' ,  '23-may'),\n",
    "('5000' ,  '25-may'),\n",
    "('10000' , '22-june'),  \n",
    "('1250'  , '03-july')]\n",
    "\n",
    "schema = ['revenue','date']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d207afce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|revenue|   date|month|\n",
      "+-------+-------+-----+\n",
      "|   3000| 22-may| NULL|\n",
      "|   5000| 23-may| NULL|\n",
      "|   5000| 25-may| NULL|\n",
      "|  10000|22-june| NULL|\n",
      "|   1250|03-july| NULL|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('month', date_format(to_date(col('date'), 'dd-MM'), 'MMM'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09aa25d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------+\n",
      "|revenue|   date|month|cumulative_sum|\n",
      "+-------+-------+-----+--------------+\n",
      "|   1250|03-july| NULL|        1250.0|\n",
      "|  10000|22-june| NULL|       11250.0|\n",
      "|   3000| 22-may| NULL|       14250.0|\n",
      "|   5000| 23-may| NULL|       19250.0|\n",
      "|   5000| 25-may| NULL|       24250.0|\n",
      "+-------+-------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec=Window.partitionBy('month').orderBy('date').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "df2=df1.withColumn('cumulative_sum',sum(col(\"revenue\")).over(window_spec))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18018084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a spark or sql code to find the employee count under each manager?\n",
    "https://www.youtube.com/watch?v=8L7BIDUySfw&list=PLDbkX3qdHA3AVyxV-OUlgjgzojAtc4WSz&index=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df232b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+----------+\n",
      "|employee_id|first_name|last_name|manager_id|\n",
      "+-----------+----------+---------+----------+\n",
      "|       4529|     Nancy|    Young|      4125|\n",
      "|       4238|      John|    Simon|      4329|\n",
      "|       4329|   Martina| Candreva|      4125|\n",
      "|       4009|     Klaus|     Koch|      4329|\n",
      "|       4125|   Mafalda|  Ranieri|      NULL|\n",
      "|       4500|     Jakub|   Hrabal|      4529|\n",
      "|       4118|     Moira|    Areas|      4952|\n",
      "|       4012|       Jon|  Nilssen|      4952|\n",
      "|       4952|    Sandra| Rajkovic|      4529|\n",
      "|       4444|    Seamus|    Quinn|      4329|\n",
      "+-----------+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('4529', 'Nancy', 'Young', '4125'),\n",
    "('4238','John', 'Simon', '4329'),\n",
    "('4329', 'Martina', 'Candreva', '4125'),\n",
    "('4009', 'Klaus', 'Koch', '4329'),\n",
    "('4125', 'Mafalda', 'Ranieri', 'NULL'),\n",
    "('4500', 'Jakub', 'Hrabal', '4529'),\n",
    "('4118', 'Moira', 'Areas', '4952'),\n",
    "('4012', 'Jon', 'Nilssen', '4952'),\n",
    "('4952', 'Sandra', 'Rajkovic', '4529'),\n",
    "('4444', 'Seamus', 'Quinn', '4329')]\n",
    "\n",
    "schema = ['employee_id' ,'first_name', 'last_name', 'manager_id']\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dfd77b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|manager_id|count|\n",
      "+----------+-----+\n",
      "|      4125|    2|\n",
      "|      4329|    3|\n",
      "|      NULL|    1|\n",
      "|      4529|    2|\n",
      "|      4952|    2|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+------+\n",
      "|manager_id|count1|\n",
      "+----------+------+\n",
      "|      4125|     2|\n",
      "|      4329|     3|\n",
      "|      NULL|     1|\n",
      "|      4529|     2|\n",
      "|      4952|     2|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('manager_id').count().show()\n",
    "df.groupBy('manager_id').agg(count('*').alias('count1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f735f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('EMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec3b7981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|manager_id|no_of_emp|mangr_name|\n",
      "+----------+---------+----------+\n",
      "|      4125|        2|   Mafalda|\n",
      "|      4329|        3|   Martina|\n",
      "|      4529|        2|     Nancy|\n",
      "|      4952|        2|    Sandra|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''select  e.manager_id as manager_id, \n",
    "count(e.employee_id) as no_of_emp,(m.First_name) as mangr_name \n",
    "from  emp e\n",
    "inner join emp m on m.employee_id =e.manager_id group by 1,3 '''\n",
    "result=spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077ace3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
