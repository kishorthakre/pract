{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82194951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df34adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a68edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Kishor</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c41e7dc790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Kishor').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if j have 3 keys in all line and 4 key in one line?\n",
    "# It will print Null for rest of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b9d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5bce832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+------+\n",
      "|age|gender|    name|salary|\n",
      "+---+------+--------+------+\n",
      "| 20|  NULL|  Manish| 20000|\n",
      "| 25|  NULL|  Nikita| 21000|\n",
      "| 16|  NULL|  Pritam| 22000|\n",
      "| 35|  NULL|Prantosh| 25000|\n",
      "| 67|     M|  Vikash| 40000|\n",
      "+---+------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rj = spark.read.format('json')\\\n",
    "      .option('inferSchema', True)\\\n",
    "      .option('mode', \"PERMISIVE\")\\\n",
    "      .load('single_file_json_with_extra_field.json')\n",
    "rj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65921c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading multiline json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a79adc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|age|    name|salary|\n",
      "+---+--------+------+\n",
      "| 20|  Manish| 20000|\n",
      "| 25|  Nikita| 21000|\n",
      "| 16|  Pritam| 22000|\n",
      "| 35|Prantosh| 25000|\n",
      "| 67|  Vikash| 40000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rj = spark.read.format('json')\\\n",
    "      .option('inferSchema', True)\\\n",
    "      .option('mode', \"PERMISIVE\")\\\n",
    "      .option('multiline',True)\\\n",
    "      .load('Multi_line_correct.json')\n",
    "rj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiline inccorect json file ( will print only one line, need to provide list [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8c5cd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|age|  name|salary|\n",
      "+---+------+------+\n",
      "| 20|Manish| 20000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rj = spark.read.format('json')\\\n",
    "      .option('inferSchema', True)\\\n",
    "      .option('mode', \"PERMISIVE\")\\\n",
    "      .option('multiline',True)\\\n",
    "      .load('Multi_line_incorrect.json')\n",
    "rj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1802137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrupted json file ( will create new column for corrupt data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cda826b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----+--------+------+\n",
      "|_corrupt_record                         |age |name    |salary|\n",
      "+----------------------------------------+----+--------+------+\n",
      "|NULL                                    |20  |Manish  |20000 |\n",
      "|NULL                                    |25  |Nikita  |21000 |\n",
      "|NULL                                    |16  |Pritam  |22000 |\n",
      "|NULL                                    |35  |Prantosh|25000 |\n",
      "|{\"name\":\"Vikash\",\"age\":67,\"salary\":40000|NULL|NULL    |NULL  |\n",
      "+----------------------------------------+----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rj = spark.read.format('json')\\\n",
    "      .option('inferSchema', True)\\\n",
    "      .option('mode', \"PERMISIVE\")\\\n",
    "      .load('corrupted_json.json')\n",
    "rj.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5385206a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab75cf8a",
   "metadata": {},
   "source": [
    "# Write dadaframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d0fa81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+--------+----------+-----------+\n",
      "|id | name     |     age|  salary|   address|    gender |\n",
      "+---+----------+--------+--------+----------+-----------+\n",
      "|1  |  Manish  |26      |75000   |    INDIA |        m  |\n",
      "|2  |  Nikita  |23      |100000  |   USA    |          f|\n",
      "|3  |  Pritam  |22      |150000  |   INDIA  |        m  |\n",
      "|4  |  Prantosh|17      |200000  |   JAPAN  |        m  |\n",
      "|5  |  Vikash  |31      |300000  |   USA    |          m|\n",
      "|6  |  Rahul   |55      |300000  |   INDIA  |        m  |\n",
      "|7  |  Raju    |67      |540000  |   USA    |          m|\n",
      "|8  |  Praveen |28      |70000   |    JAPAN |        m  |\n",
      "|9  |  Dev     |32      |150000  |   JAPAN  |        m  |\n",
      "|10 | Sherin   |16      |25000   |    RUSSIA|       f   |\n",
      "|11 | Ragu     |12      |35000   |    INDIA |        f  |\n",
      "|12 | Sweta    |43      |200000  |   INDIA  |        f  |\n",
      "|13 | Raushan  |48      |650000  |   USA    |          m|\n",
      "|14 | Mukesh   |36      |95000   |    RUSSIA|       m   |\n",
      "|15 | Prakash  |52      |750000  |   INDIA  |        m  |\n",
      "+---+----------+--------+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "      .option('header', True)\\\n",
    "      .option('inferSchema', True)\\\n",
    "      .option('mode', \"PERMISIVE\")\\\n",
    "      .load('csv_write.csv')\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2c28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e36531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('csv')\\\n",
    "        .option('header', True)\\\n",
    "        .option('mode', 'overwrite')\\\n",
    "        .option('path','csv_w/')\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ba92664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created partition with 3 csv files\n",
    "\n",
    "df.repartition(3).write.format('csv')\\\n",
    "        .option('header', True)\\\n",
    "        .option('mode', 'overwrite')\\\n",
    "        .option('path','csv_wr/')\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a4251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7269d91f",
   "metadata": {},
   "source": [
    "# Select column multiple ways and expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeae63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "406b0e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+------+---------+-----------+\n",
      "| id|      name|age|salary|  address|     gender|\n",
      "+---+----------+---+------+---------+-----------+\n",
      "|  1|    Manish| 26| 75000|    INDIA|          m|\n",
      "|  2|    Nikita| 23|100000|      USA|          f|\n",
      "|  3|    Pritam| 22|150000|    INDIA|          m|\n",
      "|  4|  Prantosh| 17|200000|    JAPAN|          m|\n",
      "|  5|    Vikash| 31|300000|      USA|          m|\n",
      "+---+----------+---+------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "          .option('header', True)\\\n",
    "          .option('inferSchema', True)\\\n",
    "          .load('csv_write.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43bd8818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      name|\n",
      "+----------+\n",
      "|    Manish|\n",
      "|    Nikita|\n",
      "|    Pritam|\n",
      "|  Prantosh|\n",
      "|    Vikash|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce8e4c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+------+---------+-----------+\n",
      "| id|      name|age|salary|  address|     gender|\n",
      "+---+----------+---+------+---------+-----------+\n",
      "|  1|    Manish| 26| 75000|    INDIA|          m|\n",
      "|  2|    Nikita| 23|100000|      USA|          f|\n",
      "|  3|    Pritam| 22|150000|    INDIA|          m|\n",
      "|  4|  Prantosh| 17|200000|    JAPAN|          m|\n",
      "|  5|    Vikash| 31|300000|      USA|          m|\n",
      "+---+----------+---+------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0ce12e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      name|\n",
      "+----------+\n",
      "|    Manish|\n",
      "|    Nikita|\n",
      "|    Pritam|\n",
      "|  Prantosh|\n",
      "|    Vikash|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"name\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6207199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---+\n",
      "|      name|      name|      name| id|\n",
      "+----------+----------+----------+---+\n",
      "|    Manish|    Manish|    Manish|  1|\n",
      "|    Nikita|    Nikita|    Nikita|  2|\n",
      "|    Pritam|    Pritam|    Pritam|  3|\n",
      "|  Prantosh|  Prantosh|  Prantosh|  4|\n",
      "|    Vikash|    Vikash|    Vikash|  5|\n",
      "+----------+----------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.select(\"name\", 'age', \"salary\").show(5)\n",
    "#df.select(col(\"name\"), col(\"age\"), col(\"salary\")).show(5)\n",
    "\n",
    "df.select(\"name\", col(\"name\"), df[\"name\"], df.id).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca8630d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|(id + 10)|\n",
      "+---------+\n",
      "|       11|\n",
      "|       12|\n",
      "|       13|\n",
      "|       14|\n",
      "|       15|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using expr manupulate col with select method \n",
    "\n",
    "df.select(expr(\"id + 10\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce1a453a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------------------+\n",
      "|employee_id|  New_Name|concat(name, address)|\n",
      "+-----------+----------+---------------------+\n",
      "|          1|    Manish|      Manish    INDIA|\n",
      "|          2|    Nikita|         Nikita   USA|\n",
      "|          3|    Pritam|       Pritam   INDIA|\n",
      "|          4|  Prantosh|     Prantosh   JAPAN|\n",
      "|          5|    Vikash|         Vikash   USA|\n",
      "+-----------+----------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"id as employee_id\"), expr('name as New_Name'), expr(\"concat(name , address)\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391cf284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6be2bbcb",
   "metadata": {},
   "source": [
    "Aliasing\n",
    "filter / where \n",
    "literal\n",
    "adding COLUMNS\n",
    "renaming columns \n",
    "casting data types \n",
    "removing columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0064c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------+\n",
      "| id|    name|age|salary|     address| nominee|     _c6|\n",
      "+---+--------+---+------+------------+--------+--------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|    NULL|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|    NULL|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|nominee3|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|nominee4|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|    NULL|\n",
      "+---+--------+---+------+------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "          .option('header', True)\\\n",
    "          .option('inferSchema', True)\\\n",
    "          .load('employee_data.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4723d51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|employee_id|    name|salary|\n",
      "+-----------+--------+------+\n",
      "|          1|  Manish| 75000|\n",
      "|          2|  Nikita|100000|\n",
      "|          3|  Pritam|150000|\n",
      "|          4|Prantosh|200000|\n",
      "|          5|  Vikash|300000|\n",
      "+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"id\").alias('employee_id'), 'name', 'salary').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5494450",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|(salary > 100000)|\n",
      "+-----------------+\n",
      "|            false|\n",
      "|            false|\n",
      "|             true|\n",
      "|             true|\n",
      "|             true|\n",
      "+-----------------+\n",
      "\n",
      "+---+--------+---+------+---------+--------+--------+\n",
      "| id|    name|age|salary|  address| nominee|     _c6|\n",
      "+---+--------+---+------+---------+--------+--------+\n",
      "|  3|  Pritam| 22|150000|Bangalore|   India|nominee3|\n",
      "|  4|Prantosh| 17|200000|  Kolkata|   India|nominee4|\n",
      "|  5|  Vikash| 31|300000|     NULL|nominee5|    NULL|\n",
      "+---+--------+---+------+---------+--------+--------+\n",
      "\n",
      "+---+--------+---+------+---------+--------+--------+\n",
      "| id|    name|age|salary|  address| nominee|     _c6|\n",
      "+---+--------+---+------+---------+--------+--------+\n",
      "|  3|  Pritam| 22|150000|Bangalore|   India|nominee3|\n",
      "|  4|Prantosh| 17|200000|  Kolkata|   India|nominee4|\n",
      "|  5|  Vikash| 31|300000|     NULL|nominee5|    NULL|\n",
      "+---+--------+---+------+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"salary\") > 100000).show(5)\n",
    "df.filter(col(\"salary\") > 100000).show()\n",
    "df.where(col(\"salary\") > 100000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afa42f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+---------+-------+--------+\n",
      "| id|    name|age|salary|  address|nominee|     _c6|\n",
      "+---+--------+---+------+---------+-------+--------+\n",
      "|  3|  Pritam| 22|150000|Bangalore|  India|nominee3|\n",
      "|  4|Prantosh| 17|200000|  Kolkata|  India|nominee4|\n",
      "+---+--------+---+------+---------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((col(\"salary\") > 100000) & (col(\"age\") < 30)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0eebdda3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------+---------+\n",
      "| id|    name|age|salary|     address| nominee|     _c6|Last_name|\n",
      "+---+--------+---+------+------------+--------+--------+---------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|    NULL|   Kishor|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|    NULL|   Kishor|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|nominee3|   Kishor|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|nominee4|   Kishor|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|    NULL|   Kishor|\n",
      "+---+--------+---+------+------------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\", lit(\"Kishor\").alias('Last_name')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d673221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------+--------+\n",
      "| id|    name|age|salary|     address| nominee|     _c6|sur_name|\n",
      "+---+--------+---+------+------------+--------+--------+--------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|    NULL|  thakre|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|    NULL|  thakre|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|nominee3|  thakre|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|nominee4|  thakre|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|    NULL|  thakre|\n",
      "+---+--------+---+------+------------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('sur_name', lit('thakre')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f270a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+------+------------+--------+--------+\n",
      "|employee_id|    name|age|salary|     address| nominee|     _c6|\n",
      "+-----------+--------+---+------+------------+--------+--------+\n",
      "|          1|  Manish| 26| 75000|       bihar|nominee1|    NULL|\n",
      "|          2|  Nikita| 23|100000|uttarpradesh|nominee2|    NULL|\n",
      "|          3|  Pritam| 22|150000|   Bangalore|   India|nominee3|\n",
      "|          4|Prantosh| 17|200000|     Kolkata|   India|nominee4|\n",
      "|          5|  Vikash| 31|300000|        NULL|nominee5|    NULL|\n",
      "+-----------+--------+---+------+------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('id', 'employee_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d59bab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- nominee: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33d190ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- nominee: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"id\", col('id').cast('string'))\\\n",
    "    .withColumn(\"salary\", col('salary').cast('long')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e04c3267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------------+--------+--------+\n",
      "|    name|age|     address| nominee|     _c6|\n",
      "+--------+---+------------+--------+--------+\n",
      "|  Manish| 26|       bihar|nominee1|    NULL|\n",
      "|  Nikita| 23|uttarpradesh|nominee2|    NULL|\n",
      "|  Pritam| 22|   Bangalore|   India|nominee3|\n",
      "|Prantosh| 17|     Kolkata|   India|nominee4|\n",
      "|  Vikash| 31|        NULL|nominee5|    NULL|\n",
      "+--------+---+------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('id', col('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864b0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c07c5b52",
   "metadata": {},
   "source": [
    "# union vs unionAll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456854a",
   "metadata": {},
   "source": [
    "union and union all will give same records in spark dataframe. \n",
    "\n",
    "But in pyspl $union$ will give $unique $ record from both table and $union-all$ give $all$ records including duplicate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d7ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede69aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)] \n",
    "\n",
    "schema = ['id', 'name', 'sal', 'mngr_id']\n",
    "m_df = spark.createDataFrame(data, schema)\n",
    "m_df.show()\n",
    "m_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1c70050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------+\n",
      "| id| name|  sal|mngr_id|\n",
      "+---+-----+-----+-------+\n",
      "| 19|Sohan|50000|     18|\n",
      "| 20| Sima|75000|     17|\n",
      "+---+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1=[(19 ,'Sohan',50000, 18),\n",
    "(20 ,'Sima',75000,  17)]\n",
    "schema1 = ['id', 'name', 'sal', 'mngr_id']\n",
    "m_df1 = spark.createDataFrame(data1, schema1)\n",
    "m_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61bec33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|  name|  sal|mngr_id|\n",
      "+---+------+-----+-------+\n",
      "| 10|  Anil|50000|     18|\n",
      "| 11| Vikas|75000|     16|\n",
      "| 12| Nisha|40000|     18|\n",
      "| 13| Nidhi|60000|     17|\n",
      "| 14| Priya|80000|     18|\n",
      "| 15| Mohit|45000|     18|\n",
      "| 16|Rajesh|90000|     10|\n",
      "| 17| Raman|55000|     16|\n",
      "| 18|   Sam|65000|     17|\n",
      "| 19| Sohan|50000|     18|\n",
      "| 20|  Sima|75000|     17|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df.union(m_df1).show()\n",
    "m_df.union(m_df1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unordered column name union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "018070e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+\n",
      "| id|  sal|mngr_id| Name|\n",
      "+---+-----+-------+-----+\n",
      "| 19|50000|     18|Sohan|\n",
      "| 20|75000|     17| Sima|\n",
      "+---+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrong_column_data=[(19 ,50000, 18,'Sohan'), (20 ,75000,  17,'Sima')]\n",
    "wrong_schema = ['id', 'sal', 'mngr_id', 'Name' ]\n",
    "w_m_df = spark.createDataFrame(data =wrong_column_data , schema= wrong_schema)\n",
    "w_m_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c00811d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------+\n",
      "| id| name|  sal|mngr_id|\n",
      "+---+-----+-----+-------+\n",
      "| 19|Sohan|50000|     18|\n",
      "| 20| Sima|75000|     17|\n",
      "| 19|50000|   18|  Sohan|\n",
      "| 20|75000|   17|   Sima|\n",
      "+---+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_df1.union(w_m_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a92b0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unionByName  to match column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44b89a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------+\n",
      "| id| name|  sal|mngr_id|\n",
      "+---+-----+-----+-------+\n",
      "| 19|Sohan|50000|     18|\n",
      "| 20| Sima|75000|     17|\n",
      "| 19|Sohan|50000|     18|\n",
      "| 20| Sima|75000|     17|\n",
      "+---+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_df1.unionByName(w_m_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling extra column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5c12084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+-----+\n",
      "| id|  sal|mngr_id| Name|bonus|\n",
      "+---+-----+-------+-----+-----+\n",
      "| 19|50000|     18|Sohan|   10|\n",
      "| 20|75000|     17| Sima|   20|\n",
      "+---+-----+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrong_column_data=[(19 ,50000, 18,'Sohan',10),\n",
    "(20 ,75000,  17,'Sima',20)]\n",
    "wrong_schema = ['id', 'sal', 'mngr_id', 'Name','bonus']\n",
    "w_m_df1 = spark.createDataFrame(data =wrong_column_data , schema= wrong_schema)\n",
    "w_m_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9cd3063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+\n",
      "| id|  sal|mngr_id| Name|\n",
      "+---+-----+-------+-----+\n",
      "| 19|50000|     18|Sohan|\n",
      "| 20|75000|     17| Sima|\n",
      "| 19|50000|     18|Sohan|\n",
      "| 20|75000|     17| Sima|\n",
      "+---+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_m_df1.select('id', 'sal', 'mngr_id', 'Name').union(w_m_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d3e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "793820e5",
   "metadata": {},
   "source": [
    "# if else in spark (case when then)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e41b91d2",
   "metadata": {},
   "source": [
    "withColumn - otherwise in spark dataframe and case - when in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd11fb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n",
      "|  id|   name| age|salary|country|       dept|\n",
      "+----+-------+----+------+-------+-----------+\n",
      "|   1| manish|  26| 20000|  india|         IT|\n",
      "|   2|  rahul|NULL| 40000|germany|engineering|\n",
      "|   3|  pawan|  12| 60000|  india|      sales|\n",
      "|   4|roshini|  44|  NULL|     uk|engineering|\n",
      "|   5|raushan|  35| 70000|  india|      sales|\n",
      "|   6|   NULL|  29|200000|     uk|         IT|\n",
      "|   7|   adam|  37| 65000|     us|         IT|\n",
      "|   8|  chris|  16| 40000|     us|      sales|\n",
      "|NULL|   NULL|NULL|  NULL|   NULL|       NULL|\n",
      "|   7|   adam|  37| 65000|     us|         IT|\n",
      "+----+-------+----+------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "emp_schema = ['id', 'name', 'age', 'salary', 'country', 'dept']\n",
    "e_df = spark.createDataFrame(data = emp_data, schema=emp_schema)\n",
    "e_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d940ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bde395f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|  id|   name| age|salary|country|       dept|   adult|\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|   1| manish|  26| 20000|  india|         IT|     Yes|\n",
      "|   2|  rahul|NULL| 40000|germany|engineering|Novalues|\n",
      "|   3|  pawan|  12| 60000|  india|      sales|      No|\n",
      "|   4|roshini|  44|  NULL|     uk|engineering|     Yes|\n",
      "|   5|raushan|  35| 70000|  india|      sales|     Yes|\n",
      "|   6|   NULL|  29|200000|     uk|         IT|     Yes|\n",
      "|   7|   adam|  37| 65000|     us|         IT|     Yes|\n",
      "|   8|  chris|  16| 40000|     us|      sales|      No|\n",
      "|NULL|   NULL|NULL|  NULL|   NULL|       NULL|Novalues|\n",
      "|   7|   adam|  37| 65000|     us|         IT|     Yes|\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e_df.withColumn('adult', when(col('age')<18, 'No').when(col('age')>18, 'Yes').otherwise('Novalues')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987b3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72fdd9da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+------+-------+-----------+-----+\n",
      "|  id|   name|age|salary|country|       dept|adult|\n",
      "+----+-------+---+------+-------+-----------+-----+\n",
      "|   1| manish| 26| 20000|  india|         IT|  Yes|\n",
      "|   2|  rahul| 19| 40000|germany|engineering|  Yes|\n",
      "|   3|  pawan| 12| 60000|  india|      sales|   No|\n",
      "|   4|roshini| 44|  NULL|     uk|engineering|  Yes|\n",
      "|   5|raushan| 35| 70000|  india|      sales|  Yes|\n",
      "|   6|   NULL| 29|200000|     uk|         IT|  Yes|\n",
      "|   7|   adam| 37| 65000|     us|         IT|  Yes|\n",
      "|   8|  chris| 16| 40000|     us|      sales|   No|\n",
      "|NULL|   NULL| 19|  NULL|   NULL|       NULL|  Yes|\n",
      "|   7|   adam| 37| 65000|     us|         IT|  Yes|\n",
      "+----+-------+---+------+-------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e_df.withColumn('age', when(col('age').isNull(),lit(19)).otherwise(col('age')))\\\n",
    "    .withColumn('adult',when(col('age')>18, 'Yes').otherwise('No')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f40bc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|  id|   name| age|salary|country|       dept|age_wise|\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|   1| manish|  26| 20000|  india|         IT|     Mid|\n",
      "|   2|  rahul|NULL| 40000|germany|engineering|   Major|\n",
      "|   3|  pawan|  12| 60000|  india|      sales|   Minor|\n",
      "|   4|roshini|  44|  NULL|     uk|engineering|   Major|\n",
      "|   5|raushan|  35| 70000|  india|      sales|   Major|\n",
      "|   6|   NULL|  29|200000|     uk|         IT|     Mid|\n",
      "|   7|   adam|  37| 65000|     us|         IT|   Major|\n",
      "|   8|  chris|  16| 40000|     us|      sales|   Minor|\n",
      "|NULL|   NULL|NULL|  NULL|   NULL|       NULL|   Major|\n",
      "|   7|   adam|  37| 65000|     us|         IT|   Major|\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e_df.withColumn('age_wise', when((col('age')>0) & (col('age')<18),'Minor')\\\n",
    "                           .when((col('age')>18) & (col('age')<30), 'Mid')\n",
    "                           s.otherwise('Major')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb20e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_df.createOrReplaceTempView('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8ba6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+-------+\n",
      "|  id|   name| age|salary|country|       dept|  adult|\n",
      "+----+-------+----+------+-------+-----------+-------+\n",
      "|   1| manish|  26| 20000|  india|         IT|  major|\n",
      "|   2|  rahul|NULL| 40000|germany|engineering|novalue|\n",
      "|   3|  pawan|  12| 60000|  india|      sales|  minor|\n",
      "|   4|roshini|  44|  NULL|     uk|engineering|  major|\n",
      "|   5|raushan|  35| 70000|  india|      sales|  major|\n",
      "|   6|   NULL|  29|200000|     uk|         IT|  major|\n",
      "|   7|   adam|  37| 65000|     us|         IT|  major|\n",
      "|   8|  chris|  16| 40000|     us|      sales|  minor|\n",
      "|NULL|   NULL|NULL|  NULL|   NULL|       NULL|novalue|\n",
      "|   7|   adam|  37| 65000|     us|         IT|  major|\n",
      "+----+-------+----+------+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select *, \n",
    "             case when age<18 then 'minor'\n",
    "\t\t\t when age> 18 then 'major'\n",
    "\t\t\t else 'novalue'\n",
    "\t\t\t end as adult\n",
    "\t\t\t from table\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e6194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "885c3462",
   "metadata": {},
   "source": [
    "# Unique & Sorted records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f8acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n",
      "| id|  name|salary|mngr_id|\n",
      "+---+------+------+-------+\n",
      "| 10|  Anil| 50000|     18|\n",
      "| 11| Vikas| 75000|     16|\n",
      "| 12| Nisha| 40000|     18|\n",
      "| 13| Nidhi| 60000|     17|\n",
      "| 14| Priya| 80000|     18|\n",
      "| 15| Mohit| 45000|     18|\n",
      "| 16|Rajesh| 90000|     10|\n",
      "| 17| Raman| 55000|     16|\n",
      "| 18|   Sam| 65000|     17|\n",
      "| 15| Mohit| 45000|     18|\n",
      "| 13| Nidhi| 60000|     17|\n",
      "| 14| Priya| 90000|     18|\n",
      "| 18|   Sam| 65000|     17|\n",
      "+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(13 ,'Nidhi',60000,  17),      \n",
    "(14 ,'Priya',90000,  18),  \n",
    "(18 ,'Sam',65000,   17)\n",
    "     ]\n",
    "sch = ['id','name','salary','mngr_id']\n",
    "df = spark.createDataFrame(data, schema=sch)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "094fe0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef890fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f7306f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "| 10|  Anil|\n",
      "| 11| Vikas|\n",
      "| 12| Nisha|\n",
      "| 13| Nidhi|\n",
      "| 15| Mohit|\n",
      "| 14| Priya|\n",
      "| 17| Raman|\n",
      "| 16|Rajesh|\n",
      "| 18|   Sam|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id','name').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd28a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n",
      "| id|  name|salary|mngr_id|\n",
      "+---+------+------+-------+\n",
      "| 10|  Anil| 50000|     18|\n",
      "| 12| Nisha| 40000|     18|\n",
      "| 11| Vikas| 75000|     16|\n",
      "| 13| Nidhi| 60000|     17|\n",
      "| 15| Mohit| 45000|     18|\n",
      "| 14| Priya| 80000|     18|\n",
      "| 16|Rajesh| 90000|     10|\n",
      "| 17| Raman| 55000|     16|\n",
      "| 18|   Sam| 65000|     17|\n",
      "| 14| Priya| 90000|     18|\n",
      "+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(['id','name','salary','mngr_id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7917fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n",
      "| id|  name|salary|mngr_id|\n",
      "+---+------+------+-------+\n",
      "| 12| Nisha| 40000|     18|\n",
      "| 15| Mohit| 45000|     18|\n",
      "| 15| Mohit| 45000|     18|\n",
      "| 10|  Anil| 50000|     18|\n",
      "| 17| Raman| 55000|     16|\n",
      "| 13| Nidhi| 60000|     17|\n",
      "| 13| Nidhi| 60000|     17|\n",
      "| 18|   Sam| 65000|     17|\n",
      "| 18|   Sam| 65000|     17|\n",
      "| 11| Vikas| 75000|     16|\n",
      "| 14| Priya| 80000|     18|\n",
      "| 14| Priya| 90000|     18|\n",
      "| 16|Rajesh| 90000|     10|\n",
      "+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort \n",
    "df.sort(col('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fb012a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n",
      "| id|  name|salary|mngr_id|\n",
      "+---+------+------+-------+\n",
      "| 16|Rajesh| 90000|     10|\n",
      "| 14| Priya| 90000|     18|\n",
      "| 14| Priya| 80000|     18|\n",
      "| 11| Vikas| 75000|     16|\n",
      "| 18|   Sam| 65000|     17|\n",
      "| 18|   Sam| 65000|     17|\n",
      "| 13| Nidhi| 60000|     17|\n",
      "| 13| Nidhi| 60000|     17|\n",
      "| 17| Raman| 55000|     16|\n",
      "| 10|  Anil| 50000|     18|\n",
      "| 15| Mohit| 45000|     18|\n",
      "| 15| Mohit| 45000|     18|\n",
      "| 12| Nisha| 40000|     18|\n",
      "+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col('salary').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "466004ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n",
      "| id|  name|salary|mngr_id|\n",
      "+---+------+------+-------+\n",
      "| 16|Rajesh| 90000|     10|\n",
      "| 14| Priya| 90000|     18|\n",
      "| 14| Priya| 80000|     18|\n",
      "| 11| Vikas| 75000|     16|\n",
      "| 18|   Sam| 65000|     17|\n",
      "| 18|   Sam| 65000|     17|\n",
      "| 13| Nidhi| 60000|     17|\n",
      "| 13| Nidhi| 60000|     17|\n",
      "| 17| Raman| 55000|     16|\n",
      "| 10|  Anil| 50000|     18|\n",
      "| 15| Mohit| 45000|     18|\n",
      "| 15| Mohit| 45000|     18|\n",
      "| 12| Nisha| 40000|     18|\n",
      "+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col('salary').desc(), col('name').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4407c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8945a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Animal|Enemy|\n",
      "+------+-----+\n",
      "|   Dog|  Cat|\n",
      "|   Cat|  Dog|\n",
      "| Mouse|  Cat|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new column from list values\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "rating = [5,4,1]\n",
    "a= spark.createDataFrame([(\"Dog\", \"Cat\"), (\"Cat\", \"Dog\"), (\"Mouse\", \"Cat\")], [\"Animal\", \"Enemy\"])\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14a5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Rating|\n",
      "+------+\n",
      "|     5|\n",
      "|     4|\n",
      "|     1|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = spark.createDataFrame([(l,) for l in rating], ['Rating'])\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41cd7688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "|Animal|Enemy|row_idx|\n",
      "+------+-----+-------+\n",
      "|   Dog|  Cat|      1|\n",
      "|   Cat|  Dog|      2|\n",
      "| Mouse|  Cat|      3|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = a.withColumn('row_idx', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf81d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|Rating|row_idx|\n",
      "+------+-------+\n",
      "|     5|      1|\n",
      "|     4|      2|\n",
      "|     1|      3|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = b.withColumn('row_idx', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2da10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+\n",
      "|Animal|Enemy|Rating|\n",
      "+------+-----+------+\n",
      "|   Dog|  Cat|     5|\n",
      "|   Cat|  Dog|     4|\n",
      "| Mouse|  Cat|     1|\n",
      "+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.join(b, a.row_idx == b.row_idx).drop('row_idx').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1addb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d765add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n",
      "+----------+\n",
      "|FLOOR(5.2)|\n",
      "+----------+\n",
      "|         5|\n",
      "|         5|\n",
      "|         5|\n",
      "|         5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(4)\n",
    "df.show()\n",
    "df.select(floor(lit(5.2))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98fd8cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|rng|\n",
      "+---+\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(3, 9).toDF('rng').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880e749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf11b8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|AnonID|         Query|\n",
      "+------+--------------+\n",
      "|   142|     Big House|\n",
      "|   142|Big Green Frog|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([[142,\"Big House\"],[142,\"Big Green Frog\"]],[\"AnonID\",\"Query\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3304ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------+\n",
      "|AnonID|Query                      |\n",
      "+------+---------------------------+\n",
      "|142   |[Big House, Big Green Frog]|\n",
      "+------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data= df.groupBy(\"AnonID\").agg(collect_list(\"Query\").alias(\"Query\"))\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dc79fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------+-------------------------+\n",
      "|AnonID|Query                      |distinct                 |\n",
      "+------+---------------------------+-------------------------+\n",
      "|142   |[Big House, Big Green Frog]|[Big, House, Green, Frog]|\n",
      "+------+---------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('distinct', array_distinct(flatten(transform(data['Query'], lambda x:split(x, ' '))))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf436a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5be800f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| value|\n",
      "+---+------+\n",
      "|  1| apple|\n",
      "|  2|banana|\n",
      "|  3|cherry|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting DataFrame Column to List \n",
    "data = [(\"1\", \"apple\"), (\"2\", \"banana\"), (\"3\", \"cherry\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4738c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'cherry']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = df.select('value').rdd.flatMap(lambda x:x).collect()\n",
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa346bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3badee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+---------------------------------+\n",
      "|timestamp          |level|message                          |\n",
      "+-------------------+-----+---------------------------------+\n",
      "|2024-06-13 12:00:00|INFO |Server started                   |\n",
      "|2024-06-13 12:01:00|ERROR|Failed to connect to database    |\n",
      "|2024-06-13 12:02:00|INFO |User login successful            |\n",
      "|2024-06-13 12:03:00|ERROR|Timeout while reading from socket|\n",
      "|2024-06-13 12:04:00|INFO |File uploaded successfully       |\n",
      "+-------------------+-----+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accumulator\n",
    "\n",
    "data = [('2024-06-13 12:00:00', 'INFO', 'Server started'),\n",
    "        ('2024-06-13 12:01:00', 'ERROR', 'Failed to connect to database'),\n",
    "        ('2024-06-13 12:02:00', 'INFO', 'User login successful'),\n",
    "        ('2024-06-13 12:03:00', 'ERROR', 'Timeout while reading from socket'),\n",
    "        ('2024-06-13 12:04:00', 'INFO', 'File uploaded successfully')]\n",
    "columns = ['timestamp', 'level', 'message']\n",
    "df  = spark.createDataFrame(data, columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeb4dc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of error messages: 2\n"
     ]
    }
   ],
   "source": [
    "error_count_accumulator = spark.sparkContext.accumulator(0)\n",
    "\n",
    "#Define a function to increment the accumulator for each error login \n",
    "\n",
    "def count_errors(row):\n",
    "    if row['level'] == 'ERROR':\n",
    "        error_count_accumulator.add(1)\n",
    "        \n",
    "# Use foreach to apply the function to each row in the DataFrame\n",
    "\n",
    "df.foreach(lambda row: count_errors(row))\n",
    "\n",
    "# After the foreach action, the accumulator will hold the count of error messages\n",
    "print(f'Total number of error messages: {error_count_accumulator.value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb5c2682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+---------------------------------+\n",
      "|timestamp          |level|message                          |\n",
      "+-------------------+-----+---------------------------------+\n",
      "|2024-06-13 12:01:00|ERROR|Failed to connect to database    |\n",
      "|2024-06-13 12:03:00|ERROR|Timeout while reading from socket|\n",
      "+-------------------+-----+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Further processing of the DataFrame (e.g. filtering errors, saving results, etc.) \n",
    "\n",
    "error_df = df.filter(col('level') == 'ERROR')\n",
    "error_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d12cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f32f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c27fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
