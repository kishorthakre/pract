{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe057f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, Row \n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DecimalType, FloatType, DateType\n",
    "from pyspark.sql.functions import sum, avg, max, min,count, col, lower,trim, countDistinct\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3f8ae6f",
   "metadata": {},
   "source": [
    "1. Create a SparkSession in PySpark.\n",
    "2. Read a CSV file into a DataFrame using PySpark.\n",
    "3. Show the schema of a DataFrame in PySpark.\n",
    "4. Select specific columns from a DataFrame in PySpark.\n",
    "5. Filter rows based on a condition in PySpark DataFrame.\n",
    "6. Group by a column and perform an aggregation in PySpark.\n",
    "7. Join two DataFrames in PySpark.\n",
    "8. Rename columns in a PySpark DataFrame.\n",
    "9. Handle missing or null values in PySpark DataFrame.\n",
    "10. Create a new column derived from existing columns in PySpark DataFrame.\n",
    "11. Remove duplicate rows from a PySpark DataFrame.\n",
    "12. Sort a DataFrame based on one or multiple columns in PySpark.\n",
    "13. Perform a simple arithmetic operation on DataFrame columns in PySpark.\n",
    "14. Calculate descriptive statistics for numeric columns in PySpark.\n",
    "15. Apply user-defined functions (UDF) on PySpark DataFrame.\n",
    "16. Convert a PySpark DataFrame to Pandas DataFrame.\n",
    "17. Write a PySpark DataFrame to a CSV file.\n",
    "18. Cache or persist a PySpark DataFrame for better performance.\n",
    "19. Handle broadcast joins in PySpark.\n",
    "20. Perform window functions in PySpark (e.g., rank, row number, etc.).\n",
    "21. Handle nested structures or arrays in PySpark DataFrame.\n",
    "22. Handle time-series data in PySpark.\n",
    "23. Calculate the correlation between columns in a PySpark DataFrame.\n",
    "24. Create a pivot table in PySpark.\n",
    "25. Perform cross-tabulation (crosstab) in PySpark.\n",
    "26. Handle large-scale data using PySpark (memory management, optimizations).\n",
    "27. Handle skewed data in PySpark.\n",
    "28. Perform machine learning tasks (e.g., regression, classification) using PySpark MLlib.\n",
    "29. Optimize PySpark jobs for performance (tuning configurations, parallelism, etc.).\n",
    "30. Handle different file formats (Parquet, Avro, ORC) in PySpark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e88d67",
   "metadata": {},
   "source": [
    "### 1. Createa SparkSession in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461b1784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3V2ROQ70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Q</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23545fb5750>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Q').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ece899",
   "metadata": {},
   "source": [
    "### 2. Read a CSV file into a DataFrame using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13bd20ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c150b5",
   "metadata": {},
   "source": [
    "### 3. show the schema of a Dataframe in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8675762f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06f7165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('employee_name', StringType(), True), StructField('department', StringType(), True), StructField('state', StringType(), True), StructField('salary', LongType(), True), StructField('age', LongType(), True), StructField('bonus', LongType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0629b0",
   "metadata": {},
   "source": [
    "### 4. Select specific columns from a dataframe in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e2ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|salary|\n",
      "+---+------+\n",
      "| 34| 90000|\n",
      "| 56| 86000|\n",
      "| 30| 81000|\n",
      "| 24| 90000|\n",
      "| 40| 99000|\n",
      "| 36| 83000|\n",
      "| 53| 79000|\n",
      "| 25| 80000|\n",
      "| 50| 91000|\n",
      "+---+------+\n",
      "\n",
      "+---+------+\n",
      "|age|salary|\n",
      "+---+------+\n",
      "| 34| 90000|\n",
      "| 56| 86000|\n",
      "| 30| 81000|\n",
      "| 24| 90000|\n",
      "| 40| 99000|\n",
      "| 36| 83000|\n",
      "| 53| 79000|\n",
      "| 25| 80000|\n",
      "| 50| 91000|\n",
      "+---+------+\n",
      "\n",
      "+---+------+\n",
      "|age|salary|\n",
      "+---+------+\n",
      "| 34| 90000|\n",
      "| 56| 86000|\n",
      "| 30| 81000|\n",
      "| 24| 90000|\n",
      "| 40| 99000|\n",
      "| 36| 83000|\n",
      "| 53| 79000|\n",
      "| 25| 80000|\n",
      "| 50| 91000|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age','salary').show()\n",
    "df.select(df['age'], df['salary']).show()\n",
    "df.select(df.age, df.salary).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75288328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55e69837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+\n",
      "|employee_name|department|state|\n",
      "+-------------+----------+-----+\n",
      "|        James|     Sales|   NY|\n",
      "|      Michael|     Sales|   NY|\n",
      "|       Robert|     Sales|   CA|\n",
      "+-------------+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select first 3 column and top 3 rows \n",
    "\n",
    "df.select(df.columns[:3]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046abb2",
   "metadata": {},
   "source": [
    "### 5. Filter rows based on a condition in pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25fb8cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Salary < 25000').show()\n",
    "df.filter(df['Salary'] < 25000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11968f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`age`, `state`, `bonus`, `salary`, `department`].;\n'Project ['Name, age#4L]\n+- Filter (Salary#3L < cast(25000 as bigint))\n   +- LogicalRDD [employee_name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalary < 25000\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py:3227\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   3182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   3183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3184\u001b[0m \n\u001b[0;32m   3185\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3225\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3227\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jcols(\u001b[38;5;241m*\u001b[39mcols))\n\u001b[0;32m   3228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`age`, `state`, `bonus`, `salary`, `department`].;\n'Project ['Name, age#4L]\n+- Filter (Salary#3L < cast(25000 as bigint))\n   +- LogicalRDD [employee_name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n"
     ]
    }
   ],
   "source": [
    "df.filter('Salary < 25000').select(['Name', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e68b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df['Salary'] < 25000) & (df['Experience'] < 3 )).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT Operation \n",
    "\n",
    "df.filter(~(df['Salary'] <= 25000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a2d36",
   "metadata": {},
   "source": [
    "### 6. Group by a Column and perform an aggregation in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e982e0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "493e4bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|Sales     |3    |\n",
      "|Finance   |4    |\n",
      "|Marketing |2    |\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').count().show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0b7cf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').sum('salary').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05f0cf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |81000      |\n",
      "|Finance   |79000      |\n",
      "|Marketing |80000      |\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |90000      |\n",
      "|Finance   |99000      |\n",
      "|Marketing |91000      |\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|department|avg(salary)      |\n",
      "+----------+-----------------+\n",
      "|Sales     |85666.66666666667|\n",
      "|Finance   |87750.0          |\n",
      "|Marketing |85500.0          |\n",
      "+----------+-----------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|department|avg(salary)      |\n",
      "+----------+-----------------+\n",
      "|Sales     |85666.66666666667|\n",
      "|Finance   |87750.0          |\n",
      "|Marketing |85500.0          |\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').min('salary').show(truncate = False)\n",
    "df.groupBy('department').max('salary').show(truncate = False)\n",
    "df.groupBy('department').avg('salary').show(truncate = False)\n",
    "df.groupBy('department').mean('salary').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0004c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|min(salary)|min(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|Sales     |NY   |86000      |10000     |\n",
      "|Sales     |CA   |81000      |23000     |\n",
      "|Finance   |CA   |90000      |23000     |\n",
      "|Finance   |NY   |79000      |15000     |\n",
      "|Marketing |NY   |91000      |21000     |\n",
      "|Marketing |CA   |80000      |18000     |\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GroupBy on multiple columns \n",
    "\n",
    "df.groupBy('department', 'state').min('salary', 'bonus').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d83d5d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |min_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|10000    |23000    |\n",
      "|Finance   |351000    |87750.0          |15000    |24000    |\n",
      "|Marketing |171000    |85500.0          |18000    |21000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').agg(sum('salary').alias('sum_salary'), avg('salary').alias('avg_salary'),\\\n",
    "                              min('bonus').alias('min_bonus'), max('bonus').alias('max_bonus')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593788db",
   "metadata": {},
   "source": [
    "### 7. Join two dataframes in pyspark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff609bd8",
   "metadata": {},
   "source": [
    "Inner Join: Returns only the rows with matching keys in both Dataframes. \n",
    "Left Join: Returns all rows from the left dataframe and matching rows from the right dataframe. \n",
    "Right Join: Returns all rows from the right dataframe and matching rows from the left dataframe. \n",
    "Full Outer Join: Returns all rows from both dataframes, including matching and non-matching rows. \n",
    "    \n",
    "Left semi Join: Returns all rows from the left dataframe where there is a match in the right dataframe. \n",
    "Left anti Join: Returns all rows from the left dataframe where there is no match in the right dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdc6cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "empdf = spark.createDataFrame(data = emp, schema = empColumns)\n",
    "empdf.show(truncate=False) \n",
    "\n",
    "ept = [(\"Finance\",10), (\"Marketing\",20), (\"Sales\",30), (\"IT\",40) ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptdf = spark.createDataFrame(data = ept, schema = deptColumns)\n",
    "deptdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "babf5c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pyspark Inner Join dataframe \n",
    "\n",
    "empdf.join(deptdf, empdf.emp_dept_id == deptdf.dept_id, 'inner').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc8f3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark Left Outer Join"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80ffc221",
   "metadata": {},
   "source": [
    "Left aks leftouter join returns all rows from the left dataset regardless of match found on the right dataset when join expression doesn't match, it assigns null for that record and drops records from right where match not found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86631b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.join(deptdf, empdf.emp_dept_id == deptdf.dept_id, 'left').show(truncate =False)\n",
    "#empdf.join(deptdf, empdf.emp_dept_id == deptdf.dept_id, 'leftouter').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ead7aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right Outer Join "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1e2b9e2",
   "metadata": {},
   "source": [
    "Right aks rightouter join is opposite of left join, here it return all rows from the right dataset ragardless of match found on the left dataset, when join expression doesn't match, it assigns null for that record and drops records from left where match not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56001629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.join(deptdf, empdf.emp_dept_id == deptdf.dept_id, 'right').show(truncate=False)\n",
    "#empdf.join(deptdf, empdf.emp_dept_id == deptdf.dept_id, 'rightouter').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1ccdd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PYspark Full Outer Join\n",
    "\n",
    "empdf.join(deptdf, empdf.emp_dept_id == deptdf.dept_id, 'outer').show(truncate=False)\n",
    "empdf.join(deptdf, empdf.emp_dept_id == deptdf.dept_id, 'full').show(truncate=False)\n",
    "empdf.join(deptdf, empdf.emp_dept_id == deptdf.dept_id, 'fullouter').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fde65ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left semi join\n",
    "\n",
    "empdf.join(deptdf, empdf.emp_dept_id == deptdf.dept_id, 'leftsemi').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a66136da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left anti join \n",
    "\n",
    "empdf.join(deptdf, empdf.emp_dept_id == deptdf.dept_id, 'leftanti').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60d8a0",
   "metadata": {},
   "source": [
    "### 8. Rename columns in a Pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04efd887",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_ACCEPT_OBJECT_IN_TYPE] `DateType()` can not accept object `1991-04-01` in type `str`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m dataDF \u001b[38;5;241m=\u001b[39m [((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJames\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmith\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1991-04-01\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m3000\u001b[39m),\n\u001b[0;32m      2\u001b[0m   ((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMichael\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRose\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2000-05-19\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m4000\u001b[39m),\n\u001b[0;32m      3\u001b[0m   ((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRobert\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWilliams\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1978-09-05\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m4000\u001b[39m),\n\u001b[0;32m      4\u001b[0m   ((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaria\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnne\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJones\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1967-12-01\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m4000\u001b[39m),\n\u001b[0;32m      5\u001b[0m   ((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJen\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMary\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBrown\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1980-02-17\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m ]\n\u001b[0;32m      7\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, StructType([StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirstname\u001b[39m\u001b[38;5;124m'\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      8\u001b[0m                                                     \n\u001b[0;32m      9\u001b[0m                                                      StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiddlename\u001b[39m\u001b[38;5;124m'\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m                     StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msalary\u001b[39m\u001b[38;5;124m'\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m                     ])\n\u001b[1;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data \u001b[38;5;241m=\u001b[39m dataDF, schema\u001b[38;5;241m=\u001b[39m schema)\n\u001b[0;32m     16\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m     17\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1442\u001b[0m     )\n\u001b[1;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m )\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[1;32m-> 1459\u001b[0m     verify_func(obj)\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[1;32m-> 2187\u001b[0m         verify_value(obj)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\types.py:2160\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[0;32m   2151\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2152\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2157\u001b[0m             },\n\u001b[0;32m   2158\u001b[0m         )\n\u001b[0;32m   2159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[1;32m-> 2160\u001b[0m         verifier(v)\n\u001b[0;32m   2161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2162\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[1;32m-> 2187\u001b[0m         verify_value(obj)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\types.py:2181\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_default\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_default\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2180\u001b[0m     assert_acceptable_types(obj)\n\u001b[1;32m-> 2181\u001b[0m     verify_acceptable_types(obj)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\types.py:2006\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2004\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[1;32m-> 2006\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   2007\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_ACCEPT_OBJECT_IN_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2008\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   2009\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(dataType),\n\u001b[0;32m   2010\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(obj),\n\u001b[0;32m   2011\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m   2012\u001b[0m             },\n\u001b[0;32m   2013\u001b[0m         )\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DateType()` can not accept object `1991-04-01` in type `str`."
     ]
    }
   ],
   "source": [
    "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
    "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
    "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
    "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
    "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
    "]\n",
    "schema = StructType([StructField('name', StructType([StructField('firstname', StringType(), True),\n",
    "                                                    \n",
    "                                                     StructField('middlename', StringType(), True),\n",
    "                                                     StructField('lastname', StringType(), True)])),\n",
    "                    StructField('dob', DateType(), True),\n",
    "                    StructField('gender', StringType(), True),\n",
    "                    StructField('salary', IntegerType(), True)\n",
    "                    ])\n",
    "df = spark.createDataFrame(data = dataDF, schema= schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark withcolumnRenamed() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5003d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumnRenamed('dob', 'DateofBirth').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e11a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c484ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumnRenamed('dob', 'DateOfBirth').withColumnRenamed('salary', 'salary_amount')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pyspark StrutType - To rename a nested column in DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f84660",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2 = StructType([\n",
    "    StructField('fname', StringType()),\n",
    "    StructField('middlename', StringType()),\n",
    "    StructField('Lname', StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf51508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.select(col('name').cast(schema2), col('dob'),col('gender'), col('salary'))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7016387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.select('name.*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pyspark dataframe withcolumn - to rename nested columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.withColumn('fname', col('name.firstname')).withColumn('mname', col('name.middlename')).withColumn('lname',col('name.lastname'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using toDF() - to change all columns in a pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb113205",
   "metadata": {},
   "outputs": [],
   "source": [
    "newColumns = ['newCol1', 'newCol2', 'newCol3', 'newCol4']\n",
    "df.toDF(*newColumns).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af01b3",
   "metadata": {},
   "source": [
    "### 9. Handle missing or null values in DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c44b74b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
      "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|      NULL|\n",
      "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               NULL|   TX|      NULL|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('small_zipcode.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4329d1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
      "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               NULL|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
      "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               NULL|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace 0 for null for all integer columns\n",
    "df.na.fill(value=0).show()\n",
    "\n",
    "# Replace 0 for null on only population column\n",
    "df.na.fill(value=0, subset=['population']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc65a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|                   |   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|                   |   TX|      NULL|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39164bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------------------+-----+----------+\n",
      "| id|zipcode|      type|               city|state|population|\n",
      "+---+-------+----------+-------------------+-----+----------+\n",
      "|  1|    704|  STANDARD|          --HELLO--|   PR|     30100|\n",
      "|  2|    704|----HI----|PASEO COSTA DEL SUR|   PR|      NULL|\n",
      "|  3|    709|----HI----|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|    UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|  STANDARD|          --HELLO--|   TX|      NULL|\n",
      "+---+-------+----------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('--HELLO--', ['city']).na.fill('----HI----', ['type']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "518a98fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|              empty|   PR|     30100|\n",
      "|  2|    704|      YO|PASEO COSTA DEL SUR|   PR|      NULL|\n",
      "|  3|    709|      YO|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|              empty|   TX|      NULL|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill({'type':'YO', 'city':'empty'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62bda88",
   "metadata": {},
   "source": [
    "### 10. Cretae a new column derived from existing columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ade2720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)]\n",
    "columns = ['firstname', 'middlename','lastname', 'dob', 'gender','salary']\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f621ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|copiedColumn|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|       -3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|       -4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|       -4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|       -4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           1|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('copiedColumn', col('salary')* -1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b1ebe",
   "metadata": {},
   "source": [
    "### 11. Remove duplicate rows from a Pyspark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72e4b01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d90df9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct count: 9\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying distinct() to remove duplicate rows \n",
    "\n",
    "distinctDF=df.distinct()\n",
    "#df2 = df.dropDuplicate()\n",
    "print('distinct count: '+str(distinctDF.count()))\n",
    "distinctDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61ed6cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct count:  8\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        Maria|   Finance|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|       Robert|     Sales|  4100|\n",
      "|      Michael|     Sales|  4600|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropDisDF = df.dropDuplicates(['department', 'salary'])\n",
    "print('distinct count: ', str(dropDisDF.count()))\n",
    "dropDisDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035359c",
   "metadata": {},
   "source": [
    "### 12. Sort a dataframe based on one or multiple  columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3798e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e17dc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame sorting using the sort() function\n",
    "# sort() takes a Boolean argument for ascending or descending order. \n",
    "\n",
    "#df.sort('department', 'state').show()\n",
    "df.sort('department', 'state', ascending=[True, True]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66791bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col('department'), col('state')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74812ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame sorting using orderBy() function \n",
    "# by default, it orders by ascending.\n",
    "\n",
    "#df.orderBy('department', 'state').show()\n",
    "df.orderBy(col('department'), col('state')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93d671c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort by ascending ASC / DESC\n",
    "\n",
    "df.orderBy(col('department').asc(), col('state').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac309df",
   "metadata": {},
   "source": [
    "### 13. Perform a simple arithmetic operation on DF columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2e7f3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "| 100|   2|   1|\n",
      "| 200|   3|   4|\n",
      "| 300|   4|   4|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(100, 2,1), (200, 3,4), (300, 4,4 )]\n",
    "df = spark.createDataFrame(data).toDF('col1', 'col2', 'col3')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7387ea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col1 + col2)|\n",
      "+-------------+\n",
      "|          102|\n",
      "|          203|\n",
      "|          304|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 - col2)|\n",
      "+-------------+\n",
      "|           98|\n",
      "|          197|\n",
      "|          296|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 * col2)|\n",
      "+-------------+\n",
      "|          200|\n",
      "|          600|\n",
      "|         1200|\n",
      "+-------------+\n",
      "\n",
      "+-----------------+\n",
      "|    (col1 / col2)|\n",
      "+-----------------+\n",
      "|             50.0|\n",
      "|66.66666666666667|\n",
      "|             75.0|\n",
      "+-----------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 % col2)|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            2|\n",
      "|            0|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 > col2)|\n",
      "+-------------+\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 < col2)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 = col2)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show()\n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "\n",
    "df.select(df.col1 > df.col2).show()\n",
    "df.select(df.col1 < df.col2).show()\n",
    "df.select(df.col1 == df.col2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e8e39",
   "metadata": {},
   "source": [
    "### 14. calculate descriptive statistics for numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54e6fc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e02ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+-----+-----------------+------------------+------------------+\n",
      "|summary|employee_name|department|state|           salary|               age|             bonus|\n",
      "+-------+-------------+----------+-----+-----------------+------------------+------------------+\n",
      "|  count|            9|         9|    9|                9|                 9|                 9|\n",
      "|   mean|         NULL|      NULL| NULL|86555.55555555556|38.666666666666664|19222.222222222223|\n",
      "| stddev|         NULL|      NULL| NULL|6540.472290116195|11.947803145348521| 4465.920335658087|\n",
      "|    min|        James|   Finance|   CA|            79000|                24|             10000|\n",
      "|    max|        Scott|     Sales|   NY|            99000|                56|             24000|\n",
      "+-------+-------------+----------+-----+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "624e254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|                 9|\n",
      "|   mean|38.666666666666664|\n",
      "| stddev|11.947803145348521|\n",
      "|    min|                24|\n",
      "|    max|                56|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b93d8e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+\n",
      "|summary|department|               age|\n",
      "+-------+----------+------------------+\n",
      "|  count|         9|                 9|\n",
      "|   mean|      NULL|38.666666666666664|\n",
      "| stddev|      NULL|11.947803145348521|\n",
      "|    min|   Finance|                24|\n",
      "|    max|     Sales|                56|\n",
      "+-------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('department', 'age').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aba15e",
   "metadata": {},
   "source": [
    "### 15. Apply user-defined functions (UDF) on DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "601fa6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[314] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept = [('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee22cc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|  Finance| 10|\n",
      "|Marketing| 20|\n",
      "|    Sales| 30|\n",
      "|       IT| 40|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "865235bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "depcol = ['dept_name', 'dept_id']\n",
    "df2 = rdd.toDF(depcol)\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b57698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dschema = StructType([StructField('dept_name', StringType(), True),\n",
    "                     StructField('dept_id', StringType(), True)])\n",
    "df1 = spark.createDataFrame(data = rdd, schema=dschema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba1518e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8fb27a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  name|salary|\n",
      "+------+------+\n",
      "| James|  3000|\n",
      "|  Anna|  4001|\n",
      "|Robert|  6200|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James', 3000), ('Anna',4001), ('Robert', 6200)]\n",
    "d1 = spark.createDataFrame(data, ['name', 'salary'])\n",
    "d1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e72a2945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='James', salary=3000),\n",
       " Row(name='Anna', salary=4001),\n",
       " Row(name='Robert', salary=6200)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = d1.rdd\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2bdf16a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['James', 6000], ['Anna', 8002], ['Robert', 12400]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply map() transformation\n",
    "\n",
    "rdd2 = d1.rdd.map(lambda x: [x[0], x[1]*2])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c3786b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  name|bonus|\n",
      "+------+-----+\n",
      "| James| 6000|\n",
      "|  Anna| 8002|\n",
      "|Robert|12400|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdf = spark.createDataFrame(rdd2,['name','bonus'])\n",
    "rdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a86232",
   "metadata": {},
   "source": [
    "### 16. Convert a Pyspark DF to Pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8f3e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)]\n",
    "columns = ['firstname', 'middlename','lastname', 'dob', 'gender','salary']\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5ec8a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James</td>\n",
       "      <td></td>\n",
       "      <td>Smith</td>\n",
       "      <td>1991-04-01</td>\n",
       "      <td>M</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael</td>\n",
       "      <td>Rose</td>\n",
       "      <td></td>\n",
       "      <td>2000-05-19</td>\n",
       "      <td>M</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert</td>\n",
       "      <td></td>\n",
       "      <td>Williams</td>\n",
       "      <td>1978-09-05</td>\n",
       "      <td>M</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maria</td>\n",
       "      <td>Anne</td>\n",
       "      <td>Jones</td>\n",
       "      <td>1967-12-01</td>\n",
       "      <td>F</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jen</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Brown</td>\n",
       "      <td>1980-02-17</td>\n",
       "      <td>F</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  firstname middlename  lastname         dob gender  salary\n",
       "0     James                Smith  1991-04-01      M    3000\n",
       "1   Michael       Rose            2000-05-19      M    4000\n",
       "2    Robert             Williams  1978-09-05      M    4000\n",
       "3     Maria       Anne     Jones  1967-12-01      F    4000\n",
       "4       Jen       Mary     Brown  1980-02-17      F      -1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = df.toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017dd828",
   "metadata": {},
   "source": [
    "### 17. Write a pyspark DF to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4368aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)]\n",
    "columns = ['firstname', 'middlename','lastname', 'dob', 'gender','salary']\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0890115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pan = df.toPandas()\n",
    "pan.to_csv('d1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186aed87",
   "metadata": {},
   "source": [
    "### 18 . cache or persist a pyspark DF for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0cb21327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------+\n",
      "| id|    name|age|salary|     address| nominee|     _c6|\n",
      "+---+--------+---+------+------------+--------+--------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|    NULL|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|    NULL|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|nominee3|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|nominee4|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|    NULL|\n",
      "+---+--------+---+------+------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.read.format('csv')\\\n",
    "    .option('header', True)\\\n",
    "    .option('inferschema', True)\\\n",
    "    .option('mode', 'FAILFAST')\\\n",
    "    .load('employee_data.csv')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47f3fb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------+\n",
      "| id|    name|age|salary|     address| nominee|     _c6|\n",
      "+---+--------+---+------+------------+--------+--------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|    NULL|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|    NULL|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|nominee3|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|nominee4|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|    NULL|\n",
      "+---+--------+---+------+------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('employee_data.csv', header = True, inferSchema = True, mode = 'FAILFAST')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac968515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------+\n",
      "| id|    name|age|salary|     address| nominee|     _c6|\n",
      "+---+--------+---+------+------------+--------+--------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|    NULL|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|    NULL|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|nominee3|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|nominee4|\n",
      "|  5|  Vikash| 31|300000|        NULL|nominee5|    NULL|\n",
      "+---+--------+---+------+------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv('employee_data.csv', header = True, inferSchema = True, mode = 'PERMISSIVE')\n",
    "df1.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7bc368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4066f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SparkSession' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'SparkSession' object is not callable"
     ]
    }
   ],
   "source": [
    "spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6dc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
